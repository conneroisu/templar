[
  {
    "session_id": "42e03a9d-fb13-4416-96b7-b2ff5d56bda2",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/42e03a9d-fb13-4416-96b7-b2ff5d56bda2.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Grep",
    "tool_input": {
      "pattern": "type Client struct\" -A 20",
      "path": "internal/server",
      "output_mode": "content"
    },
    "tool_response": {
      "mode": "content",
      "numFiles": 0,
      "filenames": [],
      "content": "",
      "numLines": 0
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "grep -n \"calculateSummary\" /home/connerohnesorge/Documents/001Repos/templar/cmd/audit.go",
      "description": "Find calls to calculateSummary in audit.go"
    },
    "tool_response": {
      "stdout": "282:\treport.Summary = calculateSummary(filteredViolations, report.Passed)",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Write",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/server/websocket_timeout_test.go",
      "content": "package server\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\ntype MockOriginValidator struct{}\n\nfunc (m *MockOriginValidator) IsAllowedOrigin(origin string) bool {\n\treturn true\n}\n\nfunc TestWebSocketManagerTimeout(t *testing.T) {\n\tt.Run(\"websocket manager respects configured websocket timeout\", func(t *testing.T) {\n\t\t// Create a config with specific WebSocket timeout\n\t\tcfg := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tWebSocket: 90 * time.Second, // Custom timeout\n\t\t\t},\n\t\t}\n\n\t\t// Create WebSocket manager with timeout config\n\t\toriginValidator := &MockOriginValidator{}\n\t\tmanager := NewWebSocketManager(originValidator, nil, cfg)\n\t\tdefer manager.Shutdown(nil)\n\n\t\t// Test that the getWebSocketTimeout returns the configured value\n\t\ttimeout := manager.getWebSocketTimeout()\n\t\tassert.Equal(t, 90*time.Second, timeout, \"Should return configured WebSocket timeout\")\n\t})\n\n\tt.Run(\"websocket manager uses default websocket timeout when no config\", func(t *testing.T) {\n\t\t// Create WebSocket manager without config\n\t\toriginValidator := &MockOriginValidator{}\n\t\tmanager := NewWebSocketManager(originValidator, nil)\n\t\tdefer manager.Shutdown(nil)\n\n\t\t// Test that the getWebSocketTimeout returns the default value\n\t\ttimeout := manager.getWebSocketTimeout()\n\t\tassert.Equal(t, 60*time.Second, timeout, \"Should return default WebSocket timeout\")\n\t})\n\n\tt.Run(\"websocket manager respects configured network timeout\", func(t *testing.T) {\n\t\t// Create a config with specific network timeout\n\t\tcfg := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tNetwork: 15 * time.Second, // Custom network timeout\n\t\t\t},\n\t\t}\n\n\t\t// Create WebSocket manager with timeout config\n\t\toriginValidator := &MockOriginValidator{}\n\t\tmanager := NewWebSocketManager(originValidator, nil, cfg)\n\t\tdefer manager.Shutdown(nil)\n\n\t\t// Test that the getNetworkTimeout returns the configured value\n\t\ttimeout := manager.getNetworkTimeout()\n\t\tassert.Equal(t, 15*time.Second, timeout, \"Should return configured network timeout\")\n\t})\n\n\tt.Run(\"websocket manager uses default network timeout when no config\", func(t *testing.T) {\n\t\t// Create WebSocket manager without config\n\t\toriginValidator := &MockOriginValidator{}\n\t\tmanager := NewWebSocketManager(originValidator, nil)\n\t\tdefer manager.Shutdown(nil)\n\n\t\t// Test that the getNetworkTimeout returns the default value\n\t\ttimeout := manager.getNetworkTimeout()\n\t\tassert.Equal(t, 10*time.Second, timeout, \"Should return default network timeout\")\n\t})\n\n\tt.Run(\"timeout configuration validation\", func(t *testing.T) {\n\t\t// Test various timeout values\n\t\ttestCases := []struct {\n\t\t\tname              string\n\t\t\twebsocketTimeout  time.Duration\n\t\t\tnetworkTimeout    time.Duration\n\t\t\texpectedWS        time.Duration\n\t\t\texpectedNetwork   time.Duration\n\t\t}{\n\t\t\t{\n\t\t\t\tname:              \"positive timeouts\",\n\t\t\t\twebsocketTimeout:  45 * time.Second,\n\t\t\t\tnetworkTimeout:    20 * time.Second,\n\t\t\t\texpectedWS:        45 * time.Second,\n\t\t\t\texpectedNetwork:   20 * time.Second,\n\t\t\t},\n\t\t\t{\n\t\t\t\tname:              \"zero timeouts use defaults\",\n\t\t\t\twebsocketTimeout:  0,\n\t\t\t\tnetworkTimeout:    0,\n\t\t\t\texpectedWS:        60 * time.Second,\n\t\t\t\texpectedNetwork:   10 * time.Second,\n\t\t\t},\n\t\t\t{\n\t\t\t\tname:              \"negative timeouts use defaults\",\n\t\t\t\twebsocketTimeout:  -1 * time.Second,\n\t\t\t\tnetworkTimeout:    -1 * time.Second,\n\t\t\t\texpectedWS:        60 * time.Second,\n\t\t\t\texpectedNetwork:   10 * time.Second,\n\t\t\t},\n\t\t}\n\n\t\toriginValidator := &MockOriginValidator{}\n\n\t\tfor _, tc := range testCases {\n\t\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\t\tcfg := &config.Config{\n\t\t\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\t\t\tWebSocket: tc.websocketTimeout,\n\t\t\t\t\t\tNetwork:   tc.networkTimeout,\n\t\t\t\t\t},\n\t\t\t\t}\n\n\t\t\t\tmanager := NewWebSocketManager(originValidator, nil, cfg)\n\t\t\t\tdefer manager.Shutdown(nil)\n\n\t\t\t\twsTimeout := manager.getWebSocketTimeout()\n\t\t\t\tnetworkTimeout := manager.getNetworkTimeout()\n\t\t\t\t\n\t\t\t\tassert.Equal(t, tc.expectedWS, wsTimeout, \"WebSocket timeout mismatch\")\n\t\t\t\tassert.Equal(t, tc.expectedNetwork, networkTimeout, \"Network timeout mismatch\")\n\t\t\t})\n\t\t}\n\t})\n\n\tt.Run(\"multiple config parameters\", func(t *testing.T) {\n\t\t// Test multiple config parameters - should use the first one\n\t\tcfg1 := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tWebSocket: 30 * time.Second,\n\t\t\t\tNetwork:   5 * time.Second,\n\t\t\t},\n\t\t}\n\t\tcfg2 := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tWebSocket: 120 * time.Second,\n\t\t\t\tNetwork:   25 * time.Second,\n\t\t\t},\n\t\t}\n\n\t\toriginValidator := &MockOriginValidator{}\n\t\tmanager := NewWebSocketManager(originValidator, nil, cfg1, cfg2)\n\t\tdefer manager.Shutdown(nil)\n\n\t\twsTimeout := manager.getWebSocketTimeout()\n\t\tnetworkTimeout := manager.getNetworkTimeout()\n\t\t\n\t\tassert.Equal(t, 30*time.Second, wsTimeout, \"Should use first config for WebSocket timeout\")\n\t\tassert.Equal(t, 5*time.Second, networkTimeout, \"Should use first config for network timeout\")\n\t})\n}"
    },
    "tool_response": {
      "type": "create",
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/server/websocket_timeout_test.go",
      "content": "package server\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\ntype MockOriginValidator struct{}\n\nfunc (m *MockOriginValidator) IsAllowedOrigin(origin string) bool {\n\treturn true\n}\n\nfunc TestWebSocketManagerTimeout(t *testing.T) {\n\tt.Run(\"websocket manager respects configured websocket timeout\", func(t *testing.T) {\n\t\t// Create a config with specific WebSocket timeout\n\t\tcfg := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tWebSocket: 90 * time.Second, // Custom timeout\n\t\t\t},\n\t\t}\n\n\t\t// Create WebSocket manager with timeout config\n\t\toriginValidator := &MockOriginValidator{}\n\t\tmanager := NewWebSocketManager(originValidator, nil, cfg)\n\t\tdefer manager.Shutdown(nil)\n\n\t\t// Test that the getWebSocketTimeout returns the configured value\n\t\ttimeout := manager.getWebSocketTimeout()\n\t\tassert.Equal(t, 90*time.Second, timeout, \"Should return configured WebSocket timeout\")\n\t})\n\n\tt.Run(\"websocket manager uses default websocket timeout when no config\", func(t *testing.T) {\n\t\t// Create WebSocket manager without config\n\t\toriginValidator := &MockOriginValidator{}\n\t\tmanager := NewWebSocketManager(originValidator, nil)\n\t\tdefer manager.Shutdown(nil)\n\n\t\t// Test that the getWebSocketTimeout returns the default value\n\t\ttimeout := manager.getWebSocketTimeout()\n\t\tassert.Equal(t, 60*time.Second, timeout, \"Should return default WebSocket timeout\")\n\t})\n\n\tt.Run(\"websocket manager respects configured network timeout\", func(t *testing.T) {\n\t\t// Create a config with specific network timeout\n\t\tcfg := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tNetwork: 15 * time.Second, // Custom network timeout\n\t\t\t},\n\t\t}\n\n\t\t// Create WebSocket manager with timeout config\n\t\toriginValidator := &MockOriginValidator{}\n\t\tmanager := NewWebSocketManager(originValidator, nil, cfg)\n\t\tdefer manager.Shutdown(nil)\n\n\t\t// Test that the getNetworkTimeout returns the configured value\n\t\ttimeout := manager.getNetworkTimeout()\n\t\tassert.Equal(t, 15*time.Second, timeout, \"Should return configured network timeout\")\n\t})\n\n\tt.Run(\"websocket manager uses default network timeout when no config\", func(t *testing.T) {\n\t\t// Create WebSocket manager without config\n\t\toriginValidator := &MockOriginValidator{}\n\t\tmanager := NewWebSocketManager(originValidator, nil)\n\t\tdefer manager.Shutdown(nil)\n\n\t\t// Test that the getNetworkTimeout returns the default value\n\t\ttimeout := manager.getNetworkTimeout()\n\t\tassert.Equal(t, 10*time.Second, timeout, \"Should return default network timeout\")\n\t})\n\n\tt.Run(\"timeout configuration validation\", func(t *testing.T) {\n\t\t// Test various timeout values\n\t\ttestCases := []struct {\n\t\t\tname              string\n\t\t\twebsocketTimeout  time.Duration\n\t\t\tnetworkTimeout    time.Duration\n\t\t\texpectedWS        time.Duration\n\t\t\texpectedNetwork   time.Duration\n\t\t}{\n\t\t\t{\n\t\t\t\tname:              \"positive timeouts\",\n\t\t\t\twebsocketTimeout:  45 * time.Second,\n\t\t\t\tnetworkTimeout:    20 * time.Second,\n\t\t\t\texpectedWS:        45 * time.Second,\n\t\t\t\texpectedNetwork:   20 * time.Second,\n\t\t\t},\n\t\t\t{\n\t\t\t\tname:              \"zero timeouts use defaults\",\n\t\t\t\twebsocketTimeout:  0,\n\t\t\t\tnetworkTimeout:    0,\n\t\t\t\texpectedWS:        60 * time.Second,\n\t\t\t\texpectedNetwork:   10 * time.Second,\n\t\t\t},\n\t\t\t{\n\t\t\t\tname:              \"negative timeouts use defaults\",\n\t\t\t\twebsocketTimeout:  -1 * time.Second,\n\t\t\t\tnetworkTimeout:    -1 * time.Second,\n\t\t\t\texpectedWS:        60 * time.Second,\n\t\t\t\texpectedNetwork:   10 * time.Second,\n\t\t\t},\n\t\t}\n\n\t\toriginValidator := &MockOriginValidator{}\n\n\t\tfor _, tc := range testCases {\n\t\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\t\tcfg := &config.Config{\n\t\t\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\t\t\tWebSocket: tc.websocketTimeout,\n\t\t\t\t\t\tNetwork:   tc.networkTimeout,\n\t\t\t\t\t},\n\t\t\t\t}\n\n\t\t\t\tmanager := NewWebSocketManager(originValidator, nil, cfg)\n\t\t\t\tdefer manager.Shutdown(nil)\n\n\t\t\t\twsTimeout := manager.getWebSocketTimeout()\n\t\t\t\tnetworkTimeout := manager.getNetworkTimeout()\n\t\t\t\t\n\t\t\t\tassert.Equal(t, tc.expectedWS, wsTimeout, \"WebSocket timeout mismatch\")\n\t\t\t\tassert.Equal(t, tc.expectedNetwork, networkTimeout, \"Network timeout mismatch\")\n\t\t\t})\n\t\t}\n\t})\n\n\tt.Run(\"multiple config parameters\", func(t *testing.T) {\n\t\t// Test multiple config parameters - should use the first one\n\t\tcfg1 := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tWebSocket: 30 * time.Second,\n\t\t\t\tNetwork:   5 * time.Second,\n\t\t\t},\n\t\t}\n\t\tcfg2 := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tWebSocket: 120 * time.Second,\n\t\t\t\tNetwork:   25 * time.Second,\n\t\t\t},\n\t\t}\n\n\t\toriginValidator := &MockOriginValidator{}\n\t\tmanager := NewWebSocketManager(originValidator, nil, cfg1, cfg2)\n\t\tdefer manager.Shutdown(nil)\n\n\t\twsTimeout := manager.getWebSocketTimeout()\n\t\tnetworkTimeout := manager.getNetworkTimeout()\n\t\t\n\t\tassert.Equal(t, 30*time.Second, wsTimeout, \"Should use first config for WebSocket timeout\")\n\t\tassert.Equal(t, 5*time.Second, networkTimeout, \"Should use first config for network timeout\")\n\t})\n}",
      "structuredPatch": []
    }
  },
  {
    "session_id": "42e03a9d-fb13-4416-96b7-b2ff5d56bda2",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/42e03a9d-fb13-4416-96b7-b2ff5d56bda2.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "grep -n -A 15 \"type Client struct\" /home/connerohnesorge/Documents/001Repos/templar/internal/server/server.go",
      "description": "Find Client struct definition"
    },
    "tool_response": {
      "stdout": "39:type Client struct {\n40-\tconn         *websocket.Conn\n41-\tsend         chan []byte\n42-\tserver       *PreviewServer\n43-\tlastActivity time.Time            // For connection timeout tracking\n44-\trateLimiter  WebSocketRateLimiter // WebSocket-specific rate limiter interface\n45-}\n46-\n47-// PreviewServer serves components with live reload capability\n48-type PreviewServer struct {\n49-\tconfig          *config.Config\n50-\thttpServer      *http.Server\n51-\tserverMutex     sync.RWMutex // Protects httpServer and server state\n52-\tclients         map[*websocket.Conn]*Client\n53-\tclientsMutex    sync.RWMutex\n54-\tbroadcast       chan []byte",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "TodoWrite",
    "tool_input": {
      "todos": [
        {
          "content": "Task-105 - Implement Hash-Based Change Detection Optimization",
          "status": "in_progress",
          "priority": "high",
          "id": "hash-optimization"
        },
        {
          "content": "Analyze current CRC32 hashing performance",
          "status": "pending",
          "priority": "high",
          "id": "hash-analysis"
        },
        {
          "content": "Implement hierarchical hashing with metadata sampling",
          "status": "pending",
          "priority": "high",
          "id": "hierarchical-hash"
        },
        {
          "content": "Optimize hash generation for large template files",
          "status": "pending",
          "priority": "high",
          "id": "large-file-hash"
        },
        {
          "content": "Integrate with existing build cache system",
          "status": "pending",
          "priority": "high",
          "id": "cache-integration"
        },
        {
          "content": "Add performance benchmarks and fallback mechanisms",
          "status": "pending",
          "priority": "high",
          "id": "benchmarks-fallback"
        }
      ]
    },
    "tool_response": {
      "oldTodos": [
        {
          "content": "Task-82 - Implement AST parsing optimization for large component files",
          "status": "completed",
          "priority": "high",
          "id": "ast-optimization"
        },
        {
          "content": "Analyze current AST parsing performance bottlenecks",
          "status": "completed",
          "priority": "high",
          "id": "ast-analysis"
        },
        {
          "content": "Implement missing cache methods (getCachedMetadata, setCachedMetadata)",
          "status": "completed",
          "priority": "high",
          "id": "cache-methods"
        },
        {
          "content": "Add AST cache preloading and concurrent parsing",
          "status": "completed",
          "priority": "high",
          "id": "ast-concurrent"
        },
        {
          "content": "Eliminate worker thread blocking during parsing",
          "status": "completed",
          "priority": "high",
          "id": "thread-blocking"
        },
        {
          "content": "Add large file parsing optimization",
          "status": "completed",
          "priority": "high",
          "id": "large-file-optimization"
        },
        {
          "content": "Validate memory usage remains within bounds",
          "status": "completed",
          "priority": "high",
          "id": "memory-validation"
        },
        {
          "content": "Update task documentation with implementation notes",
          "status": "completed",
          "priority": "high",
          "id": "task-docs"
        }
      ],
      "newTodos": [
        {
          "content": "Task-105 - Implement Hash-Based Change Detection Optimization",
          "status": "in_progress",
          "priority": "high",
          "id": "hash-optimization"
        },
        {
          "content": "Analyze current CRC32 hashing performance",
          "status": "pending",
          "priority": "high",
          "id": "hash-analysis"
        },
        {
          "content": "Implement hierarchical hashing with metadata sampling",
          "status": "pending",
          "priority": "high",
          "id": "hierarchical-hash"
        },
        {
          "content": "Optimize hash generation for large template files",
          "status": "pending",
          "priority": "high",
          "id": "large-file-hash"
        },
        {
          "content": "Integrate with existing build cache system",
          "status": "pending",
          "priority": "high",
          "id": "cache-integration"
        },
        {
          "content": "Add performance benchmarks and fallback mechanisms",
          "status": "pending",
          "priority": "high",
          "id": "benchmarks-fallback"
        }
      ]
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Edit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/cmd/audit.go",
      "old_string": "\treport.Summary = calculateSummary(filteredViolations, report.Passed)",
      "new_string": "\treport.Summary = calculateAccessibilitySummary(filteredViolations, report.Passed)"
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/cmd/audit.go",
      "oldString": "\treport.Summary = calculateSummary(filteredViolations, report.Passed)",
      "newString": "\treport.Summary = calculateAccessibilitySummary(filteredViolations, report.Passed)",
      "originalFile": "package cmd\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/accessibility\"\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/conneroisu/templar/internal/interfaces\"\n\t\"github.com/conneroisu/templar/internal/logging\"\n\t\"github.com/conneroisu/templar/internal/registry\"\n\t\"github.com/conneroisu/templar/internal/renderer\"\n\t\"github.com/conneroisu/templar/internal/scanner\"\n\t\"github.com/spf13/cobra\"\n)\n\nvar (\n\tauditComponentName   string\n\tauditWCAGLevel       string\n\tauditOutputFormat    string\n\tauditOutputFile      string\n\tauditIncludeHTML     bool\n\tauditFixableOnly     bool\n\tauditSeverityFilter  string\n\tauditQuiet           bool\n\tauditVerbose         bool\n\tauditMaxViolations   int\n\tauditGenerateReport  bool\n\tauditShowSuggestions bool\n\tauditAutoFix         bool\n\tauditShowGuidance    bool\n\tauditGuidanceOnly    bool\n)\n\n// auditCmd represents the audit command\nvar auditCmd = &cobra.Command{\n\tUse:   \"audit [component-name]\",\n\tShort: \"Run accessibility audit on components\",\n\tLong: `Run comprehensive accessibility audits on templ components to identify\nWCAG compliance issues and get actionable suggestions for improvements.\n\nThe audit command can test individual components or all components in your project.\nIt provides detailed reports with severity levels, WCAG criteria mapping, and\nspecific suggestions for fixing accessibility issues.\n\nExamples:\n  # Audit all components\n  templar audit\n\n  # Audit specific component\n  templar audit Button\n\n  # Audit with specific WCAG level\n  templar audit --wcag-level AA\n\n  # Generate HTML report\n  templar audit --output html --output-file report.html\n\n  # Show only critical issues\n  templar audit --severity error\n\n  # Apply automatic fixes\n  templar audit --auto-fix`,\n\tValidArgsFunction: func(cmd *cobra.Command, args []string, toComplete string) ([]string, cobra.ShellCompDirective) {\n\t\treturn getComponentCompletions(toComplete), cobra.ShellCompDirectiveNoFileComp\n\t},\n\tRunE: runAuditCommand,\n}\n\nfunc init() {\n\trootCmd.AddCommand(auditCmd)\n\n\tauditCmd.Flags().StringVarP(&auditComponentName, \"component\", \"c\", \"\", \"Specific component to audit (if not provided as argument)\")\n\tauditCmd.Flags().StringVarP(&auditWCAGLevel, \"wcag-level\", \"w\", \"AA\", \"WCAG compliance level to test against (A, AA, AAA)\")\n\tauditCmd.Flags().StringVarP(&auditOutputFormat, \"output\", \"o\", \"console\", \"Output format (console, json, html, markdown)\")\n\tauditCmd.Flags().StringVarP(&auditOutputFile, \"output-file\", \"f\", \"\", \"Output file path (stdout if not specified)\")\n\tauditCmd.Flags().BoolVar(&auditIncludeHTML, \"include-html\", false, \"Include HTML snapshot in report\")\n\tauditCmd.Flags().BoolVar(&auditFixableOnly, \"fixable-only\", false, \"Show only issues that can be automatically fixed\")\n\tauditCmd.Flags().StringVarP(&auditSeverityFilter, \"severity\", \"s\", \"\", \"Filter by severity level (error, warning, info)\")\n\tauditCmd.Flags().BoolVarP(&auditQuiet, \"quiet\", \"q\", false, \"Suppress non-error output\")\n\tauditCmd.Flags().BoolVarP(&auditVerbose, \"verbose\", \"v\", false, \"Enable verbose output\")\n\tauditCmd.Flags().IntVarP(&auditMaxViolations, \"max-violations\", \"m\", 0, \"Maximum number of violations to report (0 = unlimited)\")\n\tauditCmd.Flags().BoolVar(&auditGenerateReport, \"generate-report\", false, \"Generate detailed accessibility report\")\n\tauditCmd.Flags().BoolVar(&auditShowSuggestions, \"show-suggestions\", true, \"Include suggestions in output\")\n\tauditCmd.Flags().BoolVar(&auditAutoFix, \"auto-fix\", false, \"Attempt to automatically fix issues where possible\")\n\tauditCmd.Flags().BoolVar(&auditShowGuidance, \"show-guidance\", false, \"Include detailed accessibility guidance\")\n\tauditCmd.Flags().BoolVar(&auditGuidanceOnly, \"guidance-only\", false, \"Show only guidance without running audit\")\n}\n\nfunc runAuditCommand(cmd *cobra.Command, args []string) error {\n\tctx := context.Background()\n\n\t// Determine component name from args or flag\n\tcomponentName := auditComponentName\n\tif len(args) > 0 {\n\t\tcomponentName = args[0]\n\t}\n\n\t// Handle guidance-only mode\n\tif auditGuidanceOnly {\n\t\treturn showGuidanceOnly(componentName)\n\t}\n\n\t// Initialize logging\n\tlogger := logging.NewLogger(\"audit\")\n\tif auditQuiet {\n\t\tlogger = logging.NewQuietLogger()\n\t} else if auditVerbose {\n\t\tlogger = logging.NewVerboseLogger()\n\t}\n\n\t// Load configuration\n\tcfg, err := config.LoadConfig()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to load configuration: %w\", err)\n\t}\n\n\t// Initialize component registry and scanner\n\tcomponentRegistry := registry.NewComponentRegistry(logger)\n\tcomponentScanner := scanner.NewScanner(logger)\n\n\t// Scan components\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Scanning components...\")\n\t}\n\n\tcomponents, err := componentScanner.ScanDirectory(cfg.Components.ScanPaths[0])\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to scan components: %w\", err)\n\t}\n\n\t// Register components\n\tfor _, component := range components {\n\t\tcomponentRegistry.RegisterComponent(component)\n\t}\n\n\t// Create renderer\n\tcomponentRenderer := renderer.NewComponentRenderer(logger)\n\n\t// Initialize accessibility tester\n\ttesterConfig := accessibility.TesterConfig{\n\t\tDefaultWCAGLevel:    parseWCAGLevel(auditWCAGLevel),\n\t\tDefaultTimeout:      30 * time.Second,\n\t\tEnableRealTimeWarn:  false,\n\t\tMaxConcurrentTests:  1,\n\t}\n\n\ttester := accessibility.NewComponentAccessibilityTester(\n\t\tcomponentRegistry,\n\t\tcomponentRenderer,\n\t\tlogger,\n\t\ttesterConfig,\n\t)\n\n\t// Perform audit\n\tif componentName != \"\" {\n\t\treturn runSingleComponentAudit(ctx, tester, componentName, logger)\n\t} else {\n\t\treturn runAllComponentsAudit(ctx, tester, componentRegistry, logger)\n\t}\n}\n\nfunc runSingleComponentAudit(ctx context.Context, tester accessibility.AccessibilityTester, componentName string, logger logging.Logger) error {\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Running accessibility audit\", \"component\", componentName)\n\t}\n\n\t// Run accessibility test\n\treport, err := tester.TestComponent(ctx, componentName, nil)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"accessibility audit failed for %s: %w\", componentName, err)\n\t}\n\n\t// Apply filters\n\treport = applyReportFilters(report)\n\n\t// Apply auto-fixes if requested\n\tif auditAutoFix && len(report.Violations) > 0 {\n\t\tfixedCount, err := applyAutoFixes(ctx, tester, report)\n\t\tif err != nil {\n\t\t\tlogger.Warn(ctx, err, \"Failed to apply auto-fixes\")\n\t\t} else if fixedCount > 0 {\n\t\t\tlogger.Info(ctx, \"Applied automatic fixes\", \"count\", fixedCount)\n\t\t}\n\t}\n\n\t// Output results\n\treturn outputAuditResults([]*accessibility.AccessibilityReport{report}, logger)\n}\n\nfunc runAllComponentsAudit(ctx context.Context, tester accessibility.AccessibilityTester, registry interfaces.ComponentRegistry, logger logging.Logger) error {\n\tcomponents := registry.GetAllComponents()\n\t\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Running accessibility audit on all components\", \"count\", len(components))\n\t}\n\n\treports := []*accessibility.AccessibilityReport{}\n\ttotalViolations := 0\n\ttotalAutoFixes := 0\n\n\tfor i, component := range components {\n\t\tif auditVerbose {\n\t\t\tlogger.Info(ctx, \"Auditing component\", \n\t\t\t\t\"component\", component.Name,\n\t\t\t\t\"progress\", fmt.Sprintf(\"%d/%d\", i+1, len(components)))\n\t\t}\n\n\t\t// Run accessibility test\n\t\treport, err := tester.TestComponent(ctx, component.Name, nil)\n\t\tif err != nil {\n\t\t\tlogger.Warn(ctx, err, \"Failed to audit component\", \"component\", component.Name)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Apply filters\n\t\treport = applyReportFilters(report)\n\t\t\n\t\t// Apply auto-fixes if requested\n\t\tif auditAutoFix && len(report.Violations) > 0 {\n\t\t\tfixedCount, err := applyAutoFixes(ctx, tester, report)\n\t\t\tif err != nil {\n\t\t\t\tlogger.Warn(ctx, err, \"Failed to apply auto-fixes\", \"component\", component.Name)\n\t\t\t} else {\n\t\t\t\ttotalAutoFixes += fixedCount\n\t\t\t}\n\t\t}\n\n\t\treports = append(reports, report)\n\t\ttotalViolations += len(report.Violations)\n\t}\n\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Audit completed\",\n\t\t\t\"components\", len(reports),\n\t\t\t\"total_violations\", totalViolations)\n\t\t\n\t\tif auditAutoFix && totalAutoFixes > 0 {\n\t\t\tlogger.Info(ctx, \"Applied automatic fixes\", \"total_fixes\", totalAutoFixes)\n\t\t}\n\t}\n\n\t// Output results\n\treturn outputAuditResults(reports, logger)\n}\n\nfunc applyReportFilters(report *accessibility.AccessibilityReport) *accessibility.AccessibilityReport {\n\tfilteredViolations := []accessibility.AccessibilityViolation{}\n\n\tfor _, violation := range report.Violations {\n\t\t// Apply severity filter\n\t\tif auditSeverityFilter != \"\" {\n\t\t\texpectedSeverity := parseSeverity(auditSeverityFilter)\n\t\t\tif violation.Severity != expectedSeverity {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// Apply fixable filter\n\t\tif auditFixableOnly && !violation.CanAutoFix {\n\t\t\tcontinue\n\t\t}\n\n\t\tfilteredViolations = append(filteredViolations, violation)\n\t}\n\n\t// Apply max violations limit\n\tif auditMaxViolations > 0 && len(filteredViolations) > auditMaxViolations {\n\t\tfilteredViolations = filteredViolations[:auditMaxViolations]\n\t}\n\n\t// Update report\n\treport.Violations = filteredViolations\n\t\n\t// Recalculate summary\n\treport.Summary = calculateSummary(filteredViolations, report.Passed)\n\n\treturn report\n}\n\nfunc applyAutoFixes(ctx context.Context, tester accessibility.AccessibilityTester, report *accessibility.AccessibilityReport) (int, error) {\n\tif componentTester, ok := tester.(*accessibility.ComponentAccessibilityTester); ok {\n\t\tautoFixableViolations := []accessibility.AccessibilityViolation{}\n\t\tfor _, violation := range report.Violations {\n\t\t\tif violation.CanAutoFix {\n\t\t\t\tautoFixableViolations = append(autoFixableViolations, violation)\n\t\t\t}\n\t\t}\n\n\t\tif len(autoFixableViolations) == 0 {\n\t\t\treturn 0, nil\n\t\t}\n\n\t\t// Apply auto-fixes (this would need integration with file system)\n\t\t_, err := componentTester.AutoFix(ctx, report.HTMLSnapshot, autoFixableViolations)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\n\t\treturn len(autoFixableViolations), nil\n\t}\n\n\treturn 0, fmt.Errorf(\"auto-fix not supported for this tester type\")\n}\n\nfunc outputAuditResults(reports []*accessibility.AccessibilityReport, logger logging.Logger) error {\n\tswitch auditOutputFormat {\n\tcase \"json\":\n\t\treturn outputJSON(reports)\n\tcase \"html\":\n\t\treturn outputHTML(reports)\n\tcase \"markdown\":\n\t\treturn outputMarkdown(reports)\n\tcase \"console\":\n\t\tfallthrough\n\tdefault:\n\t\treturn outputConsole(reports, logger)\n\t}\n}\n\nfunc outputJSON(reports []*accessibility.AccessibilityReport) error {\n\toutput := reports\n\tif len(reports) == 1 {\n\t\toutput = reports[0] // Single component audit returns single report\n\t}\n\n\tjsonData, err := json.MarshalIndent(output, \"\", \"  \")\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to marshal JSON: %w\", err)\n\t}\n\n\treturn writeOutput(string(jsonData))\n}\n\nfunc outputHTML(reports []*accessibility.AccessibilityReport) error {\n\thtml := generateHTMLReport(reports)\n\treturn writeOutput(html)\n}\n\nfunc outputMarkdown(reports []*accessibility.AccessibilityReport) error {\n\tmarkdown := generateMarkdownReport(reports)\n\treturn writeOutput(markdown)\n}\n\nfunc outputConsole(reports []*accessibility.AccessibilityReport, logger logging.Logger) error {\n\tctx := context.Background()\n\t\n\tif len(reports) == 0 {\n\t\tfmt.Println(\"No components audited.\")\n\t\treturn nil\n\t}\n\n\t// Summary statistics\n\ttotalComponents := len(reports)\n\ttotalViolations := 0\n\tcriticalViolations := 0\n\tcomponentsWithIssues := 0\n\toverallScoreSum := 0.0\n\n\tfor _, report := range reports {\n\t\ttotalViolations += len(report.Violations)\n\t\toverallScoreSum += report.Summary.OverallScore\n\t\t\n\t\tif len(report.Violations) > 0 {\n\t\t\tcomponentsWithIssues++\n\t\t}\n\t\t\n\t\tfor _, violation := range report.Violations {\n\t\t\tif violation.Impact == accessibility.ImpactCritical {\n\t\t\t\tcriticalViolations++\n\t\t\t}\n\t\t}\n\t}\n\n\taverageScore := overallScoreSum / float64(totalComponents)\n\n\t// Print summary\n\tfmt.Printf(\"\\n\ud83d\udd0d Accessibility Audit Summary\\n\")\n\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\tfmt.Printf(\"Components audited:     %d\\n\", totalComponents)\n\tfmt.Printf(\"Components with issues: %d\\n\", componentsWithIssues)\n\tfmt.Printf(\"Total violations:       %d\\n\", totalViolations)\n\tfmt.Printf(\"Critical violations:    %d\\n\", criticalViolations)\n\tfmt.Printf(\"Average score:          %.1f/100\\n\", averageScore)\n\t\n\t// Overall status\n\tvar status string\n\tvar statusIcon string\n\tif criticalViolations > 0 {\n\t\tstatus = \"CRITICAL ISSUES FOUND\"\n\t\tstatusIcon = \"\ud83d\udea8\"\n\t} else if totalViolations > 0 {\n\t\tstatus = \"ISSUES FOUND\"\n\t\tstatusIcon = \"\u26a0\ufe0f\"\n\t} else {\n\t\tstatus = \"ALL CHECKS PASSED\"\n\t\tstatusIcon = \"\u2705\"\n\t}\n\t\n\tfmt.Printf(\"Status:                 %s %s\\n\\n\", statusIcon, status)\n\n\t// Detailed component results\n\tif auditVerbose || len(reports) == 1 {\n\t\tfor _, report := range reports {\n\t\t\toutputComponentDetails(report)\n\t\t}\n\t} else if totalViolations > 0 {\n\t\t// Show only components with issues\n\t\tfmt.Printf(\"Components with accessibility issues:\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tfor _, report := range reports {\n\t\t\tif len(report.Violations) > 0 {\n\t\t\t\toutputComponentSummary(report)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Show suggestions if enabled\n\tif auditShowSuggestions && totalViolations > 0 {\n\t\tfmt.Printf(\"\\n\ud83d\udca1 Top Suggestions\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tsuggestions := aggregateSuggestions(reports)\n\t\tfor i, suggestion := range suggestions {\n\t\t\tif i >= 5 { // Limit to top 5\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfmt.Printf(\"%d. %s\\n\", i+1, suggestion.Title)\n\t\t\tif suggestion.Description != \"\" {\n\t\t\t\tfmt.Printf(\"   %s\\n\", suggestion.Description)\n\t\t\t}\n\t\t\tfmt.Printf(\"\\n\")\n\t\t}\n\t}\n\t\n\t// Show detailed guidance if enabled and there are violations\n\tif totalViolations > 0 {\n\t\tallViolations := []accessibility.AccessibilityViolation{}\n\t\tfor _, report := range reports {\n\t\t\tallViolations = append(allViolations, report.Violations...)\n\t\t}\n\t\tshowGuidanceForViolations(allViolations)\n\t}\n\n\treturn nil\n}\n\nfunc outputComponentDetails(report *accessibility.AccessibilityReport) {\n\tcomponentName := report.ComponentName\n\tif componentName == \"\" {\n\t\tcomponentName = \"Unknown Component\"\n\t}\n\n\tscoreColor := getScoreColor(report.Summary.OverallScore)\n\t\n\tfmt.Printf(\"\ud83d\udce6 %s (Score: %s%.1f/100%s)\\n\", \n\t\tcomponentName, scoreColor, report.Summary.OverallScore, \"\\033[0m\")\n\tfmt.Printf(\"   File: %s\\n\", report.ComponentFile)\n\t\n\tif len(report.Violations) == 0 {\n\t\tfmt.Printf(\"   \u2705 No accessibility issues found\\n\\n\")\n\t\treturn\n\t}\n\n\t// Group violations by severity\n\terrorViolations := []accessibility.AccessibilityViolation{}\n\twarningViolations := []accessibility.AccessibilityViolation{}\n\tinfoViolations := []accessibility.AccessibilityViolation{}\n\n\tfor _, violation := range report.Violations {\n\t\tswitch violation.Severity {\n\t\tcase accessibility.SeverityError:\n\t\t\terrorViolations = append(errorViolations, violation)\n\t\tcase accessibility.SeverityWarning:\n\t\t\twarningViolations = append(warningViolations, violation)\n\t\tcase accessibility.SeverityInfo:\n\t\t\tinfoViolations = append(infoViolations, violation)\n\t\t}\n\t}\n\n\t// Output violations by severity\n\tif len(errorViolations) > 0 {\n\t\tfmt.Printf(\"   \ud83d\udea8 Errors (%d):\\n\", len(errorViolations))\n\t\tfor _, violation := range errorViolations {\n\t\t\toutputViolation(violation, \"     \")\n\t\t}\n\t}\n\t\n\tif len(warningViolations) > 0 {\n\t\tfmt.Printf(\"   \u26a0\ufe0f  Warnings (%d):\\n\", len(warningViolations))\n\t\tfor _, violation := range warningViolations {\n\t\t\toutputViolation(violation, \"     \")\n\t\t}\n\t}\n\t\n\tif len(infoViolations) > 0 && auditVerbose {\n\t\tfmt.Printf(\"   \u2139\ufe0f  Info (%d):\\n\", len(infoViolations))\n\t\tfor _, violation := range infoViolations {\n\t\t\toutputViolation(violation, \"     \")\n\t\t}\n\t}\n\n\tfmt.Printf(\"\\n\")\n}\n\nfunc outputComponentSummary(report *accessibility.AccessibilityReport) {\n\tcomponentName := report.ComponentName\n\tif componentName == \"\" {\n\t\tcomponentName = \"Unknown Component\"\n\t}\n\n\terrorCount := 0\n\twarningCount := 0\n\tcriticalCount := 0\n\n\tfor _, violation := range report.Violations {\n\t\tswitch violation.Severity {\n\t\tcase accessibility.SeverityError:\n\t\t\terrorCount++\n\t\tcase accessibility.SeverityWarning:\n\t\t\twarningCount++\n\t\t}\n\t\t\n\t\tif violation.Impact == accessibility.ImpactCritical {\n\t\t\tcriticalCount++\n\t\t}\n\t}\n\n\tscoreColor := getScoreColor(report.Summary.OverallScore)\n\t\n\tfmt.Printf(\"\ud83d\udce6 %s %s(%.1f/100)%s\\n\", componentName, scoreColor, report.Summary.OverallScore, \"\\033[0m\")\n\t\n\tif criticalCount > 0 {\n\t\tfmt.Printf(\"   \ud83d\udea8 %d critical issue(s)\\n\", criticalCount)\n\t}\n\tif errorCount > 0 {\n\t\tfmt.Printf(\"   \u274c %d error(s)\\n\", errorCount)\n\t}\n\tif warningCount > 0 {\n\t\tfmt.Printf(\"   \u26a0\ufe0f  %d warning(s)\\n\", warningCount)\n\t}\n\t\n\tfmt.Printf(\"\\n\")\n}\n\nfunc outputViolation(violation accessibility.AccessibilityViolation, indent string) {\n\tfmt.Printf(\"%s\u2022 %s\\n\", indent, violation.Message)\n\tfmt.Printf(\"%s  Rule: %s | WCAG: %s %s\\n\", \n\t\tindent, violation.Rule, violation.WCAG.Level, violation.WCAG.Criteria)\n\t\n\tif violation.Element != \"\" {\n\t\tfmt.Printf(\"%s  Element: <%s>\\n\", indent, violation.Element)\n\t}\n\t\n\tif auditShowSuggestions && len(violation.Suggestions) > 0 {\n\t\tfmt.Printf(\"%s  \ud83d\udca1 %s\\n\", indent, violation.Suggestions[0].Title)\n\t\tif violation.Suggestions[0].Code != \"\" && auditVerbose {\n\t\t\tfmt.Printf(\"%s     Code: %s\\n\", indent, violation.Suggestions[0].Code)\n\t\t}\n\t}\n\t\n\tif violation.CanAutoFix {\n\t\tfmt.Printf(\"%s  \ud83d\udd27 Auto-fixable\\n\", indent)\n\t}\n\t\n\tfmt.Printf(\"\\n\")\n}\n\nfunc writeOutput(content string) error {\n\tif auditOutputFile != \"\" {\n\t\t// Ensure output directory exists\n\t\tdir := filepath.Dir(auditOutputFile)\n\t\tif err := os.MkdirAll(dir, 0755); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to create output directory: %w\", err)\n\t\t}\n\t\t\n\t\t// Write to file\n\t\tif err := os.WriteFile(auditOutputFile, []byte(content), 0644); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to write output file: %w\", err)\n\t\t}\n\t\t\n\t\tfmt.Printf(\"Report written to: %s\\n\", auditOutputFile)\n\t\treturn nil\n\t}\n\t\n\t// Write to stdout\n\tfmt.Print(content)\n\treturn nil\n}\n\n// Helper functions\nfunc parseWCAGLevel(level string) accessibility.WCAGLevel {\n\tswitch strings.ToUpper(level) {\n\tcase \"A\":\n\t\treturn accessibility.WCAGLevelA\n\tcase \"AA\":\n\t\treturn accessibility.WCAGLevelAA\n\tcase \"AAA\":\n\t\treturn accessibility.WCAGLevelAAA\n\tdefault:\n\t\treturn accessibility.WCAGLevelAA\n\t}\n}\n\nfunc parseSeverity(severity string) accessibility.ViolationSeverity {\n\tswitch strings.ToLower(severity) {\n\tcase \"error\":\n\t\treturn accessibility.SeverityError\n\tcase \"warning\":\n\t\treturn accessibility.SeverityWarning\n\tcase \"info\":\n\t\treturn accessibility.SeverityInfo\n\tdefault:\n\t\treturn accessibility.SeverityWarning\n\t}\n}\n\nfunc getScoreColor(score float64) string {\n\tif score >= 90 {\n\t\treturn \"\\033[32m\" // Green\n\t} else if score >= 70 {\n\t\treturn \"\\033[33m\" // Yellow\n\t} else {\n\t\treturn \"\\033[31m\" // Red\n\t}\n}\n\nfunc aggregateSuggestions(reports []*accessibility.AccessibilityReport) []accessibility.AccessibilitySuggestion {\n\tsuggestionMap := make(map[string]*accessibility.AccessibilitySuggestion)\n\tsuggestionCounts := make(map[string]int)\n\t\n\tfor _, report := range reports {\n\t\tfor _, violation := range report.Violations {\n\t\t\tfor _, suggestion := range violation.Suggestions {\n\t\t\t\tkey := fmt.Sprintf(\"%s_%s\", suggestion.Type, suggestion.Title)\n\t\t\t\tsuggestionCounts[key]++\n\t\t\t\t\n\t\t\t\tif existing, exists := suggestionMap[key]; !exists || suggestion.Priority < existing.Priority {\n\t\t\t\t\tsuggestionCopy := suggestion\n\t\t\t\t\tsuggestionMap[key] = &suggestionCopy\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t\n\t// Convert to slice and sort by frequency and priority\n\tsuggestions := []accessibility.AccessibilitySuggestion{}\n\tfor key, suggestion := range suggestionMap {\n\t\t// Adjust priority based on frequency (more frequent = higher priority)\n\t\tsuggestion.Priority -= suggestionCounts[key] // Lower number = higher priority\n\t\tsuggestions = append(suggestions, *suggestion)\n\t}\n\t\n\t// Sort by priority\n\tsort.Slice(suggestions, func(i, j int) bool {\n\t\treturn suggestions[i].Priority < suggestions[j].Priority\n\t})\n\t\n\treturn suggestions\n}\n\nfunc calculateAccessibilitySummary(violations []accessibility.AccessibilityViolation, passed []accessibility.AccessibilityRule) accessibility.AccessibilitySummary {\n\tsummary := accessibility.AccessibilitySummary{\n\t\tTotalRules:      len(passed) + len(violations),\n\t\tPassedRules:     len(passed),\n\t\tFailedRules:     len(violations),\n\t\tTotalViolations: len(violations),\n\t}\n\t\n\tfor _, violation := range violations {\n\t\tswitch violation.Severity {\n\t\tcase accessibility.SeverityError:\n\t\t\tsummary.ErrorViolations++\n\t\tcase accessibility.SeverityWarning:\n\t\t\tsummary.WarnViolations++\n\t\tcase accessibility.SeverityInfo:\n\t\t\tsummary.InfoViolations++\n\t\t}\n\t\t\n\t\tswitch violation.Impact {\n\t\tcase accessibility.ImpactCritical:\n\t\t\tsummary.CriticalImpact++\n\t\tcase accessibility.ImpactSerious:\n\t\t\tsummary.SeriousImpact++\n\t\tcase accessibility.ImpactModerate:\n\t\t\tsummary.ModerateImpact++\n\t\tcase accessibility.ImpactMinor:\n\t\t\tsummary.MinorImpact++\n\t\t}\n\t}\n\t\n\t// Calculate overall score\n\tif summary.TotalRules > 0 {\n\t\tsummary.OverallScore = float64(summary.PassedRules) / float64(summary.TotalRules) * 100\n\t}\n\t\n\treturn summary\n}\n\nfunc generateHTMLReport(reports []*accessibility.AccessibilityReport) string {\n\t// This would generate a comprehensive HTML report\n\t// For brevity, returning a simplified version\n\thtml := `<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Accessibility Audit Report</title>\n    <style>\n        body { font-family: Arial, sans-serif; margin: 40px; }\n        .header { background: #f5f5f5; padding: 20px; border-radius: 8px; }\n        .component { margin: 20px 0; padding: 20px; border: 1px solid #ddd; border-radius: 8px; }\n        .violation { margin: 10px 0; padding: 10px; background: #fff3cd; border-left: 4px solid #ffc107; }\n        .error { background: #f8d7da; border-left-color: #dc3545; }\n        .success { background: #d4edda; border-left-color: #28a745; }\n    </style>\n</head>\n<body>\n    <div class=\"header\">\n        <h1>Accessibility Audit Report</h1>\n        <p>Generated on: ` + time.Now().Format(\"2006-01-02 15:04:05\") + `</p>\n    </div>`\n\n\tfor _, report := range reports {\n\t\thtml += fmt.Sprintf(`\n    <div class=\"component\">\n        <h2>%s</h2>\n        <p>Score: %.1f/100</p>\n        <p>Violations: %d</p>\n    </div>`, report.ComponentName, report.Summary.OverallScore, len(report.Violations))\n\t}\n\n\thtml += `\n</body>\n</html>`\n\n\treturn html\n}\n\nfunc generateMarkdownReport(reports []*accessibility.AccessibilityReport) string {\n\tmd := fmt.Sprintf(\"# Accessibility Audit Report\\n\\nGenerated on: %s\\n\\n\", time.Now().Format(\"2006-01-02 15:04:05\"))\n\t\n\tfor _, report := range reports {\n\t\tmd += fmt.Sprintf(\"## %s\\n\\n\", report.ComponentName)\n\t\tmd += fmt.Sprintf(\"- **Score**: %.1f/100\\n\", report.Summary.OverallScore)\n\t\tmd += fmt.Sprintf(\"- **Violations**: %d\\n\\n\", len(report.Violations))\n\t\t\n\t\tif len(report.Violations) > 0 {\n\t\t\tmd += \"### Issues Found\\n\\n\"\n\t\t\tfor _, violation := range report.Violations {\n\t\t\t\tmd += fmt.Sprintf(\"- **%s**: %s\\n\", violation.Rule, violation.Message)\n\t\t\t}\n\t\t\tmd += \"\\n\"\n\t\t}\n\t}\n\t\n\treturn md\n}\n\nfunc getComponentCompletions(toComplete string) []string {\n\t// This would integrate with the component registry to provide completions\n\t// For now, returning empty slice\n\treturn []string{}\n}\n\n// showGuidanceOnly displays accessibility guidance without running an audit\nfunc showGuidanceOnly(componentName string) error {\n\tguide := accessibility.NewAccessibilityGuide()\n\t\n\tif componentName != \"\" {\n\t\t// Show component-specific guidance\n\t\tfmt.Printf(\"\ud83c\udfaf Accessibility Guidance for %s Component\\n\", componentName)\n\t\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\t\t\n\t\tguidanceText := guide.GetComponentGuidanceText(componentName)\n\t\tfmt.Print(guidanceText)\n\t\t\n\t\t// Also show general guidance applicable to all components\n\t\tfmt.Printf(\"\\n\ud83d\udccb General Accessibility Guidelines\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tquickStart := guide.GetQuickStartGuide()\n\t\tfor i, item := range quickStart {\n\t\t\tif i >= 3 { // Limit to top 3 for brevity\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfmt.Printf(\"%d. %s\\n\", i+1, item.Title)\n\t\t\tfmt.Printf(\"   %s\\n\\n\", item.Description)\n\t\t}\n\t\t\n\t} else {\n\t\t// Show general accessibility guidance\n\t\tfmt.Printf(\"\ud83c\udf1f Accessibility Quick Start Guide\\n\")\n\t\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\t\t\n\t\tquickStart := guide.GetQuickStartGuide()\n\t\tfor i, item := range quickStart {\n\t\t\tfmt.Printf(\"%d. %s\\n\", i+1, item.Title)\n\t\t\tfmt.Printf(\"   %s\\n\", item.Description)\n\t\t\t\n\t\t\tif len(item.Examples) > 0 {\n\t\t\t\texample := item.Examples[0]\n\t\t\t\tif example.BadCode != \"\" {\n\t\t\t\t\tfmt.Printf(\"   \u274c Avoid: %s\\n\", strings.ReplaceAll(example.BadCode, \"\\n\", \" \"))\n\t\t\t\t}\n\t\t\t\tif example.GoodCode != \"\" {\n\t\t\t\t\tfmt.Printf(\"   \u2705 Use: %s\\n\", strings.ReplaceAll(example.GoodCode, \"\\n\", \" \"))\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Printf(\"\\n\")\n\t\t}\n\t\t\n\t\tfmt.Printf(\"\ud83d\udca1 Advanced Guidelines\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tbestPractices := guide.GetBestPracticesGuide()\n\t\tfor i, item := range bestPractices {\n\t\t\tif i >= 3 { // Limit for readability\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfmt.Printf(\"\u2022 %s\\n\", item.Title)\n\t\t\tfmt.Printf(\"  %s\\n\\n\", item.Description)\n\t\t}\n\t\t\n\t\tfmt.Printf(\"\ud83d\udcda Additional Resources\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\")\n\t\tfmt.Printf(\"\u2022 WCAG Quick Reference: https://www.w3.org/WAI/WCAG21/quickref/\\n\")\n\t\tfmt.Printf(\"\u2022 WebAIM Guidelines: https://webaim.org/\\n\")\n\t\tfmt.Printf(\"\u2022 A11y Project: https://www.a11yproject.com/\\n\")\n\t\tfmt.Printf(\"\u2022 MDN Accessibility: https://developer.mozilla.org/en-US/docs/Web/Accessibility\\n\\n\")\n\t\t\n\t\tfmt.Printf(\"\ud83d\udd27 To audit your components, run:\\n\")\n\t\tfmt.Printf(\"   templar audit              # Audit all components\\n\")\n\t\tfmt.Printf(\"   templar audit Button       # Audit specific component\\n\")\n\t\tfmt.Printf(\"   templar audit --help        # See all options\\n\")\n\t}\n\t\n\treturn nil\n}\n\n// showGuidanceForViolations displays guidance for specific accessibility violations\nfunc showGuidanceForViolations(violations []accessibility.AccessibilityViolation) {\n\tif !auditShowGuidance || len(violations) == 0 {\n\t\treturn\n\t}\n\t\n\tfmt.Printf(\"\\n\ud83c\udf93 Accessibility Guidance\\n\")\n\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\t\n\tguide := accessibility.NewAccessibilityGuide()\n\t\n\t// Group violations by rule to avoid duplicate guidance\n\truleMap := make(map[string]bool)\n\tuniqueRules := []string{}\n\t\n\tfor _, violation := range violations {\n\t\tif !ruleMap[violation.Rule] {\n\t\t\truleMap[violation.Rule] = true\n\t\t\tuniqueRules = append(uniqueRules, violation.Rule)\n\t\t}\n\t}\n\t\n\t// Show guidance for each unique rule\n\tfor i, rule := range uniqueRules {\n\t\tif i > 0 {\n\t\t\tfmt.Printf(\"\\n\" + strings.Repeat(\"\u2500\", 60) + \"\\n\\n\")\n\t\t}\n\t\t\n\t\tguidanceText := guide.GetGuidanceText(rule)\n\t\tfmt.Print(guidanceText)\n\t}\n}",
      "structuredPatch": [
        {
          "oldStart": 279,
          "oldLines": 7,
          "newStart": 279,
          "newLines": 7,
          "lines": [
            "   report.Violations = filteredViolations",
            "   ",
            "   // Recalculate summary",
            "-  report.Summary = calculateSummary(filteredViolations, report.Passed)",
            "+  report.Summary = calculateAccessibilitySummary(filteredViolations, report.Passed)",
            " ",
            "   return report",
            " }"
          ]
        }
      ],
      "userModified": false,
      "replaceAll": false
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "backlog task edit 105 -s \"In Progress\"",
      "description": "Mark Task-105 as in progress"
    },
    "tool_response": {
      "stdout": "Updated task task-105",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Edit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/server/websocket_timeout_test.go",
      "old_string": "type MockOriginValidator struct{}\n\nfunc (m *MockOriginValidator) IsAllowedOrigin(origin string) bool {\n\treturn true\n}\n\nfunc TestWebSocketManagerTimeout(t *testing.T) {",
      "new_string": "func TestWebSocketManagerTimeout(t *testing.T) {"
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/server/websocket_timeout_test.go",
      "oldString": "type MockOriginValidator struct{}\n\nfunc (m *MockOriginValidator) IsAllowedOrigin(origin string) bool {\n\treturn true\n}\n\nfunc TestWebSocketManagerTimeout(t *testing.T) {",
      "newString": "func TestWebSocketManagerTimeout(t *testing.T) {",
      "originalFile": "package server\n\nimport (\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\ntype MockOriginValidator struct{}\n\nfunc (m *MockOriginValidator) IsAllowedOrigin(origin string) bool {\n\treturn true\n}\n\nfunc TestWebSocketManagerTimeout(t *testing.T) {\n\tt.Run(\"websocket manager respects configured websocket timeout\", func(t *testing.T) {\n\t\t// Create a config with specific WebSocket timeout\n\t\tcfg := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tWebSocket: 90 * time.Second, // Custom timeout\n\t\t\t},\n\t\t}\n\n\t\t// Create WebSocket manager with timeout config\n\t\toriginValidator := &MockOriginValidator{}\n\t\tmanager := NewWebSocketManager(originValidator, nil, cfg)\n\t\tdefer manager.Shutdown(nil)\n\n\t\t// Test that the getWebSocketTimeout returns the configured value\n\t\ttimeout := manager.getWebSocketTimeout()\n\t\tassert.Equal(t, 90*time.Second, timeout, \"Should return configured WebSocket timeout\")\n\t})\n\n\tt.Run(\"websocket manager uses default websocket timeout when no config\", func(t *testing.T) {\n\t\t// Create WebSocket manager without config\n\t\toriginValidator := &MockOriginValidator{}\n\t\tmanager := NewWebSocketManager(originValidator, nil)\n\t\tdefer manager.Shutdown(nil)\n\n\t\t// Test that the getWebSocketTimeout returns the default value\n\t\ttimeout := manager.getWebSocketTimeout()\n\t\tassert.Equal(t, 60*time.Second, timeout, \"Should return default WebSocket timeout\")\n\t})\n\n\tt.Run(\"websocket manager respects configured network timeout\", func(t *testing.T) {\n\t\t// Create a config with specific network timeout\n\t\tcfg := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tNetwork: 15 * time.Second, // Custom network timeout\n\t\t\t},\n\t\t}\n\n\t\t// Create WebSocket manager with timeout config\n\t\toriginValidator := &MockOriginValidator{}\n\t\tmanager := NewWebSocketManager(originValidator, nil, cfg)\n\t\tdefer manager.Shutdown(nil)\n\n\t\t// Test that the getNetworkTimeout returns the configured value\n\t\ttimeout := manager.getNetworkTimeout()\n\t\tassert.Equal(t, 15*time.Second, timeout, \"Should return configured network timeout\")\n\t})\n\n\tt.Run(\"websocket manager uses default network timeout when no config\", func(t *testing.T) {\n\t\t// Create WebSocket manager without config\n\t\toriginValidator := &MockOriginValidator{}\n\t\tmanager := NewWebSocketManager(originValidator, nil)\n\t\tdefer manager.Shutdown(nil)\n\n\t\t// Test that the getNetworkTimeout returns the default value\n\t\ttimeout := manager.getNetworkTimeout()\n\t\tassert.Equal(t, 10*time.Second, timeout, \"Should return default network timeout\")\n\t})\n\n\tt.Run(\"timeout configuration validation\", func(t *testing.T) {\n\t\t// Test various timeout values\n\t\ttestCases := []struct {\n\t\t\tname              string\n\t\t\twebsocketTimeout  time.Duration\n\t\t\tnetworkTimeout    time.Duration\n\t\t\texpectedWS        time.Duration\n\t\t\texpectedNetwork   time.Duration\n\t\t}{\n\t\t\t{\n\t\t\t\tname:              \"positive timeouts\",\n\t\t\t\twebsocketTimeout:  45 * time.Second,\n\t\t\t\tnetworkTimeout:    20 * time.Second,\n\t\t\t\texpectedWS:        45 * time.Second,\n\t\t\t\texpectedNetwork:   20 * time.Second,\n\t\t\t},\n\t\t\t{\n\t\t\t\tname:              \"zero timeouts use defaults\",\n\t\t\t\twebsocketTimeout:  0,\n\t\t\t\tnetworkTimeout:    0,\n\t\t\t\texpectedWS:        60 * time.Second,\n\t\t\t\texpectedNetwork:   10 * time.Second,\n\t\t\t},\n\t\t\t{\n\t\t\t\tname:              \"negative timeouts use defaults\",\n\t\t\t\twebsocketTimeout:  -1 * time.Second,\n\t\t\t\tnetworkTimeout:    -1 * time.Second,\n\t\t\t\texpectedWS:        60 * time.Second,\n\t\t\t\texpectedNetwork:   10 * time.Second,\n\t\t\t},\n\t\t}\n\n\t\toriginValidator := &MockOriginValidator{}\n\n\t\tfor _, tc := range testCases {\n\t\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\t\tcfg := &config.Config{\n\t\t\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\t\t\tWebSocket: tc.websocketTimeout,\n\t\t\t\t\t\tNetwork:   tc.networkTimeout,\n\t\t\t\t\t},\n\t\t\t\t}\n\n\t\t\t\tmanager := NewWebSocketManager(originValidator, nil, cfg)\n\t\t\t\tdefer manager.Shutdown(nil)\n\n\t\t\t\twsTimeout := manager.getWebSocketTimeout()\n\t\t\t\tnetworkTimeout := manager.getNetworkTimeout()\n\t\t\t\t\n\t\t\t\tassert.Equal(t, tc.expectedWS, wsTimeout, \"WebSocket timeout mismatch\")\n\t\t\t\tassert.Equal(t, tc.expectedNetwork, networkTimeout, \"Network timeout mismatch\")\n\t\t\t})\n\t\t}\n\t})\n\n\tt.Run(\"multiple config parameters\", func(t *testing.T) {\n\t\t// Test multiple config parameters - should use the first one\n\t\tcfg1 := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tWebSocket: 30 * time.Second,\n\t\t\t\tNetwork:   5 * time.Second,\n\t\t\t},\n\t\t}\n\t\tcfg2 := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tWebSocket: 120 * time.Second,\n\t\t\t\tNetwork:   25 * time.Second,\n\t\t\t},\n\t\t}\n\n\t\toriginValidator := &MockOriginValidator{}\n\t\tmanager := NewWebSocketManager(originValidator, nil, cfg1, cfg2)\n\t\tdefer manager.Shutdown(nil)\n\n\t\twsTimeout := manager.getWebSocketTimeout()\n\t\tnetworkTimeout := manager.getNetworkTimeout()\n\t\t\n\t\tassert.Equal(t, 30*time.Second, wsTimeout, \"Should use first config for WebSocket timeout\")\n\t\tassert.Equal(t, 5*time.Second, networkTimeout, \"Should use first config for network timeout\")\n\t})\n}",
      "structuredPatch": [
        {
          "oldStart": 8,
          "oldLines": 12,
          "newStart": 8,
          "newLines": 6,
          "lines": [
            "   \"github.com/stretchr/testify/assert\"",
            " )",
            " ",
            "-type MockOriginValidator struct{}",
            "-",
            "-func (m *MockOriginValidator) IsAllowedOrigin(origin string) bool {",
            "-  return true",
            "-}",
            "-",
            " func TestWebSocketManagerTimeout(t *testing.T) {",
            "   t.Run(\"websocket manager respects configured websocket timeout\", func(t *testing.T) {",
            "     // Create a config with specific WebSocket timeout"
          ]
        }
      ],
      "userModified": false,
      "replaceAll": false
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Read",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/logging/logger.go",
      "limit": 50
    },
    "tool_response": {
      "type": "text",
      "file": {
        "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/logging/logger.go",
        "content": "package logging\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"io\"\n\t\"log/slog\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n)\n\n// LogLevel represents different log levels\ntype LogLevel int\n\nconst (\n\tLevelDebug LogLevel = iota\n\tLevelInfo\n\tLevelWarn\n\tLevelError\n\tLevelFatal\n)\n\n// String returns the string representation of the log level\nfunc (l LogLevel) String() string {\n\tswitch l {\n\tcase LevelDebug:\n\t\treturn \"DEBUG\"\n\tcase LevelInfo:\n\t\treturn \"INFO\"\n\tcase LevelWarn:\n\t\treturn \"WARN\"\n\tcase LevelError:\n\t\treturn \"ERROR\"\n\tcase LevelFatal:\n\t\treturn \"FATAL\"\n\tdefault:\n\t\treturn \"UNKNOWN\"\n\t}\n}\n\n// Logger interface for structured logging\ntype Logger interface {\n\tDebug(ctx context.Context, msg string, fields ...interface{})\n\tInfo(ctx context.Context, msg string, fields ...interface{})\n\tWarn(ctx context.Context, err error, msg string, fields ...interface{})\n\tError(ctx context.Context, err error, msg string, fields ...interface{})\n\tFatal(ctx context.Context, err error, msg string, fields ...interface{})",
        "numLines": 50,
        "startLine": 1,
        "totalLines": 653
      }
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Grep",
    "tool_input": {
      "pattern": "crc32\\.Checksum\\|CRC32\\|hash.*crc",
      "path": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner",
      "output_mode": "content",
      "-B": 5,
      "-A": 5
    },
    "tool_response": {
      "mode": "content",
      "numFiles": 0,
      "filenames": [],
      "content": "",
      "numLines": 0
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "go test ./internal/server -run TestWebSocketManagerTimeout -v",
      "description": "Test WebSocket timeout functionality"
    },
    "tool_response": {
      "stdout": "=== RUN   TestWebSocketManagerTimeout\n=== RUN   TestWebSocketManagerTimeout/websocket_manager_respects_configured_websocket_timeout\n2025/07/22 10:02:13 WebSocket manager shut down successfully\n=== RUN   TestWebSocketManagerTimeout/websocket_manager_uses_default_websocket_timeout_when_no_config\n2025/07/22 10:02:13 WebSocket manager shut down successfully\n=== RUN   TestWebSocketManagerTimeout/websocket_manager_respects_configured_network_timeout\n2025/07/22 10:02:13 WebSocket manager shut down successfully\n=== RUN   TestWebSocketManagerTimeout/websocket_manager_uses_default_network_timeout_when_no_config\n2025/07/22 10:02:13 WebSocket manager shut down successfully\n=== RUN   TestWebSocketManagerTimeout/timeout_configuration_validation\n=== RUN   TestWebSocketManagerTimeout/timeout_configuration_validation/positive_timeouts\n2025/07/22 10:02:13 WebSocket manager shut down successfully\n=== RUN   TestWebSocketManagerTimeout/timeout_configuration_validation/zero_timeouts_use_defaults\n2025/07/22 10:02:13 WebSocket manager shut down successfully\n=== RUN   TestWebSocketManagerTimeout/timeout_configuration_validation/negative_timeouts_use_defaults\n2025/07/22 10:02:13 WebSocket manager shut down successfully\n=== RUN   TestWebSocketManagerTimeout/multiple_config_parameters\n2025/07/22 10:02:13 WebSocket manager shut down successfully\n--- PASS: TestWebSocketManagerTimeout (0.00s)\n    --- PASS: TestWebSocketManagerTimeout/websocket_manager_respects_configured_websocket_timeout (0.00s)\n    --- PASS: TestWebSocketManagerTimeout/websocket_manager_uses_default_websocket_timeout_when_no_config (0.00s)\n    --- PASS: TestWebSocketManagerTimeout/websocket_manager_respects_configured_network_timeout (0.00s)\n    --- PASS: TestWebSocketManagerTimeout/websocket_manager_uses_default_network_timeout_when_no_config (0.00s)\n    --- PASS: TestWebSocketManagerTimeout/timeout_configuration_validation (0.00s)\n        --- PASS: TestWebSocketManagerTimeout/timeout_configuration_validation/positive_timeouts (0.00s)\n        --- PASS: TestWebSocketManagerTimeout/timeout_configuration_validation/zero_timeouts_use_defaults (0.00s)\n        --- PASS: TestWebSocketManagerTimeout/timeout_configuration_validation/negative_timeouts_use_defaults (0.00s)\n    --- PASS: TestWebSocketManagerTimeout/multiple_config_parameters (0.00s)\nPASS\nok  \tgithub.com/conneroisu/templar/internal/server\t0.004s",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "grep -n \"^func New\" /home/connerohnesorge/Documents/001Repos/templar/internal/logging/logger.go",
      "description": "Find available logger constructors"
    },
    "tool_response": {
      "stdout": "86:func NewLogger(config *LoggerConfig) *TemplarLogger {\n247:func NewFileLogger(config *LoggerConfig, logDir string) (*FileLogger, error) {\n307:func NewMultiLogger(loggers ...Logger) *MultiLogger {\n535:func NewStructuredError(category ErrorCategory, operation, message string) *StructuredError {\n588:func NewResilientLogger(logger Logger, maxRetries int, retryDelay time.Duration) *ResilientLogger {",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Grep",
    "tool_input": {
      "pattern": "hash.*=\\|Checksum\\|crcTable",
      "path": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner",
      "output_mode": "content",
      "-B": 3,
      "-A": 3
    },
    "tool_response": {
      "mode": "content",
      "numFiles": 0,
      "filenames": [],
      "content": "",
      "numLines": 0
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "TodoWrite",
    "tool_input": {
      "todos": [
        {
          "content": "Analyze current timeout handling patterns in the codebase",
          "status": "completed",
          "priority": "high",
          "id": "1"
        },
        {
          "content": "Design comprehensive timeout configuration system",
          "status": "completed",
          "priority": "high",
          "id": "2"
        },
        {
          "content": "Implement context-based timeouts for build operations",
          "status": "completed",
          "priority": "high",
          "id": "3"
        },
        {
          "content": "Add timeout handling to file scanning operations",
          "status": "completed",
          "priority": "high",
          "id": "4"
        },
        {
          "content": "Improve WebSocket connection timeout management",
          "status": "completed",
          "priority": "medium",
          "id": "5"
        },
        {
          "content": "Add configurable timeout values through config system",
          "status": "completed",
          "priority": "medium",
          "id": "6"
        },
        {
          "content": "Add timeout tests and validation",
          "status": "completed",
          "priority": "medium",
          "id": "8"
        },
        {
          "content": "Implement proper cancellation handling in build pipeline",
          "status": "pending",
          "priority": "high",
          "id": "7"
        }
      ]
    },
    "tool_response": {
      "oldTodos": [
        {
          "content": "Analyze current timeout handling patterns in the codebase",
          "status": "completed",
          "priority": "high",
          "id": "1"
        },
        {
          "content": "Design comprehensive timeout configuration system",
          "status": "completed",
          "priority": "high",
          "id": "2"
        },
        {
          "content": "Implement context-based timeouts for build operations",
          "status": "completed",
          "priority": "high",
          "id": "3"
        },
        {
          "content": "Add timeout handling to file scanning operations",
          "status": "completed",
          "priority": "high",
          "id": "4"
        },
        {
          "content": "Improve WebSocket connection timeout management",
          "status": "pending",
          "priority": "medium",
          "id": "5"
        },
        {
          "content": "Add configurable timeout values through config system",
          "status": "completed",
          "priority": "medium",
          "id": "6"
        },
        {
          "content": "Implement proper cancellation handling in build pipeline",
          "status": "pending",
          "priority": "high",
          "id": "7"
        },
        {
          "content": "Add timeout tests and validation",
          "status": "completed",
          "priority": "medium",
          "id": "8"
        }
      ],
      "newTodos": [
        {
          "content": "Analyze current timeout handling patterns in the codebase",
          "status": "completed",
          "priority": "high",
          "id": "1"
        },
        {
          "content": "Design comprehensive timeout configuration system",
          "status": "completed",
          "priority": "high",
          "id": "2"
        },
        {
          "content": "Implement context-based timeouts for build operations",
          "status": "completed",
          "priority": "high",
          "id": "3"
        },
        {
          "content": "Add timeout handling to file scanning operations",
          "status": "completed",
          "priority": "high",
          "id": "4"
        },
        {
          "content": "Improve WebSocket connection timeout management",
          "status": "completed",
          "priority": "medium",
          "id": "5"
        },
        {
          "content": "Add configurable timeout values through config system",
          "status": "completed",
          "priority": "medium",
          "id": "6"
        },
        {
          "content": "Add timeout tests and validation",
          "status": "completed",
          "priority": "medium",
          "id": "8"
        },
        {
          "content": "Implement proper cancellation handling in build pipeline",
          "status": "pending",
          "priority": "high",
          "id": "7"
        }
      ]
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "grep -A 10 \"type LoggerConfig\" /home/connerohnesorge/Documents/001Repos/templar/internal/logging/logger.go",
      "description": "Find LoggerConfig structure"
    },
    "tool_response": {
      "stdout": "type LoggerConfig struct {\n\tLevel      LogLevel\n\tFormat     string // \"json\" or \"text\"\n\tOutput     io.Writer\n\tTimeFormat string\n\tAddSource  bool\n\tComponent  string\n}\n\n// DefaultConfig returns default logger configuration\nfunc DefaultConfig() *LoggerConfig {",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Grep",
    "tool_input": {
      "pattern": "hash\\|Hash",
      "path": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/scanner.go",
      "output_mode": "content",
      "-B": 2,
      "-A": 2
    },
    "tool_response": {
      "mode": "content",
      "numFiles": 0,
      "filenames": [],
      "content": "",
      "numLines": 0
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Read",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
      "offset": 233,
      "limit": 50
    },
    "tool_response": {
      "type": "text",
      "file": {
        "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
        "content": "\n// ClearCache clears the build cache\nfunc (bp *BuildPipeline) ClearCache() {\n\tbp.cache.Clear()\n}\n\n// GetCacheStats returns cache statistics\nfunc (bp *BuildPipeline) GetCacheStats() (int, int64, int64) {\n\treturn bp.cache.GetStats()\n}\n\n// getBuildTimeout returns the configured timeout for build operations\nfunc (bp *BuildPipeline) getBuildTimeout() time.Duration {\n\tif bp.config != nil && bp.config.Timeouts.Build > 0 {\n\t\treturn bp.config.Timeouts.Build\n\t}\n\t// Default fallback timeout if no configuration is available\n\treturn 5 * time.Minute\n}\n\n// worker processes build tasks\nfunc (bp *BuildPipeline) worker(ctx context.Context) {\n\tdefer bp.workerWg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase task := <-bp.queue.priority:\n\t\t\tbp.processBuildTask(ctx, task)\n\t\tcase task := <-bp.queue.tasks:\n\t\t\tbp.processBuildTask(ctx, task)\n\t\t}\n\t}\n}\n\nfunc (bp *BuildPipeline) processBuildTask(ctx context.Context, task BuildTask) {\n\tstart := time.Now()\n\n\t// Generate content hash for caching\n\tcontentHash := bp.generateContentHash(task.Component)\n\n\t// Check cache first\n\tif result, found := bp.cache.Get(contentHash); found {\n\t\t// Use object pool for cache hit result\n\t\tbuildResult := bp.objectPools.GetBuildResult()\n\t\tbuildResult.Component = task.Component\n\t\tbuildResult.Output = result\n\t\tbuildResult.Error = nil\n\t\tbuildResult.ParsedErrors = nil",
        "numLines": 50,
        "startLine": 233,
        "totalLines": 798
      }
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Read",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/scanner.go",
      "offset": 900,
      "limit": 20
    },
    "tool_response": {
      "type": "text",
      "file": {
        "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/scanner.go",
        "content": "\t\tif s.metrics != nil {\n\t\t\tatomic.AddInt64(&s.metrics.CacheHits, 1)\n\t\t}\n\t\t\n\t\t// Register all cached components with the registry\n\t\tfor _, component := range cachedMetadata.Components {\n\t\t\t// Update file modification time to current scan time\n\t\t\tupdatedComponent := *component\n\t\t\tupdatedComponent.LastMod = info.ModTime()\n\t\t\tupdatedComponent.Hash = hash\n\t\t\ts.registry.Register(&updatedComponent)\n\t\t}\n\t\t\n\t\t// Track components found\n\t\tif s.metrics != nil {\n\t\t\tatomic.AddInt64(&s.metrics.ComponentsFound, int64(len(cachedMetadata.Components)))\n\t\t}\n\t\t\n\t\treturn nil\n\t}",
        "numLines": 20,
        "startLine": 900,
        "totalLines": 1498
      }
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Read",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
      "offset": 290,
      "limit": 40
    },
    "tool_response": {
      "type": "text",
      "file": {
        "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
        "content": "\t\t\t// Cache hit result successfully queued\n\t\tdefault:\n\t\t\t// Results queue full - this could cause result loss\n\t\t\tfmt.Printf(\"Warning: Results queue full, dropping cache hit result for component %s\\n\", buildResult.Component.Name)\n\t\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full_cache_hit\")\n\t\t}\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\t}\n\n\t// Create timeout context for build operation based on configuration\n\tbuildTimeout := bp.getBuildTimeout()\n\tbuildCtx, cancel := context.WithTimeout(ctx, buildTimeout)\n\tdefer cancel()\n\n\t// Execute build with pooled output buffer and context-based timeout\n\toutput, err := bp.compiler.CompileWithPools(buildCtx, task.Component, bp.objectPools)\n\n\t// Parse errors if build failed\n\tvar parsedErrors []*errors.ParsedError\n\tif err != nil {\n\t\t// Wrap the error with build context for better debugging\n\t\terr = errors.WrapBuild(err, errors.ErrCodeBuildFailed, \n\t\t\t\"component compilation failed\", task.Component.Name).\n\t\t\tWithLocation(task.Component.FilePath, 0, 0)\n\t\tparsedErrors = bp.errorParser.ParseError(string(output))\n\t}\n\n\t// Use object pool for build result\n\tbuildResult := bp.objectPools.GetBuildResult()\n\tbuildResult.Component = task.Component\n\tbuildResult.Output = output\n\tbuildResult.Error = err\n\tbuildResult.ParsedErrors = parsedErrors\n\tbuildResult.Duration = time.Since(start)\n\tbuildResult.CacheHit = false\n\tbuildResult.Hash = contentHash\n\n\t// Cache successful builds\n\tif err == nil {",
        "numLines": 40,
        "startLine": 290,
        "totalLines": 798
      }
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "grep -n \"crc32.Checksum\\|strconv.FormatUint\" /home/connerohnesorge/Documents/001Repos/templar/internal/scanner/scanner.go",
      "description": "Find hash calculation in scanner"
    },
    "tool_response": {
      "stdout": "895:\thash := strconv.FormatUint(uint64(crc32.Checksum(content, crcTable)), 16)",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Read",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/cmd/audit.go",
      "offset": 108,
      "limit": 15
    },
    "tool_response": {
      "type": "text",
      "file": {
        "filePath": "/home/connerohnesorge/Documents/001Repos/templar/cmd/audit.go",
        "content": "\t}\n\n\t// Initialize logging\n\tlogger := logging.NewLogger(\"audit\")\n\tif auditQuiet {\n\t\tlogger = logging.NewQuietLogger()\n\t} else if auditVerbose {\n\t\tlogger = logging.NewVerboseLogger()\n\t}\n\n\t// Load configuration\n\tcfg, err := config.LoadConfig()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to load configuration: %w\", err)\n\t}",
        "numLines": 15,
        "startLine": 108,
        "totalLines": 879
      }
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Read",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
      "offset": 151,
      "limit": 30
    },
    "tool_response": {
      "type": "text",
      "file": {
        "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
        "content": "\t// Start workers\n\tfor i := 0; i < bp.workers; i++ {\n\t\tbp.workerWg.Add(1)\n\t\tgo bp.worker(ctx)\n\t}\n\n\t// Start result processor\n\tbp.resultWg.Add(1)\n\tgo bp.processResults(ctx)\n}\n\n// Stop stops the build pipeline and waits for all goroutines to finish\nfunc (bp *BuildPipeline) Stop() {\n\tif bp.cancel != nil {\n\t\tbp.cancel()\n\t}\n\n\t// Wait for all workers to finish\n\tbp.workerWg.Wait()\n\n\t// Wait for result processor to finish\n\tbp.resultWg.Wait()\n}\n\n// Build queues a component for building\nfunc (bp *BuildPipeline) Build(component *types.ComponentInfo) {\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  1,\n\t\tTimestamp: time.Now(),",
        "numLines": 30,
        "startLine": 151,
        "totalLines": 798
      }
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Read",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/scanner.go",
      "offset": 890,
      "limit": 20
    },
    "tool_response": {
      "type": "text",
      "file": {
        "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/scanner.go",
        "content": "\tif err != nil {\n\t\treturn fmt.Errorf(\"reading file %s: %w\", cleanPath, err)\n\t}\n\n\t// Calculate file hash for cache lookup and change detection\n\thash := strconv.FormatUint(uint64(crc32.Checksum(content, crcTable)), 16)\n\t\n\t// Check cache first - avoid expensive parsing if metadata is cached\n\tif cachedMetadata, found := s.getCachedMetadata(cleanPath, hash); found {\n\t\t// Track cache hit\n\t\tif s.metrics != nil {\n\t\t\tatomic.AddInt64(&s.metrics.CacheHits, 1)\n\t\t}\n\t\t\n\t\t// Register all cached components with the registry\n\t\tfor _, component := range cachedMetadata.Components {\n\t\t\t// Update file modification time to current scan time\n\t\t\tupdatedComponent := *component\n\t\t\tupdatedComponent.LastMod = info.ModTime()\n\t\t\tupdatedComponent.Hash = hash",
        "numLines": 20,
        "startLine": 890,
        "totalLines": 1498
      }
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Edit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/cmd/audit.go",
      "old_string": "\t// Initialize logging\n\tlogger := logging.NewLogger(\"audit\")\n\tif auditQuiet {\n\t\tlogger = logging.NewQuietLogger()\n\t} else if auditVerbose {\n\t\tlogger = logging.NewVerboseLogger()\n\t}",
      "new_string": "\t// Initialize logging\n\tloggerConfig := &logging.LoggerConfig{\n\t\tLevel:     logging.LevelInfo,\n\t\tFormat:    \"text\", \n\t\tComponent: \"audit\",\n\t\tOutput:    os.Stdout,\n\t}\n\tif auditQuiet {\n\t\tloggerConfig.Level = logging.LevelError\n\t} else if auditVerbose {\n\t\tloggerConfig.Level = logging.LevelDebug\n\t}\n\tlogger := logging.NewLogger(loggerConfig)"
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/cmd/audit.go",
      "oldString": "\t// Initialize logging\n\tlogger := logging.NewLogger(\"audit\")\n\tif auditQuiet {\n\t\tlogger = logging.NewQuietLogger()\n\t} else if auditVerbose {\n\t\tlogger = logging.NewVerboseLogger()\n\t}",
      "newString": "\t// Initialize logging\n\tloggerConfig := &logging.LoggerConfig{\n\t\tLevel:     logging.LevelInfo,\n\t\tFormat:    \"text\", \n\t\tComponent: \"audit\",\n\t\tOutput:    os.Stdout,\n\t}\n\tif auditQuiet {\n\t\tloggerConfig.Level = logging.LevelError\n\t} else if auditVerbose {\n\t\tloggerConfig.Level = logging.LevelDebug\n\t}\n\tlogger := logging.NewLogger(loggerConfig)",
      "originalFile": "package cmd\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/accessibility\"\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/conneroisu/templar/internal/interfaces\"\n\t\"github.com/conneroisu/templar/internal/logging\"\n\t\"github.com/conneroisu/templar/internal/registry\"\n\t\"github.com/conneroisu/templar/internal/renderer\"\n\t\"github.com/conneroisu/templar/internal/scanner\"\n\t\"github.com/spf13/cobra\"\n)\n\nvar (\n\tauditComponentName   string\n\tauditWCAGLevel       string\n\tauditOutputFormat    string\n\tauditOutputFile      string\n\tauditIncludeHTML     bool\n\tauditFixableOnly     bool\n\tauditSeverityFilter  string\n\tauditQuiet           bool\n\tauditVerbose         bool\n\tauditMaxViolations   int\n\tauditGenerateReport  bool\n\tauditShowSuggestions bool\n\tauditAutoFix         bool\n\tauditShowGuidance    bool\n\tauditGuidanceOnly    bool\n)\n\n// auditCmd represents the audit command\nvar auditCmd = &cobra.Command{\n\tUse:   \"audit [component-name]\",\n\tShort: \"Run accessibility audit on components\",\n\tLong: `Run comprehensive accessibility audits on templ components to identify\nWCAG compliance issues and get actionable suggestions for improvements.\n\nThe audit command can test individual components or all components in your project.\nIt provides detailed reports with severity levels, WCAG criteria mapping, and\nspecific suggestions for fixing accessibility issues.\n\nExamples:\n  # Audit all components\n  templar audit\n\n  # Audit specific component\n  templar audit Button\n\n  # Audit with specific WCAG level\n  templar audit --wcag-level AA\n\n  # Generate HTML report\n  templar audit --output html --output-file report.html\n\n  # Show only critical issues\n  templar audit --severity error\n\n  # Apply automatic fixes\n  templar audit --auto-fix`,\n\tValidArgsFunction: func(cmd *cobra.Command, args []string, toComplete string) ([]string, cobra.ShellCompDirective) {\n\t\treturn getComponentCompletions(toComplete), cobra.ShellCompDirectiveNoFileComp\n\t},\n\tRunE: runAuditCommand,\n}\n\nfunc init() {\n\trootCmd.AddCommand(auditCmd)\n\n\tauditCmd.Flags().StringVarP(&auditComponentName, \"component\", \"c\", \"\", \"Specific component to audit (if not provided as argument)\")\n\tauditCmd.Flags().StringVarP(&auditWCAGLevel, \"wcag-level\", \"w\", \"AA\", \"WCAG compliance level to test against (A, AA, AAA)\")\n\tauditCmd.Flags().StringVarP(&auditOutputFormat, \"output\", \"o\", \"console\", \"Output format (console, json, html, markdown)\")\n\tauditCmd.Flags().StringVarP(&auditOutputFile, \"output-file\", \"f\", \"\", \"Output file path (stdout if not specified)\")\n\tauditCmd.Flags().BoolVar(&auditIncludeHTML, \"include-html\", false, \"Include HTML snapshot in report\")\n\tauditCmd.Flags().BoolVar(&auditFixableOnly, \"fixable-only\", false, \"Show only issues that can be automatically fixed\")\n\tauditCmd.Flags().StringVarP(&auditSeverityFilter, \"severity\", \"s\", \"\", \"Filter by severity level (error, warning, info)\")\n\tauditCmd.Flags().BoolVarP(&auditQuiet, \"quiet\", \"q\", false, \"Suppress non-error output\")\n\tauditCmd.Flags().BoolVarP(&auditVerbose, \"verbose\", \"v\", false, \"Enable verbose output\")\n\tauditCmd.Flags().IntVarP(&auditMaxViolations, \"max-violations\", \"m\", 0, \"Maximum number of violations to report (0 = unlimited)\")\n\tauditCmd.Flags().BoolVar(&auditGenerateReport, \"generate-report\", false, \"Generate detailed accessibility report\")\n\tauditCmd.Flags().BoolVar(&auditShowSuggestions, \"show-suggestions\", true, \"Include suggestions in output\")\n\tauditCmd.Flags().BoolVar(&auditAutoFix, \"auto-fix\", false, \"Attempt to automatically fix issues where possible\")\n\tauditCmd.Flags().BoolVar(&auditShowGuidance, \"show-guidance\", false, \"Include detailed accessibility guidance\")\n\tauditCmd.Flags().BoolVar(&auditGuidanceOnly, \"guidance-only\", false, \"Show only guidance without running audit\")\n}\n\nfunc runAuditCommand(cmd *cobra.Command, args []string) error {\n\tctx := context.Background()\n\n\t// Determine component name from args or flag\n\tcomponentName := auditComponentName\n\tif len(args) > 0 {\n\t\tcomponentName = args[0]\n\t}\n\n\t// Handle guidance-only mode\n\tif auditGuidanceOnly {\n\t\treturn showGuidanceOnly(componentName)\n\t}\n\n\t// Initialize logging\n\tlogger := logging.NewLogger(\"audit\")\n\tif auditQuiet {\n\t\tlogger = logging.NewQuietLogger()\n\t} else if auditVerbose {\n\t\tlogger = logging.NewVerboseLogger()\n\t}\n\n\t// Load configuration\n\tcfg, err := config.LoadConfig()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to load configuration: %w\", err)\n\t}\n\n\t// Initialize component registry and scanner\n\tcomponentRegistry := registry.NewComponentRegistry(logger)\n\tcomponentScanner := scanner.NewScanner(logger)\n\n\t// Scan components\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Scanning components...\")\n\t}\n\n\tcomponents, err := componentScanner.ScanDirectory(cfg.Components.ScanPaths[0])\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to scan components: %w\", err)\n\t}\n\n\t// Register components\n\tfor _, component := range components {\n\t\tcomponentRegistry.RegisterComponent(component)\n\t}\n\n\t// Create renderer\n\tcomponentRenderer := renderer.NewComponentRenderer(logger)\n\n\t// Initialize accessibility tester\n\ttesterConfig := accessibility.TesterConfig{\n\t\tDefaultWCAGLevel:    parseWCAGLevel(auditWCAGLevel),\n\t\tDefaultTimeout:      30 * time.Second,\n\t\tEnableRealTimeWarn:  false,\n\t\tMaxConcurrentTests:  1,\n\t}\n\n\ttester := accessibility.NewComponentAccessibilityTester(\n\t\tcomponentRegistry,\n\t\tcomponentRenderer,\n\t\tlogger,\n\t\ttesterConfig,\n\t)\n\n\t// Perform audit\n\tif componentName != \"\" {\n\t\treturn runSingleComponentAudit(ctx, tester, componentName, logger)\n\t} else {\n\t\treturn runAllComponentsAudit(ctx, tester, componentRegistry, logger)\n\t}\n}\n\nfunc runSingleComponentAudit(ctx context.Context, tester accessibility.AccessibilityTester, componentName string, logger logging.Logger) error {\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Running accessibility audit\", \"component\", componentName)\n\t}\n\n\t// Run accessibility test\n\treport, err := tester.TestComponent(ctx, componentName, nil)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"accessibility audit failed for %s: %w\", componentName, err)\n\t}\n\n\t// Apply filters\n\treport = applyReportFilters(report)\n\n\t// Apply auto-fixes if requested\n\tif auditAutoFix && len(report.Violations) > 0 {\n\t\tfixedCount, err := applyAutoFixes(ctx, tester, report)\n\t\tif err != nil {\n\t\t\tlogger.Warn(ctx, err, \"Failed to apply auto-fixes\")\n\t\t} else if fixedCount > 0 {\n\t\t\tlogger.Info(ctx, \"Applied automatic fixes\", \"count\", fixedCount)\n\t\t}\n\t}\n\n\t// Output results\n\treturn outputAuditResults([]*accessibility.AccessibilityReport{report}, logger)\n}\n\nfunc runAllComponentsAudit(ctx context.Context, tester accessibility.AccessibilityTester, registry interfaces.ComponentRegistry, logger logging.Logger) error {\n\tcomponents := registry.GetAllComponents()\n\t\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Running accessibility audit on all components\", \"count\", len(components))\n\t}\n\n\treports := []*accessibility.AccessibilityReport{}\n\ttotalViolations := 0\n\ttotalAutoFixes := 0\n\n\tfor i, component := range components {\n\t\tif auditVerbose {\n\t\t\tlogger.Info(ctx, \"Auditing component\", \n\t\t\t\t\"component\", component.Name,\n\t\t\t\t\"progress\", fmt.Sprintf(\"%d/%d\", i+1, len(components)))\n\t\t}\n\n\t\t// Run accessibility test\n\t\treport, err := tester.TestComponent(ctx, component.Name, nil)\n\t\tif err != nil {\n\t\t\tlogger.Warn(ctx, err, \"Failed to audit component\", \"component\", component.Name)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Apply filters\n\t\treport = applyReportFilters(report)\n\t\t\n\t\t// Apply auto-fixes if requested\n\t\tif auditAutoFix && len(report.Violations) > 0 {\n\t\t\tfixedCount, err := applyAutoFixes(ctx, tester, report)\n\t\t\tif err != nil {\n\t\t\t\tlogger.Warn(ctx, err, \"Failed to apply auto-fixes\", \"component\", component.Name)\n\t\t\t} else {\n\t\t\t\ttotalAutoFixes += fixedCount\n\t\t\t}\n\t\t}\n\n\t\treports = append(reports, report)\n\t\ttotalViolations += len(report.Violations)\n\t}\n\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Audit completed\",\n\t\t\t\"components\", len(reports),\n\t\t\t\"total_violations\", totalViolations)\n\t\t\n\t\tif auditAutoFix && totalAutoFixes > 0 {\n\t\t\tlogger.Info(ctx, \"Applied automatic fixes\", \"total_fixes\", totalAutoFixes)\n\t\t}\n\t}\n\n\t// Output results\n\treturn outputAuditResults(reports, logger)\n}\n\nfunc applyReportFilters(report *accessibility.AccessibilityReport) *accessibility.AccessibilityReport {\n\tfilteredViolations := []accessibility.AccessibilityViolation{}\n\n\tfor _, violation := range report.Violations {\n\t\t// Apply severity filter\n\t\tif auditSeverityFilter != \"\" {\n\t\t\texpectedSeverity := parseSeverity(auditSeverityFilter)\n\t\t\tif violation.Severity != expectedSeverity {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// Apply fixable filter\n\t\tif auditFixableOnly && !violation.CanAutoFix {\n\t\t\tcontinue\n\t\t}\n\n\t\tfilteredViolations = append(filteredViolations, violation)\n\t}\n\n\t// Apply max violations limit\n\tif auditMaxViolations > 0 && len(filteredViolations) > auditMaxViolations {\n\t\tfilteredViolations = filteredViolations[:auditMaxViolations]\n\t}\n\n\t// Update report\n\treport.Violations = filteredViolations\n\t\n\t// Recalculate summary\n\treport.Summary = calculateAccessibilitySummary(filteredViolations, report.Passed)\n\n\treturn report\n}\n\nfunc applyAutoFixes(ctx context.Context, tester accessibility.AccessibilityTester, report *accessibility.AccessibilityReport) (int, error) {\n\tif componentTester, ok := tester.(*accessibility.ComponentAccessibilityTester); ok {\n\t\tautoFixableViolations := []accessibility.AccessibilityViolation{}\n\t\tfor _, violation := range report.Violations {\n\t\t\tif violation.CanAutoFix {\n\t\t\t\tautoFixableViolations = append(autoFixableViolations, violation)\n\t\t\t}\n\t\t}\n\n\t\tif len(autoFixableViolations) == 0 {\n\t\t\treturn 0, nil\n\t\t}\n\n\t\t// Apply auto-fixes (this would need integration with file system)\n\t\t_, err := componentTester.AutoFix(ctx, report.HTMLSnapshot, autoFixableViolations)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\n\t\treturn len(autoFixableViolations), nil\n\t}\n\n\treturn 0, fmt.Errorf(\"auto-fix not supported for this tester type\")\n}\n\nfunc outputAuditResults(reports []*accessibility.AccessibilityReport, logger logging.Logger) error {\n\tswitch auditOutputFormat {\n\tcase \"json\":\n\t\treturn outputJSON(reports)\n\tcase \"html\":\n\t\treturn outputHTML(reports)\n\tcase \"markdown\":\n\t\treturn outputMarkdown(reports)\n\tcase \"console\":\n\t\tfallthrough\n\tdefault:\n\t\treturn outputConsole(reports, logger)\n\t}\n}\n\nfunc outputJSON(reports []*accessibility.AccessibilityReport) error {\n\toutput := reports\n\tif len(reports) == 1 {\n\t\toutput = reports[0] // Single component audit returns single report\n\t}\n\n\tjsonData, err := json.MarshalIndent(output, \"\", \"  \")\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to marshal JSON: %w\", err)\n\t}\n\n\treturn writeOutput(string(jsonData))\n}\n\nfunc outputHTML(reports []*accessibility.AccessibilityReport) error {\n\thtml := generateHTMLReport(reports)\n\treturn writeOutput(html)\n}\n\nfunc outputMarkdown(reports []*accessibility.AccessibilityReport) error {\n\tmarkdown := generateMarkdownReport(reports)\n\treturn writeOutput(markdown)\n}\n\nfunc outputConsole(reports []*accessibility.AccessibilityReport, logger logging.Logger) error {\n\tctx := context.Background()\n\t\n\tif len(reports) == 0 {\n\t\tfmt.Println(\"No components audited.\")\n\t\treturn nil\n\t}\n\n\t// Summary statistics\n\ttotalComponents := len(reports)\n\ttotalViolations := 0\n\tcriticalViolations := 0\n\tcomponentsWithIssues := 0\n\toverallScoreSum := 0.0\n\n\tfor _, report := range reports {\n\t\ttotalViolations += len(report.Violations)\n\t\toverallScoreSum += report.Summary.OverallScore\n\t\t\n\t\tif len(report.Violations) > 0 {\n\t\t\tcomponentsWithIssues++\n\t\t}\n\t\t\n\t\tfor _, violation := range report.Violations {\n\t\t\tif violation.Impact == accessibility.ImpactCritical {\n\t\t\t\tcriticalViolations++\n\t\t\t}\n\t\t}\n\t}\n\n\taverageScore := overallScoreSum / float64(totalComponents)\n\n\t// Print summary\n\tfmt.Printf(\"\\n\ud83d\udd0d Accessibility Audit Summary\\n\")\n\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\tfmt.Printf(\"Components audited:     %d\\n\", totalComponents)\n\tfmt.Printf(\"Components with issues: %d\\n\", componentsWithIssues)\n\tfmt.Printf(\"Total violations:       %d\\n\", totalViolations)\n\tfmt.Printf(\"Critical violations:    %d\\n\", criticalViolations)\n\tfmt.Printf(\"Average score:          %.1f/100\\n\", averageScore)\n\t\n\t// Overall status\n\tvar status string\n\tvar statusIcon string\n\tif criticalViolations > 0 {\n\t\tstatus = \"CRITICAL ISSUES FOUND\"\n\t\tstatusIcon = \"\ud83d\udea8\"\n\t} else if totalViolations > 0 {\n\t\tstatus = \"ISSUES FOUND\"\n\t\tstatusIcon = \"\u26a0\ufe0f\"\n\t} else {\n\t\tstatus = \"ALL CHECKS PASSED\"\n\t\tstatusIcon = \"\u2705\"\n\t}\n\t\n\tfmt.Printf(\"Status:                 %s %s\\n\\n\", statusIcon, status)\n\n\t// Detailed component results\n\tif auditVerbose || len(reports) == 1 {\n\t\tfor _, report := range reports {\n\t\t\toutputComponentDetails(report)\n\t\t}\n\t} else if totalViolations > 0 {\n\t\t// Show only components with issues\n\t\tfmt.Printf(\"Components with accessibility issues:\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tfor _, report := range reports {\n\t\t\tif len(report.Violations) > 0 {\n\t\t\t\toutputComponentSummary(report)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Show suggestions if enabled\n\tif auditShowSuggestions && totalViolations > 0 {\n\t\tfmt.Printf(\"\\n\ud83d\udca1 Top Suggestions\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tsuggestions := aggregateSuggestions(reports)\n\t\tfor i, suggestion := range suggestions {\n\t\t\tif i >= 5 { // Limit to top 5\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfmt.Printf(\"%d. %s\\n\", i+1, suggestion.Title)\n\t\t\tif suggestion.Description != \"\" {\n\t\t\t\tfmt.Printf(\"   %s\\n\", suggestion.Description)\n\t\t\t}\n\t\t\tfmt.Printf(\"\\n\")\n\t\t}\n\t}\n\t\n\t// Show detailed guidance if enabled and there are violations\n\tif totalViolations > 0 {\n\t\tallViolations := []accessibility.AccessibilityViolation{}\n\t\tfor _, report := range reports {\n\t\t\tallViolations = append(allViolations, report.Violations...)\n\t\t}\n\t\tshowGuidanceForViolations(allViolations)\n\t}\n\n\treturn nil\n}\n\nfunc outputComponentDetails(report *accessibility.AccessibilityReport) {\n\tcomponentName := report.ComponentName\n\tif componentName == \"\" {\n\t\tcomponentName = \"Unknown Component\"\n\t}\n\n\tscoreColor := getScoreColor(report.Summary.OverallScore)\n\t\n\tfmt.Printf(\"\ud83d\udce6 %s (Score: %s%.1f/100%s)\\n\", \n\t\tcomponentName, scoreColor, report.Summary.OverallScore, \"\\033[0m\")\n\tfmt.Printf(\"   File: %s\\n\", report.ComponentFile)\n\t\n\tif len(report.Violations) == 0 {\n\t\tfmt.Printf(\"   \u2705 No accessibility issues found\\n\\n\")\n\t\treturn\n\t}\n\n\t// Group violations by severity\n\terrorViolations := []accessibility.AccessibilityViolation{}\n\twarningViolations := []accessibility.AccessibilityViolation{}\n\tinfoViolations := []accessibility.AccessibilityViolation{}\n\n\tfor _, violation := range report.Violations {\n\t\tswitch violation.Severity {\n\t\tcase accessibility.SeverityError:\n\t\t\terrorViolations = append(errorViolations, violation)\n\t\tcase accessibility.SeverityWarning:\n\t\t\twarningViolations = append(warningViolations, violation)\n\t\tcase accessibility.SeverityInfo:\n\t\t\tinfoViolations = append(infoViolations, violation)\n\t\t}\n\t}\n\n\t// Output violations by severity\n\tif len(errorViolations) > 0 {\n\t\tfmt.Printf(\"   \ud83d\udea8 Errors (%d):\\n\", len(errorViolations))\n\t\tfor _, violation := range errorViolations {\n\t\t\toutputViolation(violation, \"     \")\n\t\t}\n\t}\n\t\n\tif len(warningViolations) > 0 {\n\t\tfmt.Printf(\"   \u26a0\ufe0f  Warnings (%d):\\n\", len(warningViolations))\n\t\tfor _, violation := range warningViolations {\n\t\t\toutputViolation(violation, \"     \")\n\t\t}\n\t}\n\t\n\tif len(infoViolations) > 0 && auditVerbose {\n\t\tfmt.Printf(\"   \u2139\ufe0f  Info (%d):\\n\", len(infoViolations))\n\t\tfor _, violation := range infoViolations {\n\t\t\toutputViolation(violation, \"     \")\n\t\t}\n\t}\n\n\tfmt.Printf(\"\\n\")\n}\n\nfunc outputComponentSummary(report *accessibility.AccessibilityReport) {\n\tcomponentName := report.ComponentName\n\tif componentName == \"\" {\n\t\tcomponentName = \"Unknown Component\"\n\t}\n\n\terrorCount := 0\n\twarningCount := 0\n\tcriticalCount := 0\n\n\tfor _, violation := range report.Violations {\n\t\tswitch violation.Severity {\n\t\tcase accessibility.SeverityError:\n\t\t\terrorCount++\n\t\tcase accessibility.SeverityWarning:\n\t\t\twarningCount++\n\t\t}\n\t\t\n\t\tif violation.Impact == accessibility.ImpactCritical {\n\t\t\tcriticalCount++\n\t\t}\n\t}\n\n\tscoreColor := getScoreColor(report.Summary.OverallScore)\n\t\n\tfmt.Printf(\"\ud83d\udce6 %s %s(%.1f/100)%s\\n\", componentName, scoreColor, report.Summary.OverallScore, \"\\033[0m\")\n\t\n\tif criticalCount > 0 {\n\t\tfmt.Printf(\"   \ud83d\udea8 %d critical issue(s)\\n\", criticalCount)\n\t}\n\tif errorCount > 0 {\n\t\tfmt.Printf(\"   \u274c %d error(s)\\n\", errorCount)\n\t}\n\tif warningCount > 0 {\n\t\tfmt.Printf(\"   \u26a0\ufe0f  %d warning(s)\\n\", warningCount)\n\t}\n\t\n\tfmt.Printf(\"\\n\")\n}\n\nfunc outputViolation(violation accessibility.AccessibilityViolation, indent string) {\n\tfmt.Printf(\"%s\u2022 %s\\n\", indent, violation.Message)\n\tfmt.Printf(\"%s  Rule: %s | WCAG: %s %s\\n\", \n\t\tindent, violation.Rule, violation.WCAG.Level, violation.WCAG.Criteria)\n\t\n\tif violation.Element != \"\" {\n\t\tfmt.Printf(\"%s  Element: <%s>\\n\", indent, violation.Element)\n\t}\n\t\n\tif auditShowSuggestions && len(violation.Suggestions) > 0 {\n\t\tfmt.Printf(\"%s  \ud83d\udca1 %s\\n\", indent, violation.Suggestions[0].Title)\n\t\tif violation.Suggestions[0].Code != \"\" && auditVerbose {\n\t\t\tfmt.Printf(\"%s     Code: %s\\n\", indent, violation.Suggestions[0].Code)\n\t\t}\n\t}\n\t\n\tif violation.CanAutoFix {\n\t\tfmt.Printf(\"%s  \ud83d\udd27 Auto-fixable\\n\", indent)\n\t}\n\t\n\tfmt.Printf(\"\\n\")\n}\n\nfunc writeOutput(content string) error {\n\tif auditOutputFile != \"\" {\n\t\t// Ensure output directory exists\n\t\tdir := filepath.Dir(auditOutputFile)\n\t\tif err := os.MkdirAll(dir, 0755); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to create output directory: %w\", err)\n\t\t}\n\t\t\n\t\t// Write to file\n\t\tif err := os.WriteFile(auditOutputFile, []byte(content), 0644); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to write output file: %w\", err)\n\t\t}\n\t\t\n\t\tfmt.Printf(\"Report written to: %s\\n\", auditOutputFile)\n\t\treturn nil\n\t}\n\t\n\t// Write to stdout\n\tfmt.Print(content)\n\treturn nil\n}\n\n// Helper functions\nfunc parseWCAGLevel(level string) accessibility.WCAGLevel {\n\tswitch strings.ToUpper(level) {\n\tcase \"A\":\n\t\treturn accessibility.WCAGLevelA\n\tcase \"AA\":\n\t\treturn accessibility.WCAGLevelAA\n\tcase \"AAA\":\n\t\treturn accessibility.WCAGLevelAAA\n\tdefault:\n\t\treturn accessibility.WCAGLevelAA\n\t}\n}\n\nfunc parseSeverity(severity string) accessibility.ViolationSeverity {\n\tswitch strings.ToLower(severity) {\n\tcase \"error\":\n\t\treturn accessibility.SeverityError\n\tcase \"warning\":\n\t\treturn accessibility.SeverityWarning\n\tcase \"info\":\n\t\treturn accessibility.SeverityInfo\n\tdefault:\n\t\treturn accessibility.SeverityWarning\n\t}\n}\n\nfunc getScoreColor(score float64) string {\n\tif score >= 90 {\n\t\treturn \"\\033[32m\" // Green\n\t} else if score >= 70 {\n\t\treturn \"\\033[33m\" // Yellow\n\t} else {\n\t\treturn \"\\033[31m\" // Red\n\t}\n}\n\nfunc aggregateSuggestions(reports []*accessibility.AccessibilityReport) []accessibility.AccessibilitySuggestion {\n\tsuggestionMap := make(map[string]*accessibility.AccessibilitySuggestion)\n\tsuggestionCounts := make(map[string]int)\n\t\n\tfor _, report := range reports {\n\t\tfor _, violation := range report.Violations {\n\t\t\tfor _, suggestion := range violation.Suggestions {\n\t\t\t\tkey := fmt.Sprintf(\"%s_%s\", suggestion.Type, suggestion.Title)\n\t\t\t\tsuggestionCounts[key]++\n\t\t\t\t\n\t\t\t\tif existing, exists := suggestionMap[key]; !exists || suggestion.Priority < existing.Priority {\n\t\t\t\t\tsuggestionCopy := suggestion\n\t\t\t\t\tsuggestionMap[key] = &suggestionCopy\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t\n\t// Convert to slice and sort by frequency and priority\n\tsuggestions := []accessibility.AccessibilitySuggestion{}\n\tfor key, suggestion := range suggestionMap {\n\t\t// Adjust priority based on frequency (more frequent = higher priority)\n\t\tsuggestion.Priority -= suggestionCounts[key] // Lower number = higher priority\n\t\tsuggestions = append(suggestions, *suggestion)\n\t}\n\t\n\t// Sort by priority\n\tsort.Slice(suggestions, func(i, j int) bool {\n\t\treturn suggestions[i].Priority < suggestions[j].Priority\n\t})\n\t\n\treturn suggestions\n}\n\nfunc calculateAccessibilitySummary(violations []accessibility.AccessibilityViolation, passed []accessibility.AccessibilityRule) accessibility.AccessibilitySummary {\n\tsummary := accessibility.AccessibilitySummary{\n\t\tTotalRules:      len(passed) + len(violations),\n\t\tPassedRules:     len(passed),\n\t\tFailedRules:     len(violations),\n\t\tTotalViolations: len(violations),\n\t}\n\t\n\tfor _, violation := range violations {\n\t\tswitch violation.Severity {\n\t\tcase accessibility.SeverityError:\n\t\t\tsummary.ErrorViolations++\n\t\tcase accessibility.SeverityWarning:\n\t\t\tsummary.WarnViolations++\n\t\tcase accessibility.SeverityInfo:\n\t\t\tsummary.InfoViolations++\n\t\t}\n\t\t\n\t\tswitch violation.Impact {\n\t\tcase accessibility.ImpactCritical:\n\t\t\tsummary.CriticalImpact++\n\t\tcase accessibility.ImpactSerious:\n\t\t\tsummary.SeriousImpact++\n\t\tcase accessibility.ImpactModerate:\n\t\t\tsummary.ModerateImpact++\n\t\tcase accessibility.ImpactMinor:\n\t\t\tsummary.MinorImpact++\n\t\t}\n\t}\n\t\n\t// Calculate overall score\n\tif summary.TotalRules > 0 {\n\t\tsummary.OverallScore = float64(summary.PassedRules) / float64(summary.TotalRules) * 100\n\t}\n\t\n\treturn summary\n}\n\nfunc generateHTMLReport(reports []*accessibility.AccessibilityReport) string {\n\t// This would generate a comprehensive HTML report\n\t// For brevity, returning a simplified version\n\thtml := `<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Accessibility Audit Report</title>\n    <style>\n        body { font-family: Arial, sans-serif; margin: 40px; }\n        .header { background: #f5f5f5; padding: 20px; border-radius: 8px; }\n        .component { margin: 20px 0; padding: 20px; border: 1px solid #ddd; border-radius: 8px; }\n        .violation { margin: 10px 0; padding: 10px; background: #fff3cd; border-left: 4px solid #ffc107; }\n        .error { background: #f8d7da; border-left-color: #dc3545; }\n        .success { background: #d4edda; border-left-color: #28a745; }\n    </style>\n</head>\n<body>\n    <div class=\"header\">\n        <h1>Accessibility Audit Report</h1>\n        <p>Generated on: ` + time.Now().Format(\"2006-01-02 15:04:05\") + `</p>\n    </div>`\n\n\tfor _, report := range reports {\n\t\thtml += fmt.Sprintf(`\n    <div class=\"component\">\n        <h2>%s</h2>\n        <p>Score: %.1f/100</p>\n        <p>Violations: %d</p>\n    </div>`, report.ComponentName, report.Summary.OverallScore, len(report.Violations))\n\t}\n\n\thtml += `\n</body>\n</html>`\n\n\treturn html\n}\n\nfunc generateMarkdownReport(reports []*accessibility.AccessibilityReport) string {\n\tmd := fmt.Sprintf(\"# Accessibility Audit Report\\n\\nGenerated on: %s\\n\\n\", time.Now().Format(\"2006-01-02 15:04:05\"))\n\t\n\tfor _, report := range reports {\n\t\tmd += fmt.Sprintf(\"## %s\\n\\n\", report.ComponentName)\n\t\tmd += fmt.Sprintf(\"- **Score**: %.1f/100\\n\", report.Summary.OverallScore)\n\t\tmd += fmt.Sprintf(\"- **Violations**: %d\\n\\n\", len(report.Violations))\n\t\t\n\t\tif len(report.Violations) > 0 {\n\t\t\tmd += \"### Issues Found\\n\\n\"\n\t\t\tfor _, violation := range report.Violations {\n\t\t\t\tmd += fmt.Sprintf(\"- **%s**: %s\\n\", violation.Rule, violation.Message)\n\t\t\t}\n\t\t\tmd += \"\\n\"\n\t\t}\n\t}\n\t\n\treturn md\n}\n\nfunc getComponentCompletions(toComplete string) []string {\n\t// This would integrate with the component registry to provide completions\n\t// For now, returning empty slice\n\treturn []string{}\n}\n\n// showGuidanceOnly displays accessibility guidance without running an audit\nfunc showGuidanceOnly(componentName string) error {\n\tguide := accessibility.NewAccessibilityGuide()\n\t\n\tif componentName != \"\" {\n\t\t// Show component-specific guidance\n\t\tfmt.Printf(\"\ud83c\udfaf Accessibility Guidance for %s Component\\n\", componentName)\n\t\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\t\t\n\t\tguidanceText := guide.GetComponentGuidanceText(componentName)\n\t\tfmt.Print(guidanceText)\n\t\t\n\t\t// Also show general guidance applicable to all components\n\t\tfmt.Printf(\"\\n\ud83d\udccb General Accessibility Guidelines\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tquickStart := guide.GetQuickStartGuide()\n\t\tfor i, item := range quickStart {\n\t\t\tif i >= 3 { // Limit to top 3 for brevity\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfmt.Printf(\"%d. %s\\n\", i+1, item.Title)\n\t\t\tfmt.Printf(\"   %s\\n\\n\", item.Description)\n\t\t}\n\t\t\n\t} else {\n\t\t// Show general accessibility guidance\n\t\tfmt.Printf(\"\ud83c\udf1f Accessibility Quick Start Guide\\n\")\n\t\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\t\t\n\t\tquickStart := guide.GetQuickStartGuide()\n\t\tfor i, item := range quickStart {\n\t\t\tfmt.Printf(\"%d. %s\\n\", i+1, item.Title)\n\t\t\tfmt.Printf(\"   %s\\n\", item.Description)\n\t\t\t\n\t\t\tif len(item.Examples) > 0 {\n\t\t\t\texample := item.Examples[0]\n\t\t\t\tif example.BadCode != \"\" {\n\t\t\t\t\tfmt.Printf(\"   \u274c Avoid: %s\\n\", strings.ReplaceAll(example.BadCode, \"\\n\", \" \"))\n\t\t\t\t}\n\t\t\t\tif example.GoodCode != \"\" {\n\t\t\t\t\tfmt.Printf(\"   \u2705 Use: %s\\n\", strings.ReplaceAll(example.GoodCode, \"\\n\", \" \"))\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Printf(\"\\n\")\n\t\t}\n\t\t\n\t\tfmt.Printf(\"\ud83d\udca1 Advanced Guidelines\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tbestPractices := guide.GetBestPracticesGuide()\n\t\tfor i, item := range bestPractices {\n\t\t\tif i >= 3 { // Limit for readability\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfmt.Printf(\"\u2022 %s\\n\", item.Title)\n\t\t\tfmt.Printf(\"  %s\\n\\n\", item.Description)\n\t\t}\n\t\t\n\t\tfmt.Printf(\"\ud83d\udcda Additional Resources\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\")\n\t\tfmt.Printf(\"\u2022 WCAG Quick Reference: https://www.w3.org/WAI/WCAG21/quickref/\\n\")\n\t\tfmt.Printf(\"\u2022 WebAIM Guidelines: https://webaim.org/\\n\")\n\t\tfmt.Printf(\"\u2022 A11y Project: https://www.a11yproject.com/\\n\")\n\t\tfmt.Printf(\"\u2022 MDN Accessibility: https://developer.mozilla.org/en-US/docs/Web/Accessibility\\n\\n\")\n\t\t\n\t\tfmt.Printf(\"\ud83d\udd27 To audit your components, run:\\n\")\n\t\tfmt.Printf(\"   templar audit              # Audit all components\\n\")\n\t\tfmt.Printf(\"   templar audit Button       # Audit specific component\\n\")\n\t\tfmt.Printf(\"   templar audit --help        # See all options\\n\")\n\t}\n\t\n\treturn nil\n}\n\n// showGuidanceForViolations displays guidance for specific accessibility violations\nfunc showGuidanceForViolations(violations []accessibility.AccessibilityViolation) {\n\tif !auditShowGuidance || len(violations) == 0 {\n\t\treturn\n\t}\n\t\n\tfmt.Printf(\"\\n\ud83c\udf93 Accessibility Guidance\\n\")\n\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\t\n\tguide := accessibility.NewAccessibilityGuide()\n\t\n\t// Group violations by rule to avoid duplicate guidance\n\truleMap := make(map[string]bool)\n\tuniqueRules := []string{}\n\t\n\tfor _, violation := range violations {\n\t\tif !ruleMap[violation.Rule] {\n\t\t\truleMap[violation.Rule] = true\n\t\t\tuniqueRules = append(uniqueRules, violation.Rule)\n\t\t}\n\t}\n\t\n\t// Show guidance for each unique rule\n\tfor i, rule := range uniqueRules {\n\t\tif i > 0 {\n\t\t\tfmt.Printf(\"\\n\" + strings.Repeat(\"\u2500\", 60) + \"\\n\\n\")\n\t\t}\n\t\t\n\t\tguidanceText := guide.GetGuidanceText(rule)\n\t\tfmt.Print(guidanceText)\n\t}\n}",
      "structuredPatch": [
        {
          "oldStart": 108,
          "oldLines": 12,
          "newStart": 108,
          "newLines": 18,
          "lines": [
            "   }",
            " ",
            "   // Initialize logging",
            "-  logger := logging.NewLogger(\"audit\")",
            "+  loggerConfig := &logging.LoggerConfig{",
            "+    Level:     logging.LevelInfo,",
            "+    Format:    \"text\", ",
            "+    Component: \"audit\",",
            "+    Output:    os.Stdout,",
            "+  }",
            "   if auditQuiet {",
            "-    logger = logging.NewQuietLogger()",
            "+    loggerConfig.Level = logging.LevelError",
            "   } else if auditVerbose {",
            "-    logger = logging.NewVerboseLogger()",
            "+    loggerConfig.Level = logging.LevelDebug",
            "   }",
            "+  logger := logging.NewLogger(loggerConfig)",
            " ",
            "   // Load configuration",
            "   cfg, err := config.LoadConfig()"
          ]
        }
      ],
      "userModified": false,
      "replaceAll": false
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Read",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/cmd/audit.go",
      "limit": 15
    },
    "tool_response": {
      "type": "text",
      "file": {
        "filePath": "/home/connerohnesorge/Documents/001Repos/templar/cmd/audit.go",
        "content": "package cmd\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/accessibility\"\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/conneroisu/templar/internal/interfaces\"",
        "numLines": 15,
        "startLine": 1,
        "totalLines": 885
      }
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "TodoWrite",
    "tool_input": {
      "todos": [
        {
          "content": "Analyze current CRC32 hashing performance",
          "status": "completed",
          "priority": "high",
          "id": "hash-analysis"
        },
        {
          "content": "Task-105 - Implement Hash-Based Change Detection Optimization",
          "status": "in_progress",
          "priority": "high",
          "id": "hash-optimization"
        },
        {
          "content": "Design hierarchical hashing strategy",
          "status": "in_progress",
          "priority": "high",
          "id": "hierarchical-design"
        },
        {
          "content": "Implement optimized hash generation for large files",
          "status": "pending",
          "priority": "high",
          "id": "large-file-hash"
        },
        {
          "content": "Integrate with existing build cache system",
          "status": "pending",
          "priority": "high",
          "id": "cache-integration"
        },
        {
          "content": "Add performance benchmarks and fallback mechanisms",
          "status": "pending",
          "priority": "high",
          "id": "benchmarks-fallback"
        }
      ]
    },
    "tool_response": {
      "oldTodos": [
        {
          "content": "Task-105 - Implement Hash-Based Change Detection Optimization",
          "status": "in_progress",
          "priority": "high",
          "id": "hash-optimization"
        },
        {
          "content": "Analyze current CRC32 hashing performance",
          "status": "pending",
          "priority": "high",
          "id": "hash-analysis"
        },
        {
          "content": "Implement hierarchical hashing with metadata sampling",
          "status": "pending",
          "priority": "high",
          "id": "hierarchical-hash"
        },
        {
          "content": "Optimize hash generation for large template files",
          "status": "pending",
          "priority": "high",
          "id": "large-file-hash"
        },
        {
          "content": "Integrate with existing build cache system",
          "status": "pending",
          "priority": "high",
          "id": "cache-integration"
        },
        {
          "content": "Add performance benchmarks and fallback mechanisms",
          "status": "pending",
          "priority": "high",
          "id": "benchmarks-fallback"
        }
      ],
      "newTodos": [
        {
          "content": "Analyze current CRC32 hashing performance",
          "status": "completed",
          "priority": "high",
          "id": "hash-analysis"
        },
        {
          "content": "Task-105 - Implement Hash-Based Change Detection Optimization",
          "status": "in_progress",
          "priority": "high",
          "id": "hash-optimization"
        },
        {
          "content": "Design hierarchical hashing strategy",
          "status": "in_progress",
          "priority": "high",
          "id": "hierarchical-design"
        },
        {
          "content": "Implement optimized hash generation for large files",
          "status": "pending",
          "priority": "high",
          "id": "large-file-hash"
        },
        {
          "content": "Integrate with existing build cache system",
          "status": "pending",
          "priority": "high",
          "id": "cache-integration"
        },
        {
          "content": "Add performance benchmarks and fallback mechanisms",
          "status": "pending",
          "priority": "high",
          "id": "benchmarks-fallback"
        }
      ]
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Edit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
      "old_string": "\t// Check cache first\n\tif result, found := bp.cache.Get(contentHash); found {\n\t\t// Use object pool for cache hit result\n\t\tbuildResult := bp.objectPools.GetBuildResult()\n\t\tbuildResult.Component = task.Component\n\t\tbuildResult.Output = result\n\t\tbuildResult.Error = nil\n\t\tbuildResult.ParsedErrors = nil\n\t\tbuildResult.Duration = time.Since(start)\n\t\tbuildResult.CacheHit = true\n\t\tbuildResult.Hash = contentHash\n\n\t\t// Non-blocking send to results channel to prevent worker hangs\n\t\tselect {\n\t\tcase bp.queue.results <- *buildResult:\n\t\t\t// Cache hit result successfully queued\n\t\tdefault:\n\t\t\t// Results queue full - this could cause result loss\n\t\t\tfmt.Printf(\"Warning: Results queue full, dropping cache hit result for component %s\\n\", buildResult.Component.Name)\n\t\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full_cache_hit\")\n\t\t}\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\t}",
      "new_string": "\t// Check if context is cancelled before starting work\n\tselect {\n\tcase <-ctx.Done():\n\t\t// Context cancelled, return error\n\t\tbuildResult := bp.objectPools.GetBuildResult()\n\t\tbuildResult.Component = task.Component\n\t\tbuildResult.Output = nil\n\t\tbuildResult.Error = ctx.Err()\n\t\tbuildResult.ParsedErrors = nil\n\t\tbuildResult.Duration = time.Since(start)\n\t\tbuildResult.CacheHit = false\n\t\tbuildResult.Hash = contentHash\n\n\t\t// Non-blocking send to results channel\n\t\tselect {\n\t\tcase bp.queue.results <- *buildResult:\n\t\tdefault:\n\t\t\tbp.metrics.RecordDroppedResult(task.Component.Name, \"results_queue_full_cancelled\")\n\t\t}\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\tdefault:\n\t}\n\n\t// Check cache first\n\tif result, found := bp.cache.Get(contentHash); found {\n\t\t// Use object pool for cache hit result\n\t\tbuildResult := bp.objectPools.GetBuildResult()\n\t\tbuildResult.Component = task.Component\n\t\tbuildResult.Output = result\n\t\tbuildResult.Error = nil\n\t\tbuildResult.ParsedErrors = nil\n\t\tbuildResult.Duration = time.Since(start)\n\t\tbuildResult.CacheHit = true\n\t\tbuildResult.Hash = contentHash\n\n\t\t// Non-blocking send to results channel to prevent worker hangs\n\t\tselect {\n\t\tcase bp.queue.results <- *buildResult:\n\t\t\t// Cache hit result successfully queued\n\t\tcase <-ctx.Done():\n\t\t\t// Context cancelled while sending result\n\t\t\tbuildResult.Error = ctx.Err()\n\t\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\t\treturn\n\t\tdefault:\n\t\t\t// Results queue full - this could cause result loss\n\t\t\tfmt.Printf(\"Warning: Results queue full, dropping cache hit result for component %s\\n\", buildResult.Component.Name)\n\t\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full_cache_hit\")\n\t\t}\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\t}"
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
      "oldString": "\t// Check cache first\n\tif result, found := bp.cache.Get(contentHash); found {\n\t\t// Use object pool for cache hit result\n\t\tbuildResult := bp.objectPools.GetBuildResult()\n\t\tbuildResult.Component = task.Component\n\t\tbuildResult.Output = result\n\t\tbuildResult.Error = nil\n\t\tbuildResult.ParsedErrors = nil\n\t\tbuildResult.Duration = time.Since(start)\n\t\tbuildResult.CacheHit = true\n\t\tbuildResult.Hash = contentHash\n\n\t\t// Non-blocking send to results channel to prevent worker hangs\n\t\tselect {\n\t\tcase bp.queue.results <- *buildResult:\n\t\t\t// Cache hit result successfully queued\n\t\tdefault:\n\t\t\t// Results queue full - this could cause result loss\n\t\t\tfmt.Printf(\"Warning: Results queue full, dropping cache hit result for component %s\\n\", buildResult.Component.Name)\n\t\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full_cache_hit\")\n\t\t}\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\t}",
      "newString": "\t// Check if context is cancelled before starting work\n\tselect {\n\tcase <-ctx.Done():\n\t\t// Context cancelled, return error\n\t\tbuildResult := bp.objectPools.GetBuildResult()\n\t\tbuildResult.Component = task.Component\n\t\tbuildResult.Output = nil\n\t\tbuildResult.Error = ctx.Err()\n\t\tbuildResult.ParsedErrors = nil\n\t\tbuildResult.Duration = time.Since(start)\n\t\tbuildResult.CacheHit = false\n\t\tbuildResult.Hash = contentHash\n\n\t\t// Non-blocking send to results channel\n\t\tselect {\n\t\tcase bp.queue.results <- *buildResult:\n\t\tdefault:\n\t\t\tbp.metrics.RecordDroppedResult(task.Component.Name, \"results_queue_full_cancelled\")\n\t\t}\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\tdefault:\n\t}\n\n\t// Check cache first\n\tif result, found := bp.cache.Get(contentHash); found {\n\t\t// Use object pool for cache hit result\n\t\tbuildResult := bp.objectPools.GetBuildResult()\n\t\tbuildResult.Component = task.Component\n\t\tbuildResult.Output = result\n\t\tbuildResult.Error = nil\n\t\tbuildResult.ParsedErrors = nil\n\t\tbuildResult.Duration = time.Since(start)\n\t\tbuildResult.CacheHit = true\n\t\tbuildResult.Hash = contentHash\n\n\t\t// Non-blocking send to results channel to prevent worker hangs\n\t\tselect {\n\t\tcase bp.queue.results <- *buildResult:\n\t\t\t// Cache hit result successfully queued\n\t\tcase <-ctx.Done():\n\t\t\t// Context cancelled while sending result\n\t\t\tbuildResult.Error = ctx.Err()\n\t\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\t\treturn\n\t\tdefault:\n\t\t\t// Results queue full - this could cause result loss\n\t\t\tfmt.Printf(\"Warning: Results queue full, dropping cache hit result for component %s\\n\", buildResult.Component.Name)\n\t\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full_cache_hit\")\n\t\t}\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\t}",
      "originalFile": "// Package build provides a concurrent build pipeline for templ components\n// with caching, error collection, and performance metrics.\n//\n// The build pipeline processes components through worker pools, maintains\n// an LRU cache for build results, and provides real-time build status\n// through callbacks and metrics. It supports parallel execution with\n// configurable worker counts and implements security-hardened command\n// execution with proper validation.\npackage build\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"hash/crc32\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/conneroisu/templar/internal/errors\"\n\t\"github.com/conneroisu/templar/internal/interfaces\"\n\t\"github.com/conneroisu/templar/internal/types\"\n)\n\n// crcTable is a pre-computed CRC32 Castagnoli table for faster hash generation\nvar crcTable = crc32.MakeTable(crc32.Castagnoli)\n\n// BuildPipeline manages the build process for templ components with concurrent\n// execution, intelligent caching, and comprehensive error handling.\n//\n// The pipeline provides:\n// - Concurrent build execution with configurable worker pools\n// - LRU caching with CRC32-based change detection\n// - Priority-based build queue management\n// - Real-time build metrics and status callbacks\n// - Memory optimization through object pooling\n// - Security-hardened command execution\n// - Comprehensive timeout management\ntype BuildPipeline struct {\n\t// compiler handles templ compilation with security validation\n\tcompiler *TemplCompiler\n\t// cache provides LRU-based build result caching\n\tcache *BuildCache\n\t// queue manages build tasks with priority ordering\n\tqueue *BuildQueue\n\t// workers defines the number of concurrent build workers\n\tworkers int\n\t// registry provides component information and change notifications\n\tregistry interfaces.ComponentRegistry\n\t// errorParser processes build errors and provides detailed diagnostics\n\terrorParser *errors.ErrorParser\n\t// metrics tracks build performance and success rates\n\tmetrics *BuildMetrics\n\t// callbacks receive build status updates for UI integration\n\tcallbacks []BuildCallback\n\t// workerWg synchronizes worker goroutine lifecycle\n\tworkerWg sync.WaitGroup\n\t// resultWg synchronizes result processing\n\tresultWg sync.WaitGroup\n\t// cancel terminates all pipeline operations gracefully\n\tcancel context.CancelFunc\n\t// objectPools optimize memory allocation for frequently used objects\n\tobjectPools *ObjectPools\n\t// slicePools reduce slice allocation overhead\n\tslicePools *SlicePools\n\t// workerPool manages the lifecycle of build workers\n\tworkerPool *WorkerPool\n\t// config provides timeout configuration for build operations\n\tconfig *config.Config\n}\n\n// BuildTask represents a build task in the priority queue with metadata\n// for scheduling and execution tracking.\ntype BuildTask struct {\n\t// Component contains the component information to be built\n\tComponent *types.ComponentInfo\n\t// Priority determines build order (higher values built first)\n\tPriority int\n\t// Timestamp records when the task was created for ordering\n\tTimestamp time.Time\n}\n\n// BuildResult represents the result of a build operation\ntype BuildResult struct {\n\tComponent    *types.ComponentInfo\n\tOutput       []byte\n\tError        error\n\tParsedErrors []*errors.ParsedError\n\tDuration     time.Duration\n\tCacheHit     bool\n\tHash         string\n}\n\n// BuildCallback is called when a build completes\ntype BuildCallback func(result BuildResult)\n\n// BuildQueue manages build tasks\ntype BuildQueue struct {\n\ttasks    chan BuildTask\n\tresults  chan BuildResult\n\tpriority chan BuildTask\n}\n\n// NewBuildPipeline creates a new build pipeline with optional timeout configuration\nfunc NewBuildPipeline(workers int, registry interfaces.ComponentRegistry, cfg ...*config.Config) *BuildPipeline {\n\tcompiler := NewTemplCompiler()\n\tcache := NewBuildCache(100*1024*1024, time.Hour) // 100MB, 1 hour TTL\n\n\tqueue := &BuildQueue{\n\t\ttasks:    make(chan BuildTask, 100),\n\t\tresults:  make(chan BuildResult, 100),\n\t\tpriority: make(chan BuildTask, 10),\n\t}\n\n\tmetrics := NewBuildMetrics()\n\n\t// Use first config if provided, otherwise nil\n\tvar config *config.Config\n\tif len(cfg) > 0 {\n\t\tconfig = cfg[0]\n\t}\n\n\treturn &BuildPipeline{\n\t\tcompiler:    compiler,\n\t\tcache:       cache,\n\t\tqueue:       queue,\n\t\tworkers:     workers,\n\t\tregistry:    registry,\n\t\terrorParser: errors.NewErrorParser(),\n\t\tmetrics:     metrics,\n\t\tcallbacks:   make([]BuildCallback, 0),\n\t\t// Initialize object pools for memory optimization\n\t\tobjectPools: NewObjectPools(),\n\t\tslicePools:  NewSlicePools(),\n\t\tworkerPool:  NewWorkerPool(),\n\t\tconfig:      config,\n\t}\n}\n\n// Start starts the build pipeline\nfunc (bp *BuildPipeline) Start(ctx context.Context) {\n\t// Create cancellable context\n\tctx, bp.cancel = context.WithCancel(ctx)\n\n\t// Start workers\n\tfor i := 0; i < bp.workers; i++ {\n\t\tbp.workerWg.Add(1)\n\t\tgo bp.worker(ctx)\n\t}\n\n\t// Start result processor\n\tbp.resultWg.Add(1)\n\tgo bp.processResults(ctx)\n}\n\n// Stop stops the build pipeline and waits for all goroutines to finish\nfunc (bp *BuildPipeline) Stop() {\n\tif bp.cancel != nil {\n\t\tbp.cancel()\n\t}\n\n\t// Wait for all workers to finish\n\tbp.workerWg.Wait()\n\n\t// Wait for result processor to finish\n\tbp.resultWg.Wait()\n}\n\n// Build queues a component for building\nfunc (bp *BuildPipeline) Build(component *types.ComponentInfo) {\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  1,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.tasks <- task:\n\t\t// Task successfully queued\n\tdefault:\n\t\t// Queue full - implement backpressure handling\n\t\t// Log the error and update metrics\n\t\tfmt.Printf(\"Warning: Build queue full, dropping task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"task_queue_full\")\n\n\t\t// Try to handle with retry or priority queue\n\t\tselect {\n\t\tcase bp.queue.priority <- task:\n\t\t\tfmt.Printf(\"Task for %s promoted to priority queue\\n\", component.Name)\n\t\tdefault:\n\t\t\tfmt.Printf(\"Error: Both queues full, build request lost for component %s\\n\", component.Name)\n\t\t\t// TODO: Implement persistent queue or callback for dropped tasks\n\t\t}\n\t}\n}\n\n// BuildWithPriority queues a component for building with high priority\nfunc (bp *BuildPipeline) BuildWithPriority(component *types.ComponentInfo) {\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  10,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.priority <- task:\n\t\t// Priority task successfully queued\n\tdefault:\n\t\t// Priority queue also full - this is a critical error\n\t\tfmt.Printf(\"Critical: Priority queue full, dropping high-priority task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"priority_queue_full\")\n\n\t\t// Could implement emergency handling here (e.g., block briefly or expand queue)\n\t\t// For now, log the critical error\n\t}\n}\n\n// AddCallback adds a callback to be called when builds complete\nfunc (bp *BuildPipeline) AddCallback(callback BuildCallback) {\n\tbp.callbacks = append(bp.callbacks, callback)\n}\n\n// GetMetrics returns the current build metrics\nfunc (bp *BuildPipeline) GetMetrics() BuildMetrics {\n\treturn bp.metrics.GetSnapshot()\n}\n\n// ClearCache clears the build cache\nfunc (bp *BuildPipeline) ClearCache() {\n\tbp.cache.Clear()\n}\n\n// GetCacheStats returns cache statistics\nfunc (bp *BuildPipeline) GetCacheStats() (int, int64, int64) {\n\treturn bp.cache.GetStats()\n}\n\n// getBuildTimeout returns the configured timeout for build operations\nfunc (bp *BuildPipeline) getBuildTimeout() time.Duration {\n\tif bp.config != nil && bp.config.Timeouts.Build > 0 {\n\t\treturn bp.config.Timeouts.Build\n\t}\n\t// Default fallback timeout if no configuration is available\n\treturn 5 * time.Minute\n}\n\n// worker processes build tasks\nfunc (bp *BuildPipeline) worker(ctx context.Context) {\n\tdefer bp.workerWg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase task := <-bp.queue.priority:\n\t\t\tbp.processBuildTask(ctx, task)\n\t\tcase task := <-bp.queue.tasks:\n\t\t\tbp.processBuildTask(ctx, task)\n\t\t}\n\t}\n}\n\nfunc (bp *BuildPipeline) processBuildTask(ctx context.Context, task BuildTask) {\n\tstart := time.Now()\n\n\t// Generate content hash for caching\n\tcontentHash := bp.generateContentHash(task.Component)\n\n\t// Check cache first\n\tif result, found := bp.cache.Get(contentHash); found {\n\t\t// Use object pool for cache hit result\n\t\tbuildResult := bp.objectPools.GetBuildResult()\n\t\tbuildResult.Component = task.Component\n\t\tbuildResult.Output = result\n\t\tbuildResult.Error = nil\n\t\tbuildResult.ParsedErrors = nil\n\t\tbuildResult.Duration = time.Since(start)\n\t\tbuildResult.CacheHit = true\n\t\tbuildResult.Hash = contentHash\n\n\t\t// Non-blocking send to results channel to prevent worker hangs\n\t\tselect {\n\t\tcase bp.queue.results <- *buildResult:\n\t\t\t// Cache hit result successfully queued\n\t\tdefault:\n\t\t\t// Results queue full - this could cause result loss\n\t\t\tfmt.Printf(\"Warning: Results queue full, dropping cache hit result for component %s\\n\", buildResult.Component.Name)\n\t\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full_cache_hit\")\n\t\t}\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\t}\n\n\t// Create timeout context for build operation based on configuration\n\tbuildTimeout := bp.getBuildTimeout()\n\tbuildCtx, cancel := context.WithTimeout(ctx, buildTimeout)\n\tdefer cancel()\n\n\t// Execute build with pooled output buffer and context-based timeout\n\toutput, err := bp.compiler.CompileWithPools(buildCtx, task.Component, bp.objectPools)\n\n\t// Parse errors if build failed\n\tvar parsedErrors []*errors.ParsedError\n\tif err != nil {\n\t\t// Wrap the error with build context for better debugging\n\t\terr = errors.WrapBuild(err, errors.ErrCodeBuildFailed, \n\t\t\t\"component compilation failed\", task.Component.Name).\n\t\t\tWithLocation(task.Component.FilePath, 0, 0)\n\t\tparsedErrors = bp.errorParser.ParseError(string(output))\n\t}\n\n\t// Use object pool for build result\n\tbuildResult := bp.objectPools.GetBuildResult()\n\tbuildResult.Component = task.Component\n\tbuildResult.Output = output\n\tbuildResult.Error = err\n\tbuildResult.ParsedErrors = parsedErrors\n\tbuildResult.Duration = time.Since(start)\n\tbuildResult.CacheHit = false\n\tbuildResult.Hash = contentHash\n\n\t// Cache successful builds\n\tif err == nil {\n\t\tbp.cache.Set(contentHash, output)\n\t}\n\n\t// Non-blocking send to results channel to prevent worker hangs\n\tselect {\n\tcase bp.queue.results <- *buildResult:\n\t\t// Result successfully queued\n\tdefault:\n\t\t// Results queue full - this could cause result loss\n\t\tfmt.Printf(\"Warning: Results queue full, dropping result for component %s\\n\", buildResult.Component.Name)\n\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full\")\n\t}\n\tbp.objectPools.PutBuildResult(buildResult)\n}\n\nfunc (bp *BuildPipeline) processResults(ctx context.Context) {\n\tdefer bp.resultWg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase result := <-bp.queue.results:\n\t\t\tbp.handleBuildResult(result)\n\t\t}\n\t}\n}\n\nfunc (bp *BuildPipeline) handleBuildResult(result BuildResult) {\n\t// Update metrics\n\tbp.metrics.RecordBuild(result)\n\n\t// Print result\n\tif result.Error != nil {\n\t\tfmt.Printf(\"Build failed for %s: %v\\n\", result.Component.Name, result.Error)\n\t\tif len(result.ParsedErrors) > 0 {\n\t\t\tfmt.Println(\"Parsed errors:\")\n\t\t\tfor _, err := range result.ParsedErrors {\n\t\t\t\tfmt.Print(err.FormatError())\n\t\t\t}\n\t\t}\n\t} else {\n\t\tstatus := \"succeeded\"\n\t\tif result.CacheHit {\n\t\t\tstatus = \"cached\"\n\t\t}\n\t\tfmt.Printf(\"Build %s for %s in %v\\n\", status, result.Component.Name, result.Duration)\n\t}\n\n\t// Call callbacks\n\tfor _, callback := range bp.callbacks {\n\t\tcallback(result)\n\t}\n}\n\n// generateContentHash generates a hash for component content with optimized single I/O operation\nfunc (bp *BuildPipeline) generateContentHash(component *types.ComponentInfo) string {\n\t// OPTIMIZATION: Use Stat() first to get metadata without opening file\n\t// This reduces file I/O operations by 70-90% for cached files\n\tstat, err := os.Stat(component.FilePath)\n\tif err != nil {\n\t\t// File not accessible, return fallback hash\n\t\t// Note: We don't need to wrap this error as it's an internal optimization\n\t\treturn component.FilePath\n\t}\n\n\t// Create metadata-based hash key for cache lookup\n\tmetadataKey := fmt.Sprintf(\"%s:%d:%d\", component.FilePath, stat.ModTime().Unix(), stat.Size())\n\n\t// Two-tier cache system: Check metadata cache first (no file I/O)\n\tif hash, found := bp.cache.GetHash(metadataKey); found {\n\t\t// Cache hit - no file I/O needed, just return cached hash\n\t\treturn hash\n\t}\n\n\t// Cache miss: Now we need to read file content and generate hash\n\t// Only open file when we actually need to read content\n\tfile, err := os.Open(component.FilePath)\n\tif err != nil {\n\t\treturn component.FilePath\n\t}\n\tdefer file.Close()\n\n\t// Use mmap for large files (>64KB) for better performance\n\tvar content []byte\n\tif stat.Size() > 64*1024 {\n\t\t// Use mmap for large files\n\t\tcontent, err = bp.readFileWithMmap(file, stat.Size())\n\t\tif err != nil {\n\t\t\t// Fallback to regular read\n\t\t\tcontent, err = io.ReadAll(file)\n\t\t}\n\t} else {\n\t\t// Regular read for small files\n\t\tcontent, err = io.ReadAll(file)\n\t}\n\n\tif err != nil {\n\t\t// Fallback to metadata-based hash\n\t\treturn fmt.Sprintf(\"%s:%d\", component.FilePath, stat.ModTime().Unix())\n\t}\n\n\t// Generate content hash using CRC32 Castagnoli for faster file change detection\n\tcrcHash := crc32.Checksum(content, crcTable)\n\tcontentHash := strconv.FormatUint(uint64(crcHash), 16)\n\n\t// Cache the hash with metadata key for future lookups\n\tbp.cache.SetHash(metadataKey, contentHash)\n\n\treturn contentHash\n}\n\n// readFileWithMmap reads file content using memory mapping for better performance on large files\nfunc (bp *BuildPipeline) readFileWithMmap(file *os.File, size int64) ([]byte, error) {\n\t// Memory map the file for efficient reading\n\tmmap, err := syscall.Mmap(int(file.Fd()), 0, int(size), syscall.PROT_READ, syscall.MAP_SHARED)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Copy the mapped data to avoid keeping the mapping open\n\tcontent := make([]byte, size)\n\tcopy(content, mmap)\n\n\t// Unmap the memory\n\tif err := syscall.Munmap(mmap); err != nil {\n\t\t// Log warning but don't fail - we have the content\n\t\t// Could add logging here if logger is available\n\t}\n\n\treturn content, nil\n}\n\n// generateContentHashesBatch processes multiple components in a single batch for better I/O efficiency\nfunc (bp *BuildPipeline) generateContentHashesBatch(components []*types.ComponentInfo) map[string]string {\n\tresults := make(map[string]string, len(components))\n\n\t// Group components by whether they need content reading (cache misses)\n\tvar needsReading []*types.ComponentInfo\n\n\t// First pass: check metadata-based cache for all components (no file I/O)\n\tfor _, component := range components {\n\t\t// OPTIMIZATION: Use efficient Stat() + metadata cache check first\n\t\tif stat, err := os.Stat(component.FilePath); err == nil {\n\t\t\tmetadataKey := fmt.Sprintf(\"%s:%d:%d\", component.FilePath, stat.ModTime().Unix(), stat.Size())\n\n\t\t\t// Check cache with metadata key\n\t\t\tif hash, found := bp.cache.GetHash(metadataKey); found {\n\t\t\t\t// Cache hit - no file reading needed\n\t\t\t\tresults[component.FilePath] = hash\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// Cache miss - needs content reading\n\t\tneedsReading = append(needsReading, component)\n\t}\n\n\t// Second pass: batch process cache misses with optimized I/O\n\tif len(needsReading) > 0 {\n\t\thashResults := bp.batchReadAndHash(needsReading)\n\t\tfor filePath, hash := range hashResults {\n\t\t\tresults[filePath] = hash\n\t\t}\n\t}\n\n\treturn results\n}\n\n// batchReadAndHash reads and hashes multiple files efficiently\nfunc (bp *BuildPipeline) batchReadAndHash(components []*types.ComponentInfo) map[string]string {\n\tresults := make(map[string]string, len(components))\n\n\t// Process each component with optimized I/O\n\tfor _, component := range components {\n\t\thash := bp.generateContentHash(component)\n\t\tresults[component.FilePath] = hash\n\t}\n\n\treturn results\n}\n\n// FileDiscoveryResult represents the result of discovering files in a directory\ntype FileDiscoveryResult struct {\n\tFiles       []*types.ComponentInfo\n\tErrors      []error\n\tDuration    time.Duration\n\tDiscovered  int64\n\tSkipped     int64\n}\n\n// FileDiscoveryStats tracks file discovery performance metrics\ntype FileDiscoveryStats struct {\n\tTotalFiles     int64\n\tProcessedFiles int64\n\tSkippedFiles   int64\n\tErrors         int64\n\tDuration       time.Duration\n\tWorkerCount    int\n}\n\n// ParallelFileProcessor provides parallel file processing capabilities\ntype ParallelFileProcessor struct {\n\tworkerCount int\n\tmaxDepth    int\n\tfilters     []string\n\tstats       *FileDiscoveryStats\n}\n\n// NewParallelFileProcessor creates a new parallel file processor\nfunc NewParallelFileProcessor(workerCount int) *ParallelFileProcessor {\n\treturn &ParallelFileProcessor{\n\t\tworkerCount: workerCount,\n\t\tmaxDepth:    10, // Default max depth\n\t\tfilters:     []string{\".templ\"},\n\t\tstats:       &FileDiscoveryStats{},\n\t}\n}\n\n// DiscoverFiles discovers component files in parallel using filepath.WalkDir\nfunc (pfp *ParallelFileProcessor) DiscoverFiles(ctx context.Context, rootPaths []string) (*FileDiscoveryResult, error) {\n\tstart := time.Now()\n\tdefer func() {\n\t\tpfp.stats.Duration = time.Since(start)\n\t}()\n\n\t// Create channels for work distribution\n\tpathCh := make(chan string, len(rootPaths))\n\tresultCh := make(chan *types.ComponentInfo, 100)\n\terrorCh := make(chan error, 100)\n\n\t// Start workers\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < pfp.workerCount; i++ {\n\t\twg.Add(1)\n\t\tgo pfp.worker(ctx, pathCh, resultCh, errorCh, &wg)\n\t}\n\n\t// Send root paths to workers\n\tgo func() {\n\t\tdefer close(pathCh)\n\t\tfor _, path := range rootPaths {\n\t\t\tselect {\n\t\t\tcase pathCh <- path:\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Collect results\n\tvar files []*types.ComponentInfo\n\tvar errors []error\n\tvar discovered, skipped int64\n\n\t// Result collection goroutine\n\tdone := make(chan struct{})\n\tgo func() {\n\t\tdefer close(done)\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase component, ok := <-resultCh:\n\t\t\t\tif !ok {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tfiles = append(files, component)\n\t\t\t\tatomic.AddInt64(&discovered, 1)\n\t\t\tcase err, ok := <-errorCh:\n\t\t\t\tif !ok {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\terrors = append(errors, err)\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Wait for workers to complete\n\twg.Wait()\n\tclose(resultCh)\n\tclose(errorCh)\n\n\t// Wait for result collection to complete\n\t<-done\n\n\t// Update stats\n\tatomic.StoreInt64(&pfp.stats.ProcessedFiles, discovered)\n\tatomic.StoreInt64(&pfp.stats.SkippedFiles, skipped)\n\tatomic.StoreInt64(&pfp.stats.Errors, int64(len(errors)))\n\tpfp.stats.WorkerCount = pfp.workerCount\n\n\treturn &FileDiscoveryResult{\n\t\tFiles:      files,\n\t\tErrors:     errors,\n\t\tDuration:   time.Since(start),\n\t\tDiscovered: discovered,\n\t\tSkipped:    skipped,\n\t}, nil\n}\n\n// worker processes file discovery work\nfunc (pfp *ParallelFileProcessor) worker(ctx context.Context, pathCh <-chan string, resultCh chan<- *types.ComponentInfo, errorCh chan<- error, wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase rootPath, ok := <-pathCh:\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Walk directory tree using filepath.WalkDir for better performance\n\t\t\terr := filepath.WalkDir(rootPath, func(path string, d os.DirEntry, err error) error {\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\t// Skip directories\n\t\t\t\tif d.IsDir() {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\n\t\t\t\t// Check if file matches our filters\n\t\t\t\tif !pfp.matchesFilter(path) {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\n\t\t\t\t// Create component info\n\t\t\t\tcomponent := &types.ComponentInfo{\n\t\t\t\t\tName:       pfp.extractComponentName(path),\n\t\t\t\t\tFilePath:   path,\n\t\t\t\t\tPackage:    pfp.extractPackage(path),\n\t\t\t\t\tParameters: []types.ParameterInfo{},\n\t\t\t\t}\n\n\t\t\t\t// Send result non-blocking\n\t\t\t\tselect {\n\t\t\t\tcase resultCh <- component:\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn ctx.Err()\n\t\t\t\tdefault:\n\t\t\t\t\t// Channel full, skip this component\n\t\t\t\t\tatomic.AddInt64(&pfp.stats.SkippedFiles, 1)\n\t\t\t\t}\n\n\t\t\t\treturn nil\n\t\t\t})\n\n\t\t\tif err != nil {\n\t\t\t\tselect {\n\t\t\t\tcase errorCh <- err:\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\tdefault:\n\t\t\t\t\t// Error channel full, skip error\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// matchesFilter checks if a file path matches the processor's filters\nfunc (pfp *ParallelFileProcessor) matchesFilter(path string) bool {\n\tfor _, filter := range pfp.filters {\n\t\tif strings.HasSuffix(path, filter) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// extractComponentName extracts component name from file path\nfunc (pfp *ParallelFileProcessor) extractComponentName(path string) string {\n\tbase := filepath.Base(path)\n\text := filepath.Ext(base)\n\treturn strings.TrimSuffix(base, ext)\n}\n\n// extractPackage extracts package name from file path\nfunc (pfp *ParallelFileProcessor) extractPackage(path string) string {\n\tdir := filepath.Dir(path)\n\treturn filepath.Base(dir)\n}\n\n// ProcessFilesBatch processes multiple files in parallel batches\nfunc (bp *BuildPipeline) ProcessFilesBatch(ctx context.Context, components []*types.ComponentInfo, batchSize int) (*FileDiscoveryResult, error) {\n\tstart := time.Now()\n\tvar totalDiscovered, totalSkipped int64\n\tvar allErrors []error\n\tvar allResults []*types.ComponentInfo\n\n\t// Process components in batches\n\tfor i := 0; i < len(components); i += batchSize {\n\t\tend := i + batchSize\n\t\tif end > len(components) {\n\t\t\tend = len(components)\n\t\t}\n\n\t\tbatch := components[i:end]\n\t\thashes := bp.generateContentHashesBatch(batch)\n\n\t\t// Process batch with caching\n\t\tfor _, component := range batch {\n\t\t\thash, exists := hashes[component.FilePath]\n\t\t\tif !exists {\n\t\t\t\tatomic.AddInt64(&totalSkipped, 1)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check cache first\n\t\t\tif _, found := bp.cache.Get(hash); found {\n\t\t\t\t// Cache hit, no processing needed\n\t\t\t\tallResults = append(allResults, component)\n\t\t\t\tatomic.AddInt64(&totalDiscovered, 1)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Process component\n\t\t\tallResults = append(allResults, component)\n\t\t\tatomic.AddInt64(&totalDiscovered, 1)\n\t\t}\n\t}\n\n\treturn &FileDiscoveryResult{\n\t\tFiles:      allResults,\n\t\tErrors:     allErrors,\n\t\tDuration:   time.Since(start),\n\t\tDiscovered: totalDiscovered,\n\t\tSkipped:    totalSkipped,\n\t}, nil\n}\n\n// BuildDirectory builds all components in a directory using parallel processing\nfunc (bp *BuildPipeline) BuildDirectory(ctx context.Context, rootPath string) error {\n\t// Create parallel file processor\n\tprocessor := NewParallelFileProcessor(bp.workers)\n\n\t// Discover files\n\tresult, err := processor.DiscoverFiles(ctx, []string{rootPath})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to discover files: %w\", err)\n\t}\n\n\t// Queue all discovered components for building\n\tfor _, component := range result.Files {\n\t\tbp.Build(component)\n\t}\n\n\tfmt.Printf(\"Directory build queued: %d components discovered in %v\\n\", \n\t\tresult.Discovered, result.Duration)\n\n\treturn nil\n}\n\n// GetFileDiscoveryStats returns file discovery performance statistics\nfunc (pfp *ParallelFileProcessor) GetFileDiscoveryStats() FileDiscoveryStats {\n\treturn FileDiscoveryStats{\n\t\tTotalFiles:     atomic.LoadInt64(&pfp.stats.TotalFiles),\n\t\tProcessedFiles: atomic.LoadInt64(&pfp.stats.ProcessedFiles),\n\t\tSkippedFiles:   atomic.LoadInt64(&pfp.stats.SkippedFiles),\n\t\tErrors:         atomic.LoadInt64(&pfp.stats.Errors),\n\t\tDuration:       pfp.stats.Duration,\n\t\tWorkerCount:    pfp.stats.WorkerCount,\n\t}\n}\n",
      "structuredPatch": [
        {
          "oldStart": 272,
          "oldLines": 6,
          "newStart": 272,
          "newLines": 30,
          "lines": [
            "   // Generate content hash for caching",
            "   contentHash := bp.generateContentHash(task.Component)",
            " ",
            "+  // Check if context is cancelled before starting work",
            "+  select {",
            "+  case <-ctx.Done():",
            "+    // Context cancelled, return error",
            "+    buildResult := bp.objectPools.GetBuildResult()",
            "+    buildResult.Component = task.Component",
            "+    buildResult.Output = nil",
            "+    buildResult.Error = ctx.Err()",
            "+    buildResult.ParsedErrors = nil",
            "+    buildResult.Duration = time.Since(start)",
            "+    buildResult.CacheHit = false",
            "+    buildResult.Hash = contentHash",
            "+",
            "+    // Non-blocking send to results channel",
            "+    select {",
            "+    case bp.queue.results <- *buildResult:",
            "+    default:",
            "+      bp.metrics.RecordDroppedResult(task.Component.Name, \"results_queue_full_cancelled\")",
            "+    }",
            "+    bp.objectPools.PutBuildResult(buildResult)",
            "+    return",
            "+  default:",
            "+  }",
            "+",
            "   // Check cache first",
            "   if result, found := bp.cache.Get(contentHash); found {",
            "     // Use object pool for cache hit result"
          ]
        },
        {
          "oldStart": 288,
          "oldLines": 6,
          "newStart": 312,
          "newLines": 11,
          "lines": [
            "     select {",
            "     case bp.queue.results <- *buildResult:",
            "       // Cache hit result successfully queued",
            "+    case <-ctx.Done():",
            "+      // Context cancelled while sending result",
            "+      buildResult.Error = ctx.Err()",
            "+      bp.objectPools.PutBuildResult(buildResult)",
            "+      return",
            "     default:",
            "       // Results queue full - this could cause result loss",
            "       fmt.Printf(\"Warning: Results queue full, dropping cache hit result for component %s\\n\", buildResult.Component.Name)"
          ]
        }
      ],
      "userModified": false,
      "replaceAll": false
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "grep -n \"^func.*Config\" /home/connerohnesorge/Documents/001Repos/templar/internal/config/*.go",
      "description": "Find available config functions"
    },
    "tool_response": {
      "stdout": "/home/connerohnesorge/Documents/001Repos/templar/internal/config/builder.go:40:func NewConfigBuilder() *ConfigBuilder {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/builder.go:49:func (cb *ConfigBuilder) WithBasicSettings() *ConfigBuilder {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/builder.go:71:func (cb *ConfigBuilder) WithDevelopmentMode() *ConfigBuilder {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/builder.go:98:func (cb *ConfigBuilder) WithProductionOptimizations() *ConfigBuilder {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/builder.go:148:func (cb *ConfigBuilder) WithEnterpriseFeatures() *ConfigBuilder {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/builder.go:183:func (cb *ConfigBuilder) WithCustomServer(port int, host string) *ConfigBuilder {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/builder.go:191:func (cb *ConfigBuilder) WithComponentPaths(paths ...string) *ConfigBuilder {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/builder.go:198:func (cb *ConfigBuilder) WithCSS(framework string, config *CSSConfig) *ConfigBuilder {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/builder.go:225:func (cb *ConfigBuilder) WithPlugins(enabled []string, discoveryPaths []string) *ConfigBuilder {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/builder.go:236:func (cb *ConfigBuilder) WithEnvironment(env string) *ConfigBuilder {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/builder.go:261:func (cb *ConfigBuilder) FromViper() *ConfigBuilder {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/builder.go:270:func (cb *ConfigBuilder) AddValidator(validator ValidatorFunc) *ConfigBuilder {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/builder.go:276:func (cb *ConfigBuilder) Build() (*Config, error) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/builder.go:296:func (cb *ConfigBuilder) GetTier() ConfigTier {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/builder.go:301:func (cb *ConfigBuilder) addValidator(err error) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/builder.go:310:func (cb *ConfigBuilder) mergeViperConfig(viperConfig *Config) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/builder.go:325:func (cb *ConfigBuilder) applyViperWorkarounds() {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config_fuzz_test.go:16:func FuzzLoadConfig(f *testing.F) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config_fuzz_test.go:96:func FuzzConfigValidation(f *testing.F) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config_fuzz_test.go:236:func ValidateConfig(config *Config) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config.go:452:func loadDefaults(config *Config) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config.go:544:func loadProductionDefaults(prod *ProductionConfig) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config.go:756:func loadTimeoutDefaults(timeouts *TimeoutConfig) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config.go:826:func applyOverrides(config *Config) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config.go:913:func Load() (*Config, error) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config.go:934:func (c *Config) GetEnvironmentConfig(env string) (*EnvironmentConfig, bool) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config.go:943:func (c *Config) ApplyEnvironmentOverrides(env string) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config.go:987:func (c *Config) GetProductionConfig(env string) (*ProductionConfig, error) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config_property_test.go:19:func TestConfigurationProperties(t *testing.T) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config_property_test.go:97:func TestServerConfigProperties(t *testing.T) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config_property_test.go:153:func TestComponentsConfigProperties(t *testing.T) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config_property_test.go:223:func TestBuildConfigProperties(t *testing.T) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config_property_test.go:279:func TestConfigMergingProperties(t *testing.T) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config_property_test.go:360:func validateConfig(cfg *Config) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config_property_test.go:373:func validateServerConfig(cfg *ServerConfig) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config_property_test.go:386:func validateComponentsConfig(cfg *ComponentsConfig) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config_property_test.go:398:func validateBuildConfig(cfg *BuildConfig) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config_property_test.go:413:func getDefaultConfig() *Config {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config_property_test.go:436:func mergeConfigs(base, override *Config) *Config {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config_test.go:86:func TestConfigStructure(t *testing.T) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/config_test.go:145:func TestConfigDefaults(t *testing.T) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/security_test.go:11:func TestValidateServerConfig_Security(t *testing.T) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/security_test.go:124:func TestValidateBuildConfig_Security(t *testing.T) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/security_test.go:193:func TestValidateComponentsConfig_Security(t *testing.T) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/security_test.go:332:func TestSecurityRegression_ConfigSecurity(t *testing.T) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:63:func NewConfigValidator() *ConfigValidator {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:70:func (cv *ConfigValidator) ValidateAll(config *Config) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:87:func (cv *ConfigValidator) validateServer(config *ServerConfig) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:111:func (cv *ConfigValidator) validateAuth(config *AuthConfig) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:142:func (cv *ConfigValidator) validateBuild(config *BuildConfig) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:169:func (cv *ConfigValidator) validateComponents(config *ComponentsConfig) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:190:func (cv *ConfigValidator) validatePlugins(config *PluginsConfig) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:207:func (cv *ConfigValidator) validateMonitoring(config *MonitoringConfig) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:234:func (cv *ConfigValidator) validateProduction(config *ProductionConfig) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:257:func (cv *ConfigValidator) validateCompression(config *CompressionSettings) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:284:func (cv *ConfigValidator) validateSecurity(config *SecuritySettings) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:300:func (cv *ConfigValidator) validateDeployment(config *DeploymentSettings) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:320:func (cv *ConfigValidator) validateHostSecurity(host string) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:331:func (cv *ConfigValidator) validateSecurePath(path string) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:346:func (cv *ConfigValidator) validatePath(path string) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:370:func (cv *ConfigValidator) validateIPAddress(ip string) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:390:func (cv *ConfigValidator) addError(field string, err error) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:395:func (cv *ConfigValidator) contains(slice []string, item string) bool {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:405:func (cv *ConfigValidator) combineErrors() error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:419:func ValidateConfigWithDetails(config *Config) *ValidationResult {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:450:func validateServerConfigDetails(config *ServerConfig, result *ValidationResult) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:462:func validateBuildConfigDetails(config *BuildConfig, result *ValidationResult) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:472:func validateComponentsConfigDetails(config *ComponentsConfig, result *ValidationResult) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:482:func validatePreviewConfigDetails(config *PreviewConfig, result *ValidationResult) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:496:func validateDevelopmentConfigDetails(config *DevelopmentConfig, result *ValidationResult) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:507:func validatePluginsConfigDetails(config *PluginsConfig, result *ValidationResult) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:520:func validateConfig(config *Config) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:526:func validateServerConfig(config *ServerConfig) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:535:func validateBuildConfig(config *BuildConfig) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:544:func validateComponentsConfig(config *ComponentsConfig) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:553:func validatePluginsConfig(config *PluginsConfig) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/validation.go:562:func validateMonitoringConfig(config *MonitoringConfig) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:32:func NewConfigWizard() *ConfigWizard {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:40:func NewConfigWizardWithProjectDir(projectDir string) *ConfigWizard {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:51:func (w *ConfigWizard) Run() (*Config, error) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:122:func (w *ConfigWizard) configureServer() error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:156:func (w *ConfigWizard) configureComponents() error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:226:func (w *ConfigWizard) configureBuild() error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:266:func (w *ConfigWizard) configureDevelopment() error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:279:func (w *ConfigWizard) configurePreview() error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:304:func (w *ConfigWizard) configurePlugins() error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:351:func (w *ConfigWizard) configureMonitoring() error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:378:func (w *ConfigWizard) askString(prompt, defaultValue string) string {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:405:func (w *ConfigWizard) askInt(prompt string, defaultValue, min, max int) (int, error) {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:434:func (w *ConfigWizard) askBool(prompt string, defaultValue bool) bool {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:455:func (w *ConfigWizard) askChoice(prompt string, choices []string, defaultValue string) string {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:481:func (w *ConfigWizard) WriteConfigFile(filename string) error {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:502:func (w *ConfigWizard) generateYAMLConfig() string {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:613:func (w *ConfigWizard) detectProjectStructure() {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:644:func (w *ConfigWizard) fileExists(path string) bool {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:654:func (w *ConfigWizard) hasTemplFiles() bool {\n/home/connerohnesorge/Documents/001Repos/templar/internal/config/wizard.go:675:func (w *ConfigWizard) inferProjectType() string {",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Edit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/scanner.go",
      "old_string": "// crcTable is a pre-computed CRC32 Castagnoli table for faster hash generation\nvar crcTable = crc32.MakeTable(crc32.Castagnoli)",
      "new_string": "// crcTable is a pre-computed CRC32 Castagnoli table for faster hash generation\nvar crcTable = crc32.MakeTable(crc32.Castagnoli)\n\n// Hash generation strategy constants\nconst (\n\t// Small files (< 4KB) - use full content CRC32\n\tsmallFileThreshold = 4 * 1024\n\t// Medium files (4KB - 256KB) - use content sampling\n\tmediumFileThreshold = 256 * 1024\n\t// Large files (> 256KB) - use hierarchical sampling\n\tlargeFileThreshold = 256 * 1024\n\t// Content sample size for large files\n\tcontentSampleSize = 1024\n)\n\n// FileHashStrategy represents different hashing approaches\ntype FileHashStrategy int\n\nconst (\n\tHashStrategyFull FileHashStrategy = iota\n\tHashStrategySampled\n\tHashStrategyHierarchical\n)"
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/scanner.go",
      "oldString": "// crcTable is a pre-computed CRC32 Castagnoli table for faster hash generation\nvar crcTable = crc32.MakeTable(crc32.Castagnoli)",
      "newString": "// crcTable is a pre-computed CRC32 Castagnoli table for faster hash generation\nvar crcTable = crc32.MakeTable(crc32.Castagnoli)\n\n// Hash generation strategy constants\nconst (\n\t// Small files (< 4KB) - use full content CRC32\n\tsmallFileThreshold = 4 * 1024\n\t// Medium files (4KB - 256KB) - use content sampling\n\tmediumFileThreshold = 256 * 1024\n\t// Large files (> 256KB) - use hierarchical sampling\n\tlargeFileThreshold = 256 * 1024\n\t// Content sample size for large files\n\tcontentSampleSize = 1024\n)\n\n// FileHashStrategy represents different hashing approaches\ntype FileHashStrategy int\n\nconst (\n\tHashStrategyFull FileHashStrategy = iota\n\tHashStrategySampled\n\tHashStrategyHierarchical\n)",
      "originalFile": "// Package scanner provides component discovery and analysis for templ templates.\n//\n// The scanner traverses file systems to find .templ files, parses them using\n// Go's AST parser to extract component metadata including parameters, dependencies,\n// and documentation. It integrates with the component registry to broadcast\n// change events and supports recursive directory scanning with exclude patterns.\n// The scanner maintains file hashes for change detection and provides both\n// single-file and batch scanning capabilities.\npackage scanner\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"go/ast\"\n\t\"go/parser\"\n\t\"go/token\"\n\t\"hash/crc32\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/conneroisu/templar/internal/errors\"\n\t\"github.com/conneroisu/templar/internal/interfaces\"\n\t\"github.com/conneroisu/templar/internal/registry\"\n\t\"github.com/conneroisu/templar/internal/types\"\n)\n\n// crcTable is a pre-computed CRC32 Castagnoli table for faster hash generation\nvar crcTable = crc32.MakeTable(crc32.Castagnoli)\n\n// ScanJob represents a scanning job for the worker pool containing the file\n// path to scan and a result channel for asynchronous communication.\ntype ScanJob struct {\n\t// filePath is the absolute path to the .templ file to be scanned\n\tfilePath string\n\t// result channel receives the scan result or error asynchronously\n\tresult chan<- ScanResult\n}\n\n// HashResult represents the result of asynchronous hash calculation\ntype HashResult struct {\n\thash string\n\terr  error\n}\n\n// BufferPool manages reusable byte buffers for file reading optimization\ntype BufferPool struct {\n\tpool sync.Pool\n}\n\n// NewBufferPool creates a new buffer pool with initial buffer size\nfunc NewBufferPool() *BufferPool {\n\treturn &BufferPool{\n\t\tpool: sync.Pool{\n\t\t\tNew: func() interface{} {\n\t\t\t\t// Pre-allocate 64KB buffers for typical component files\n\t\t\t\treturn make([]byte, 0, 64*1024)\n\t\t\t},\n\t\t},\n\t}\n}\n\n// Get retrieves a buffer from the pool\nfunc (bp *BufferPool) Get() []byte {\n\treturn bp.pool.Get().([]byte)[:0] // Reset length but keep capacity\n}\n\n// Put returns a buffer to the pool\nfunc (bp *BufferPool) Put(buf []byte) {\n\t// Only pool reasonably-sized buffers to avoid memory leaks\n\tif cap(buf) <= 1024*1024 { // 1MB limit\n\t\tbp.pool.Put(buf)\n\t}\n}\n\n// ScanResult represents the result of a scanning operation, containing either\n// success status or error information for a specific file.\ntype ScanResult struct {\n\t// filePath is the path that was scanned\n\tfilePath string\n\t// err contains any error that occurred during scanning, nil on success\n\terr error\n}\n\n// WorkerPool manages persistent scanning workers for performance optimization\n// using a work-stealing approach to distribute scanning jobs across CPU cores.\ntype WorkerPool struct {\n\t// jobQueue buffers scanning jobs for worker distribution\n\tjobQueue chan ScanJob\n\t// workers holds references to all active worker goroutines\n\tworkers []*ScanWorker\n\t// workerCount defines the number of concurrent workers (typically NumCPU)\n\tworkerCount int\n\t// scanner is the shared component scanner instance\n\tscanner *ComponentScanner\n\t// stop signals all workers to terminate gracefully\n\tstop chan struct{}\n\t// stopped tracks pool shutdown state\n\tstopped bool\n\t// mu protects concurrent access to pool state\n\tmu sync.RWMutex\n}\n\n// ScanWorker represents a persistent worker goroutine that processes scanning\n// jobs from the shared job queue. Each worker operates independently and\n// can handle multiple file types concurrently.\ntype ScanWorker struct {\n\t// id uniquely identifies this worker for debugging and metrics\n\tid int\n\t// jobQueue receives scanning jobs from the worker pool\n\tjobQueue <-chan ScanJob\n\t// scanner provides the component parsing functionality\n\tscanner *ComponentScanner\n\t// stop signals this worker to terminate gracefully\n\tstop chan struct{}\n}\n\n// ComponentScanner discovers and parses templ components using Go's AST parser.\n//\n// The scanner provides:\n// - Recursive directory traversal with exclude patterns\n// - AST-based component metadata extraction\n// - Concurrent processing via worker pool\n// - Integration with component registry for event broadcasting\n// - File change detection using CRC32 hashing\n// - Optimized path validation with cached working directory\n// - Buffer pooling for memory optimization in large codebases\n// - Component metadata caching with LRU eviction for performance\ntype ComponentScanner struct {\n\t// registry receives discovered components and broadcasts change events\n\tregistry *registry.ComponentRegistry\n\t// fileSet tracks file positions for AST parsing and error reporting\n\tfileSet *token.FileSet\n\t// workerPool manages concurrent scanning operations\n\tworkerPool *WorkerPool\n\t// pathCache contains cached path validation data to avoid repeated syscalls\n\tpathCache *pathValidationCache\n\t// bufferPool provides reusable byte buffers for file reading optimization\n\tbufferPool *BufferPool\n\t// metadataCache caches parsed component metadata by file hash to avoid re-parsing unchanged files\n\tmetadataCache *MetadataCache\n\t// astParsingPool provides concurrent AST parsing to avoid blocking worker threads\n\tastParsingPool *ASTParsingPool\n\t// metrics tracks performance metrics during scanning operations\n\tmetrics *ScannerMetrics\n\t// config provides timeout configuration for scanning operations\n\tconfig *config.Config\n}\n\n// Interface compliance verification - ComponentScanner implements interfaces.ComponentScanner\nvar _ interfaces.ComponentScanner = (*ComponentScanner)(nil)\n\n// ASTParseJob represents a parsing job for the AST parsing pool\ntype ASTParseJob struct {\n\tfilePath string\n\tcontent  []byte\n\tfileSet  *token.FileSet\n\tresult   chan<- ASTParseResult\n}\n\n// ASTParseResult contains the result of AST parsing\ntype ASTParseResult struct {\n\tastFile *ast.File\n\terr     error\n\tfilePath string\n}\n\n// ASTParsingPool manages concurrent AST parsing to avoid blocking worker threads\ntype ASTParsingPool struct {\n\tworkers   int\n\tjobChan   chan ASTParseJob\n\tcloseChan chan struct{}\n\twg        sync.WaitGroup\n}\n\n// NewASTParsingPool creates a new AST parsing pool with specified worker count\nfunc NewASTParsingPool(workers int) *ASTParsingPool {\n\tif workers <= 0 {\n\t\tworkers = runtime.NumCPU() / 2 // Use half CPU cores for AST parsing\n\t\tif workers < 1 {\n\t\t\tworkers = 1\n\t\t}\n\t}\n\t\n\tpool := &ASTParsingPool{\n\t\tworkers:   workers,\n\t\tjobChan:   make(chan ASTParseJob, workers*2),\n\t\tcloseChan: make(chan struct{}),\n\t}\n\t\n\t// Start worker goroutines\n\tfor i := 0; i < workers; i++ {\n\t\tpool.wg.Add(1)\n\t\tgo pool.worker()\n\t}\n\t\n\treturn pool\n}\n\n// worker processes AST parsing jobs\nfunc (p *ASTParsingPool) worker() {\n\tdefer p.wg.Done()\n\t\n\tfor {\n\t\tselect {\n\t\tcase job := <-p.jobChan:\n\t\t\t// Parse the AST\n\t\t\tastFile, err := parser.ParseFile(job.fileSet, job.filePath, job.content, parser.ParseComments)\n\t\t\t\n\t\t\t// Send result back\n\t\t\tselect {\n\t\t\tcase job.result <- ASTParseResult{\n\t\t\t\tastFile: astFile,\n\t\t\t\terr:     err,\n\t\t\t\tfilePath: job.filePath,\n\t\t\t}:\n\t\t\tcase <-p.closeChan:\n\t\t\t\treturn\n\t\t\t}\n\t\t\t\n\t\tcase <-p.closeChan:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// ParseAsync submits an AST parsing job and returns a result channel\nfunc (p *ASTParsingPool) ParseAsync(filePath string, content []byte, fileSet *token.FileSet) <-chan ASTParseResult {\n\tresult := make(chan ASTParseResult, 1)\n\t\n\t// For very large files, use optimized parsing approach\n\tif len(content) > 1024*1024 { // 1MB threshold\n\t\tgo p.parseLargeFileAsync(filePath, content, fileSet, result)\n\t\treturn result\n\t}\n\t\n\tselect {\n\tcase p.jobChan <- ASTParseJob{\n\t\tfilePath: filePath,\n\t\tcontent:  content,\n\t\tfileSet:  fileSet,\n\t\tresult:   result,\n\t}:\n\t\treturn result\n\tcase <-p.closeChan:\n\t\t// Pool is closed, return error result\n\t\tgo func() {\n\t\t\tresult <- ASTParseResult{\n\t\t\t\tastFile: nil,\n\t\t\t\terr:     fmt.Errorf(\"AST parsing pool is closed\"),\n\t\t\t\tfilePath: filePath,\n\t\t\t}\n\t\t}()\n\t\treturn result\n\t}\n}\n\n// parseLargeFileAsync handles large file parsing with memory optimization\nfunc (p *ASTParsingPool) parseLargeFileAsync(filePath string, content []byte, fileSet *token.FileSet, result chan<- ASTParseResult) {\n\tdefer close(result)\n\t\n\t// For large files, use streaming approach with limited memory usage\n\t// Parse with limited goroutines to prevent memory exhaustion\n\tastFile, err := parser.ParseFile(fileSet, filePath, content, parser.ParseComments|parser.SkipObjectResolution)\n\t\n\tresult <- ASTParseResult{\n\t\tastFile: astFile,\n\t\terr:     err,\n\t\tfilePath: filePath,\n\t}\n}\n\n// Close shuts down the AST parsing pool\nfunc (p *ASTParsingPool) Close() {\n\tclose(p.closeChan)\n\tclose(p.jobChan)\n\tp.wg.Wait()\n}\n\n// pathValidationCache caches expensive filesystem operations for optimal performance\ntype pathValidationCache struct {\n\t// mu protects concurrent access to cache fields\n\tmu sync.RWMutex\n\t// currentWorkingDir is the cached current working directory (absolute path)\n\tcurrentWorkingDir string\n\t// initialized indicates whether the cache has been populated\n\tinitialized bool\n}\n\n// CachedComponentMetadata stores pre-parsed component information for cache optimization\ntype CachedComponentMetadata struct {\n\t// Components is a slice of all components found in the file\n\tComponents []*types.ComponentInfo\n\t// FileHash is the CRC32 hash of the file content when cached\n\tFileHash string\n\t// ParsedAt records when the metadata was cached\n\tParsedAt time.Time\n}\n\n// ScannerMetrics tracks performance metrics during scanning operations\ntype ScannerMetrics struct {\n\t// FilesProcessed is the total number of files processed\n\tFilesProcessed int64\n\t// ComponentsFound is the total number of components discovered\n\tComponentsFound int64\n\t// CacheHits tracks how many files were served from cache\n\tCacheHits int64\n\t// CacheMisses tracks how many files required parsing\n\tCacheMisses int64\n\t// TotalScanTime tracks time spent in scanning operations\n\tTotalScanTime time.Duration\n\t// PeakMemoryUsage tracks the peak memory usage during scanning\n\tPeakMemoryUsage uint64\n\t// ConcurrentJobs tracks the peak number of concurrent jobs\n\tConcurrentJobs int64\n}\n\n// MetadataCache implements a simple LRU cache for component metadata\ntype MetadataCache struct {\n\tmu       sync.RWMutex\n\tentries  map[string]*MetadataCacheEntry\n\tmaxSize  int\n\tttl      time.Duration\n\t// LRU doubly-linked list\n\thead *MetadataCacheEntry\n\ttail *MetadataCacheEntry\n}\n\n// MetadataCacheEntry represents a cached metadata entry with LRU pointers\ntype MetadataCacheEntry struct {\n\tKey       string\n\tData      []byte\n\tCreatedAt time.Time\n\t// LRU pointers\n\tprev *MetadataCacheEntry\n\tnext *MetadataCacheEntry\n}\n\n// NewMetadataCache creates a new metadata cache\nfunc NewMetadataCache(maxSize int, ttl time.Duration) *MetadataCache {\n\tcache := &MetadataCache{\n\t\tentries: make(map[string]*MetadataCacheEntry),\n\t\tmaxSize: maxSize,\n\t\tttl:     ttl,\n\t}\n\t\n\t// Initialize dummy head and tail for LRU\n\tcache.head = &MetadataCacheEntry{}\n\tcache.tail = &MetadataCacheEntry{}\n\tcache.head.next = cache.tail\n\tcache.tail.prev = cache.head\n\t\n\treturn cache\n}\n\n// Get retrieves data from cache\nfunc (mc *MetadataCache) Get(key string) ([]byte, bool) {\n\tmc.mu.Lock()\n\tdefer mc.mu.Unlock()\n\t\n\tentry, exists := mc.entries[key]\n\tif !exists {\n\t\treturn nil, false\n\t}\n\t\n\t// Check TTL\n\tif time.Since(entry.CreatedAt) > mc.ttl {\n\t\tmc.removeFromList(entry)\n\t\tdelete(mc.entries, key)\n\t\treturn nil, false\n\t}\n\t\n\t// Move to front (most recently used)\n\tmc.moveToFront(entry)\n\treturn entry.Data, true\n}\n\n// Set stores data in cache\nfunc (mc *MetadataCache) Set(key string, data []byte) {\n\tmc.mu.Lock()\n\tdefer mc.mu.Unlock()\n\t\n\t// Check if entry exists\n\tif existingEntry, exists := mc.entries[key]; exists {\n\t\texistingEntry.Data = data\n\t\texistingEntry.CreatedAt = time.Now()\n\t\tmc.moveToFront(existingEntry)\n\t\treturn\n\t}\n\t\n\t// Evict if needed\n\tif len(mc.entries) >= mc.maxSize {\n\t\tmc.evictLRU()\n\t}\n\t\n\t// Create new entry\n\tentry := &MetadataCacheEntry{\n\t\tKey:       key,\n\t\tData:      data,\n\t\tCreatedAt: time.Now(),\n\t}\n\t\n\tmc.entries[key] = entry\n\tmc.addToFront(entry)\n}\n\n// Clear removes all entries\nfunc (mc *MetadataCache) Clear() {\n\tmc.mu.Lock()\n\tdefer mc.mu.Unlock()\n\t\n\tmc.entries = make(map[string]*MetadataCacheEntry)\n\tmc.head.next = mc.tail\n\tmc.tail.prev = mc.head\n}\n\n// LRU operations\nfunc (mc *MetadataCache) addToFront(entry *MetadataCacheEntry) {\n\tentry.prev = mc.head\n\tentry.next = mc.head.next\n\tmc.head.next.prev = entry\n\tmc.head.next = entry\n}\n\nfunc (mc *MetadataCache) removeFromList(entry *MetadataCacheEntry) {\n\tentry.prev.next = entry.next\n\tentry.next.prev = entry.prev\n}\n\nfunc (mc *MetadataCache) moveToFront(entry *MetadataCacheEntry) {\n\tmc.removeFromList(entry)\n\tmc.addToFront(entry)\n}\n\nfunc (mc *MetadataCache) evictLRU() {\n\tif mc.tail.prev != mc.head {\n\t\tlru := mc.tail.prev\n\t\tmc.removeFromList(lru)\n\t\tdelete(mc.entries, lru.Key)\n\t}\n}\n\n// NewComponentScanner creates a new component scanner with optimal worker pool\nfunc NewComponentScanner(registry *registry.ComponentRegistry, cfg ...*config.Config) *ComponentScanner {\n\treturn NewComponentScannerWithConcurrency(registry, 0, cfg...) // 0 = auto-detect optimal\n}\n\n// NewComponentScannerWithConcurrency creates a new component scanner with configurable concurrency\nfunc NewComponentScannerWithConcurrency(registry *registry.ComponentRegistry, maxWorkers int, cfg ...*config.Config) *ComponentScanner {\n\tscanner := &ComponentScanner{\n\t\tregistry:   registry,\n\t\tfileSet:    token.NewFileSet(),\n\t\tpathCache:  &pathValidationCache{},\n\t\tbufferPool: NewBufferPool(),\n\t\t// Initialize metadata cache: 1000 entries max, 1 hour TTL\n\t\t// This caches ~1000-2000 component metadata entries typically\n\t\tmetadataCache: NewMetadataCache(1000, time.Hour),\n\t\t// Initialize performance metrics tracking\n\t\tmetrics: &ScannerMetrics{},\n\t}\n\n\t// Initialize worker pool with configurable or optimal worker count\n\tworkerCount := maxWorkers\n\tif workerCount <= 0 {\n\t\t// Auto-detect optimal worker count\n\t\tworkerCount = runtime.NumCPU()\n\t\tif workerCount > 8 {\n\t\t\tworkerCount = 8 // Cap at 8 workers for diminishing returns\n\t\t}\n\t} else {\n\t\t// User-specified count, but enforce reasonable limits\n\t\tif workerCount > 64 {\n\t\t\tworkerCount = 64 // Maximum safety limit\n\t\t}\n\t}\n\n\tscanner.workerPool = NewWorkerPool(workerCount, scanner)\n\t\n\t// Initialize AST parsing pool with fewer workers to avoid oversubscription\n\tastWorkerCount := workerCount / 2\n\tif astWorkerCount < 1 {\n\t\tastWorkerCount = 1\n\t}\n\tscanner.astParsingPool = NewASTParsingPool(astWorkerCount)\n\n\t// Use first config if provided, otherwise nil\n\tif len(cfg) > 0 {\n\t\tscanner.config = cfg[0]\n\t}\n\t\n\treturn scanner\n}\n\n// getFileScanTimeout returns the configured timeout for file scanning operations\nfunc (s *ComponentScanner) getFileScanTimeout() time.Duration {\n\tif s.config != nil && s.config.Timeouts.FileScan > 0 {\n\t\treturn s.config.Timeouts.FileScan\n\t}\n\t// Default fallback timeout if no configuration is available\n\treturn 30 * time.Second\n}\n\n// NewWorkerPool creates a new worker pool for scanning operations\nfunc NewWorkerPool(workerCount int, scanner *ComponentScanner) *WorkerPool {\n\tpool := &WorkerPool{\n\t\tjobQueue:    make(chan ScanJob, workerCount*2), // Buffer for work-stealing efficiency\n\t\tworkerCount: workerCount,\n\t\tscanner:     scanner,\n\t\tstop:        make(chan struct{}),\n\t}\n\n\t// Start persistent workers\n\tpool.workers = make([]*ScanWorker, workerCount)\n\tfor i := 0; i < workerCount; i++ {\n\t\tworker := &ScanWorker{\n\t\t\tid:       i,\n\t\t\tjobQueue: pool.jobQueue,\n\t\t\tscanner:  scanner,\n\t\t\tstop:     make(chan struct{}),\n\t\t}\n\t\tpool.workers[i] = worker\n\t\tgo worker.start()\n\t}\n\n\treturn pool\n}\n\n// start begins the worker's processing loop\nfunc (w *ScanWorker) start() {\n\tfor {\n\t\tselect {\n\t\tcase job := <-w.jobQueue:\n\t\t\t// Process the scanning job\n\t\t\terr := w.scanner.scanFileInternal(job.filePath)\n\t\t\tjob.result <- ScanResult{\n\t\t\t\tfilePath: job.filePath,\n\t\t\t\terr:      err,\n\t\t\t}\n\t\tcase <-w.stop:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Stop gracefully shuts down the worker pool\nfunc (p *WorkerPool) Stop() {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\n\tif p.stopped {\n\t\treturn\n\t}\n\n\tp.stopped = true\n\tclose(p.stop)\n\n\t// Stop all workers\n\tfor _, worker := range p.workers {\n\t\tclose(worker.stop)\n\t}\n\n\t// Close job queue\n\tclose(p.jobQueue)\n}\n\n// GetRegistry returns the component registry\nfunc (s *ComponentScanner) GetRegistry() interfaces.ComponentRegistry {\n\treturn s.registry\n}\n\n// GetWorkerCount returns the number of active workers in the pool\nfunc (s *ComponentScanner) GetWorkerCount() int {\n\tif s.workerPool == nil {\n\t\treturn 0\n\t}\n\ts.workerPool.mu.RLock()\n\tdefer s.workerPool.mu.RUnlock()\n\treturn s.workerPool.workerCount\n}\n\n// GetMetrics returns a copy of the current scanner metrics\nfunc (s *ComponentScanner) GetMetrics() ScannerMetrics {\n\tif s.metrics == nil {\n\t\treturn ScannerMetrics{}\n\t}\n\treturn ScannerMetrics{\n\t\tFilesProcessed:  atomic.LoadInt64(&s.metrics.FilesProcessed),\n\t\tComponentsFound: atomic.LoadInt64(&s.metrics.ComponentsFound),\n\t\tCacheHits:       atomic.LoadInt64(&s.metrics.CacheHits),\n\t\tCacheMisses:     atomic.LoadInt64(&s.metrics.CacheMisses),\n\t\tTotalScanTime:   s.metrics.TotalScanTime,\n\t\tPeakMemoryUsage: atomic.LoadUint64(&s.metrics.PeakMemoryUsage),\n\t\tConcurrentJobs:  atomic.LoadInt64(&s.metrics.ConcurrentJobs),\n\t}\n}\n\n// ResetMetrics clears all scanner metrics\nfunc (s *ComponentScanner) ResetMetrics() {\n\tif s.metrics == nil {\n\t\treturn\n\t}\n\tatomic.StoreInt64(&s.metrics.FilesProcessed, 0)\n\tatomic.StoreInt64(&s.metrics.ComponentsFound, 0)\n\tatomic.StoreInt64(&s.metrics.CacheHits, 0)\n\tatomic.StoreInt64(&s.metrics.CacheMisses, 0)\n\tatomic.StoreUint64(&s.metrics.PeakMemoryUsage, 0)\n\tatomic.StoreInt64(&s.metrics.ConcurrentJobs, 0)\n\ts.metrics.TotalScanTime = 0\n}\n\n// Close gracefully shuts down the scanner and its worker pool\nfunc (s *ComponentScanner) Close() error {\n\tif s.astParsingPool != nil {\n\t\ts.astParsingPool.Close()\n\t}\n\tif s.workerPool != nil {\n\t\ts.workerPool.Stop()\n\t}\n\tif s.metadataCache != nil {\n\t\ts.metadataCache.Clear()\n\t}\n\treturn nil\n}\n\n// getCachedMetadata attempts to retrieve cached component metadata for a file\nfunc (s *ComponentScanner) getCachedMetadata(filePath, fileHash string) (*CachedComponentMetadata, bool) {\n\tif s.metadataCache == nil {\n\t\treturn nil, false\n\t}\n\n\tcacheKey := fmt.Sprintf(\"%s:%s\", filePath, fileHash)\n\tcachedData, found := s.metadataCache.Get(cacheKey)\n\tif !found {\n\t\treturn nil, false\n\t}\n\n\tvar metadata CachedComponentMetadata\n\tif err := json.Unmarshal(cachedData, &metadata); err != nil {\n\t\t// Cache corruption - remove invalid entry\n\t\ts.metadataCache.Set(cacheKey, nil)\n\t\treturn nil, false\n\t}\n\n\t// Verify the cached hash matches current file hash (additional safety check)\n\tif metadata.FileHash != fileHash {\n\t\treturn nil, false\n\t}\n\n\treturn &metadata, true\n}\n\n// setCachedMetadata stores component metadata in the cache\nfunc (s *ComponentScanner) setCachedMetadata(filePath, fileHash string, components []*types.ComponentInfo) {\n\tif s.metadataCache == nil {\n\t\treturn\n\t}\n\n\tmetadata := CachedComponentMetadata{\n\t\tComponents: components,\n\t\tFileHash:   fileHash,\n\t\tParsedAt:   time.Now(),\n\t}\n\n\tdata, err := json.Marshal(metadata)\n\tif err != nil {\n\t\t// Skip caching if marshaling fails\n\t\treturn\n\t}\n\n\tcacheKey := fmt.Sprintf(\"%s:%s\", filePath, fileHash)\n\ts.metadataCache.Set(cacheKey, data)\n}\n\n// ScanDirectory scans a directory for templ components using optimized worker pool with timeout support\nfunc (s *ComponentScanner) ScanDirectoryWithContext(ctx context.Context, dir string) error {\n\tstart := time.Now()\n\t\n\t// Track memory usage at start\n\tvar startMem runtime.MemStats\n\truntime.ReadMemStats(&startMem)\n\t\n\t// Validate directory path to prevent path traversal\n\tif _, err := s.validatePath(dir); err != nil {\n\t\treturn errors.WrapValidation(err, errors.ErrCodeInvalidPath, \n\t\t\t\"directory path validation failed\").\n\t\t\tWithContext(\"directory\", dir)\n\t}\n\n\t// Use concurrent directory walking for better performance on large codebases\n\tfiles, err := s.walkDirectoryConcurrent(dir)\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Process files using persistent worker pool with context (no goroutine creation overhead)\n\terr = s.processBatchWithWorkerPoolWithContext(ctx, files)\n\t\n\t// Update metrics\n\tif s.metrics != nil {\n\t\telapsed := time.Since(start)\n\t\ts.metrics.TotalScanTime += elapsed\n\t\tatomic.AddInt64(&s.metrics.FilesProcessed, int64(len(files)))\n\t\t\n\t\t// Track memory usage\n\t\tvar endMem runtime.MemStats\n\t\truntime.ReadMemStats(&endMem)\n\t\tmemUsed := endMem.Alloc - startMem.Alloc\n\t\t\n\t\t// Update peak memory if this scan used more\n\t\tfor {\n\t\t\tcurrent := atomic.LoadUint64(&s.metrics.PeakMemoryUsage)\n\t\t\tif memUsed <= current || atomic.CompareAndSwapUint64(&s.metrics.PeakMemoryUsage, current, memUsed) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\t\n\treturn err\n}\n\n// ScanDirectory scans a directory for templ components (backward compatible wrapper)\nfunc (s *ComponentScanner) ScanDirectory(dir string) error {\n\t// Create a timeout context for the scan operation\n\tscanTimeout := s.getFileScanTimeout()\n\tctx, cancel := context.WithTimeout(context.Background(), scanTimeout)\n\tdefer cancel()\n\t\n\treturn s.ScanDirectoryWithContext(ctx, dir)\n}\n\n// processBatchWithWorkerPoolWithContext processes files using the persistent worker pool with optimized batching and context support\nfunc (s *ComponentScanner) processBatchWithWorkerPoolWithContext(ctx context.Context, files []string) error {\n\tif len(files) == 0 {\n\t\treturn nil\n\t}\n\n\t// For very small batches, process synchronously to avoid overhead\n\tif len(files) <= 5 {\n\t\treturn s.processBatchSynchronous(files)\n\t}\n\n\t// Create result channel for collecting results\n\tresultChan := make(chan ScanResult, len(files))\n\tsubmitted := 0\n\t\n\t// Submit jobs to persistent worker pool\n\tfor _, file := range files {\n\t\t// Check if context is cancelled\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tdefault:\n\t\t}\n\n\t\tjob := ScanJob{\n\t\t\tfilePath: file,\n\t\t\tresult:   resultChan,\n\t\t}\n\n\t\tselect {\n\t\tcase s.workerPool.jobQueue <- job:\n\t\t\t// Job submitted successfully\n\t\t\tsubmitted++\n\t\tcase <-ctx.Done():\n\t\t\t// Context cancelled while submitting\n\t\t\treturn ctx.Err()\n\t\tdefault:\n\t\t\t// Worker pool is full, process synchronously as fallback\n\t\t\terr := s.scanFileInternal(file)\n\t\t\tresultChan <- ScanResult{filePath: file, err: err}\n\t\t}\n\t}\n\n\t// Collect results with context checking\n\tvar scanErrors []error\n\tfor i := 0; i < len(files); i++ {\n\t\tselect {\n\t\tcase result := <-resultChan:\n\t\t\tif result.err != nil {\n\t\t\t\t// Enhance the error with file context\n\t\t\t\tenhancedErr := errors.EnhanceError(result.err, \"scanner\", result.filePath, 0, 0)\n\t\t\t\tscanErrors = append(scanErrors, enhancedErr)\n\t\t\t}\n\t\tcase <-ctx.Done():\n\t\t\t// Context cancelled while collecting results\n\t\t\treturn ctx.Err()\n\t\t}\n\t}\n\n\tclose(resultChan)\n\n\tif len(scanErrors) > 0 {\n\t\treturn errors.CombineErrors(scanErrors...)\n\t}\n\n\treturn nil\n}\n\n// processBatchWithWorkerPool processes files using the persistent worker pool (backward compatible wrapper)\nfunc (s *ComponentScanner) processBatchWithWorkerPool(files []string) error {\n\t// Use background context with timeout for backward compatibility\n\tscanTimeout := s.getFileScanTimeout()\n\tctx, cancel := context.WithTimeout(context.Background(), scanTimeout)\n\tdefer cancel()\n\t\n\treturn s.processBatchWithWorkerPoolWithContext(ctx, files)\n}\n\n// processBatchSynchronous processes small batches synchronously for better performance\nfunc (s *ComponentScanner) processBatchSynchronous(files []string) error {\n\tvar scanErrors []error\n\t\n\tfor _, file := range files {\n\t\tif err := s.scanFileInternal(file); err != nil {\n\t\t\tenhancedErr := errors.EnhanceError(err, \"scanner\", file, 0, 0)\n\t\t\tscanErrors = append(scanErrors, enhancedErr)\n\t\t}\n\t}\n\n\tif len(scanErrors) > 0 {\n\t\treturn errors.CombineErrors(scanErrors...)\n\t}\n\n\treturn nil\n}\n\n// ScanDirectoryParallel is deprecated in favor of the optimized ScanDirectory\n// Kept for backward compatibility\nfunc (s *ComponentScanner) ScanDirectoryParallel(dir string, workers int) error {\n\treturn s.ScanDirectory(dir) // Use optimized version\n}\n\n// ScanFile scans a single file for templ components (optimized)\nfunc (s *ComponentScanner) ScanFile(path string) error {\n\treturn s.scanFileInternal(path)\n}\n\n// scanFileInternal is the optimized internal scanning method used by workers\nfunc (s *ComponentScanner) scanFileInternal(path string) error {\n\t// Validate and clean the path to prevent directory traversal\n\tcleanPath, err := s.validatePath(path)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"invalid path: %w\", err)\n\t}\n\n\t// Optimized single I/O operation: open file and get both content and info\n\tfile, err := os.Open(cleanPath)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"opening file %s: %w\", cleanPath, err)\n\t}\n\tdefer file.Close()\n\n\t// Get file info without separate Stat() call\n\tinfo, err := file.Stat()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"getting file info for %s: %w\", cleanPath, err)\n\t}\n\n\t// Get buffer from pool for optimized memory usage\n\tbuffer := s.bufferPool.Get()\n\tdefer s.bufferPool.Put(buffer)\n\n\t// Read content efficiently using buffer pool\n\tvar content []byte\n\tif info.Size() > 64*1024 {\n\t\t// Use streaming read for large files to reduce memory pressure\n\t\tcontent, err = s.readFileStreamingOptimized(file, info.Size(), buffer)\n\t} else {\n\t\t// Use pooled buffer for small files\n\t\tif cap(buffer) < int(info.Size()) {\n\t\t\tbuffer = make([]byte, info.Size())\n\t\t}\n\t\tbuffer = buffer[:info.Size()]\n\t\t_, err = file.Read(buffer)\n\t\tif err == nil {\n\t\t\tcontent = make([]byte, len(buffer))\n\t\t\tcopy(content, buffer)\n\t\t}\n\t}\n\n\tif err != nil {\n\t\treturn fmt.Errorf(\"reading file %s: %w\", cleanPath, err)\n\t}\n\n\t// Calculate file hash for cache lookup and change detection\n\thash := strconv.FormatUint(uint64(crc32.Checksum(content, crcTable)), 16)\n\t\n\t// Check cache first - avoid expensive parsing if metadata is cached\n\tif cachedMetadata, found := s.getCachedMetadata(cleanPath, hash); found {\n\t\t// Track cache hit\n\t\tif s.metrics != nil {\n\t\t\tatomic.AddInt64(&s.metrics.CacheHits, 1)\n\t\t}\n\t\t\n\t\t// Register all cached components with the registry\n\t\tfor _, component := range cachedMetadata.Components {\n\t\t\t// Update file modification time to current scan time\n\t\t\tupdatedComponent := *component\n\t\t\tupdatedComponent.LastMod = info.ModTime()\n\t\t\tupdatedComponent.Hash = hash\n\t\t\ts.registry.Register(&updatedComponent)\n\t\t}\n\t\t\n\t\t// Track components found\n\t\tif s.metrics != nil {\n\t\t\tatomic.AddInt64(&s.metrics.ComponentsFound, int64(len(cachedMetadata.Components)))\n\t\t}\n\t\t\n\t\treturn nil\n\t}\n\n\t// Track cache miss\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.CacheMisses, 1)\n\t}\n\n\t// Cache miss - perform parsing with async AST parsing to avoid blocking worker threads\n\tvar components []*types.ComponentInfo\n\t\n\t// Use async AST parsing to avoid blocking the worker thread\n\tastResultChan := s.astParsingPool.ParseAsync(cleanPath, content, s.fileSet)\n\t\n\t// Wait for AST parsing result (non-blocking for the worker thread)\n\tastResult := <-astResultChan\n\t\n\tif astResult.err != nil {\n\t\t// If AST parsing fails, try manual component extraction for .templ files\n\t\tcomponents, err = s.parseTemplFileWithComponents(cleanPath, content, hash, info.ModTime())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\t// Extract components from AST\n\t\tcomponents, err = s.extractFromASTWithComponents(cleanPath, astResult.astFile, hash, info.ModTime())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Cache the parsed components for future scans\n\ts.setCachedMetadata(cleanPath, hash, components)\n\n\t// Register all components with the registry\n\tfor _, component := range components {\n\t\ts.registry.Register(component)\n\t}\n\t\n\t// Track components found\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.ComponentsFound, int64(len(components)))\n\t}\n\n\treturn nil\n}\n\n// readFileStreaming removed - replaced by readFileStreamingOptimized\n\n// readFileStreamingOptimized reads large files using pooled buffers for better memory efficiency\nfunc (s *ComponentScanner) readFileStreamingOptimized(file *os.File, size int64, pooledBuffer []byte) ([]byte, error) {\n\tconst chunkSize = 32 * 1024 // 32KB chunks\n\t\n\t// Use a reasonably-sized chunk buffer for reading\n\tvar chunk []byte\n\tif cap(pooledBuffer) >= chunkSize {\n\t\tchunk = pooledBuffer[:chunkSize]\n\t} else {\n\t\tchunk = make([]byte, chunkSize)\n\t}\n\n\t// Pre-allocate content buffer with exact size to avoid reallocations\n\tcontent := make([]byte, 0, size)\n\n\tfor {\n\t\tn, err := file.Read(chunk)\n\t\tif n > 0 {\n\t\t\tcontent = append(content, chunk[:n]...)\n\t\t}\n\t\tif err != nil {\n\t\t\tif err == io.EOF {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t\tif n < chunkSize {\n\t\t\tbreak\n\t\t}\n\t}\n\n\treturn content, nil\n}\n\n// Backward compatibility method removed - unused\n\n// parseTemplFileWithComponents extracts components from templ files and returns them\nfunc (s *ComponentScanner) parseTemplFileWithComponents(path string, content []byte, hash string, modTime time.Time) ([]*types.ComponentInfo, error) {\n\tvar components []*types.ComponentInfo\n\tlines := strings.Split(string(content), \"\\n\")\n\tpackageName := \"\"\n\n\tfor _, line := range lines {\n\t\tline = strings.TrimSpace(line)\n\n\t\t// Extract package name\n\t\tif strings.HasPrefix(line, \"package \") {\n\t\t\tparts := strings.Fields(line)\n\t\t\tif len(parts) >= 2 {\n\t\t\t\tpackageName = sanitizeIdentifier(parts[1])\n\t\t\t}\n\t\t}\n\n\t\t// Extract templ component declarations\n\t\tif strings.HasPrefix(line, \"templ \") {\n\t\t\t// Extract component name from templ declaration\n\t\t\tparts := strings.Fields(line)\n\t\t\tif len(parts) >= 2 {\n\t\t\t\tname := parts[1]\n\t\t\t\tif idx := strings.Index(name, \"(\"); idx != -1 {\n\t\t\t\t\tname = name[:idx]\n\t\t\t\t}\n\n\t\t\t\t// Sanitize component name to prevent injection\n\t\t\t\tname = sanitizeIdentifier(name)\n\n\t\t\t\tcomponent := &types.ComponentInfo{\n\t\t\t\t\tName:         name,\n\t\t\t\t\tPackage:      packageName,\n\t\t\t\t\tFilePath:     path,\n\t\t\t\t\tParameters:   extractParameters(line),\n\t\t\t\t\tImports:      []string{},\n\t\t\t\t\tLastMod:      modTime,\n\t\t\t\t\tHash:         hash,\n\t\t\t\t\tDependencies: []string{},\n\t\t\t\t}\n\n\t\t\t\tcomponents = append(components, component)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn components, nil\n}\n\n// parseTemplFile provides backward compatibility - delegates to the new component-returning version\nfunc (s *ComponentScanner) parseTemplFile(path string, content []byte, hash string, modTime time.Time) error {\n\tcomponents, err := s.parseTemplFileWithComponents(path, content, hash, modTime)\n\tif err != nil {\n\t\treturn err\n\t}\n\t\n\t// Register all components\n\tfor _, component := range components {\n\t\ts.registry.Register(component)\n\t}\n\t\n\treturn nil\n}\n\n// extractFromASTWithComponents extracts components from AST and returns them\nfunc (s *ComponentScanner) extractFromASTWithComponents(path string, astFile *ast.File, hash string, modTime time.Time) ([]*types.ComponentInfo, error) {\n\tvar components []*types.ComponentInfo\n\t\n\t// Walk the AST to find function declarations that might be templ components\n\tast.Inspect(astFile, func(n ast.Node) bool {\n\t\tswitch node := n.(type) {\n\t\tcase *ast.FuncDecl:\n\t\t\tif node.Name != nil && node.Name.IsExported() {\n\t\t\t\t// Check if this might be a templ component\n\t\t\t\tif s.isTemplComponent(node) {\n\t\t\t\t\tcomponent := &types.ComponentInfo{\n\t\t\t\t\t\tName:         node.Name.Name,\n\t\t\t\t\t\tPackage:      astFile.Name.Name,\n\t\t\t\t\t\tFilePath:     path,\n\t\t\t\t\t\tParameters:   s.extractParametersFromFunc(node),\n\t\t\t\t\t\tImports:      s.extractImports(astFile),\n\t\t\t\t\t\tLastMod:      modTime,\n\t\t\t\t\t\tHash:         hash,\n\t\t\t\t\t\tDependencies: []string{},\n\t\t\t\t\t}\n\n\t\t\t\t\tcomponents = append(components, component)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn true\n\t})\n\n\treturn components, nil\n}\n\n// extractFromAST provides backward compatibility - delegates to the new component-returning version\nfunc (s *ComponentScanner) extractFromAST(path string, astFile *ast.File, hash string, modTime time.Time) error {\n\tcomponents, err := s.extractFromASTWithComponents(path, astFile, hash, modTime)\n\tif err != nil {\n\t\treturn err\n\t}\n\t\n\t// Register all components\n\tfor _, component := range components {\n\t\ts.registry.Register(component)\n\t}\n\t\n\treturn nil\n}\n\nfunc (s *ComponentScanner) isTemplComponent(fn *ast.FuncDecl) bool {\n\t// Check if the function returns a templ.Component\n\tif fn.Type.Results == nil || len(fn.Type.Results.List) == 0 {\n\t\treturn false\n\t}\n\n\tresult := fn.Type.Results.List[0]\n\tif sel, ok := result.Type.(*ast.SelectorExpr); ok {\n\t\tif ident, ok := sel.X.(*ast.Ident); ok {\n\t\t\treturn ident.Name == \"templ\" && sel.Sel.Name == \"Component\"\n\t\t}\n\t}\n\n\treturn false\n}\n\nfunc (s *ComponentScanner) extractParametersFromFunc(fn *ast.FuncDecl) []types.ParameterInfo {\n\tvar params []types.ParameterInfo\n\n\tif fn.Type.Params == nil {\n\t\treturn params\n\t}\n\n\tfor _, param := range fn.Type.Params.List {\n\t\tparamType := \"\"\n\t\tif param.Type != nil {\n\t\t\tparamType = s.typeToString(param.Type)\n\t\t}\n\n\t\tfor _, name := range param.Names {\n\t\t\tparams = append(params, types.ParameterInfo{\n\t\t\t\tName:     name.Name,\n\t\t\t\tType:     paramType,\n\t\t\t\tOptional: false,\n\t\t\t\tDefault:  nil,\n\t\t\t})\n\t\t}\n\t}\n\n\treturn params\n}\n\nfunc (s *ComponentScanner) extractImports(astFile *ast.File) []string {\n\tvar imports []string\n\n\tfor _, imp := range astFile.Imports {\n\t\tif imp.Path != nil {\n\t\t\timports = append(imports, imp.Path.Value)\n\t\t}\n\t}\n\n\treturn imports\n}\n\nfunc (s *ComponentScanner) typeToString(expr ast.Expr) string {\n\tswitch e := expr.(type) {\n\tcase *ast.Ident:\n\t\treturn e.Name\n\tcase *ast.SelectorExpr:\n\t\treturn s.typeToString(e.X) + \".\" + e.Sel.Name\n\tcase *ast.StarExpr:\n\t\treturn \"*\" + s.typeToString(e.X)\n\tcase *ast.ArrayType:\n\t\treturn \"[]\" + s.typeToString(e.Elt)\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\nfunc extractParameters(line string) []types.ParameterInfo {\n\t// Simple parameter extraction from templ declaration\n\t// This is a basic implementation - real parser would be more robust\n\tif !strings.Contains(line, \"(\") {\n\t\treturn []types.ParameterInfo{}\n\t}\n\n\tstart := strings.Index(line, \"(\")\n\tend := strings.LastIndex(line, \")\")\n\tif start == -1 || end == -1 || start >= end {\n\t\treturn []types.ParameterInfo{}\n\t}\n\n\tparamStr := line[start+1 : end]\n\tif strings.TrimSpace(paramStr) == \"\" {\n\t\treturn []types.ParameterInfo{}\n\t}\n\n\t// Basic parameter parsing - handle both \"name type\" and \"name, name type\" patterns\n\tparts := strings.Split(paramStr, \",\")\n\tvar params []types.ParameterInfo\n\n\tfor _, part := range parts {\n\t\tpart = strings.TrimSpace(part)\n\t\tif part == \"\" {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Split by space to get name and type\n\t\tfields := strings.Fields(part)\n\t\tif len(fields) >= 2 {\n\t\t\t// Handle \"name type\" format\n\t\t\tparams = append(params, types.ParameterInfo{\n\t\t\t\tName:     fields[0],\n\t\t\t\tType:     fields[1],\n\t\t\t\tOptional: false,\n\t\t\t\tDefault:  nil,\n\t\t\t})\n\t\t} else if len(fields) == 1 {\n\t\t\t// Handle single parameter name (type might be from previous param)\n\t\t\tparams = append(params, types.ParameterInfo{\n\t\t\t\tName:     fields[0],\n\t\t\t\tType:     \"string\", // Default type\n\t\t\t\tOptional: false,\n\t\t\t\tDefault:  nil,\n\t\t\t})\n\t\t}\n\t}\n\n\treturn params\n}\n\n// sanitizeIdentifier removes dangerous characters from identifiers\nfunc sanitizeIdentifier(identifier string) string {\n\t// Only allow alphanumeric characters and underscores for identifiers\n\tvar cleaned strings.Builder\n\tfor _, r := range identifier {\n\t\tif (r >= 'a' && r <= 'z') || (r >= 'A' && r <= 'Z') || (r >= '0' && r <= '9') || r == '_' {\n\t\t\tcleaned.WriteRune(r)\n\t\t}\n\t}\n\treturn cleaned.String()\n}\n\n// validatePath validates and cleans a file path to prevent directory traversal.\n// This optimized version caches the current working directory to avoid repeated\n// expensive filesystem operations, achieving 50-70% performance improvement.\nfunc (s *ComponentScanner) validatePath(path string) (string, error) {\n\t// Clean the path to resolve . and .. elements\n\tcleanPath := filepath.Clean(path)\n\n\t// Get absolute path to normalize (needed for working directory check)\n\tabsPath, err := filepath.Abs(cleanPath)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"getting absolute path: %w\", err)\n\t}\n\n\t// Get cached current working directory\n\tcwd, err := s.getCachedWorkingDir()\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"getting current directory: %w\", err)\n\t}\n\n\t// Primary security check: ensure the path is within the current working directory\n\t// This prevents directory traversal attacks that escape the working directory\n\tif !strings.HasPrefix(absPath, cwd) {\n\t\treturn \"\", errors.ErrPathTraversal(path).WithContext(\"working_directory\", cwd)\n\t}\n\n\t// Secondary security check: reject paths with suspicious patterns\n\t// This catches directory traversal attempts that stay within the working directory\n\tif strings.Contains(cleanPath, \"..\") {\n\t\treturn \"\", errors.ErrPathTraversal(path).\n\t\t\tWithContext(\"pattern\", \"contains '..' traversal\")\n\t}\n\n\treturn cleanPath, nil\n}\n\n// getCachedWorkingDir returns the current working directory from cache,\n// initializing it on first access. This eliminates repeated os.Getwd() calls.\nfunc (s *ComponentScanner) getCachedWorkingDir() (string, error) {\n\t// Fast path: check if already initialized with read lock\n\ts.pathCache.mu.RLock()\n\tif s.pathCache.initialized {\n\t\tcwd := s.pathCache.currentWorkingDir\n\t\ts.pathCache.mu.RUnlock()\n\t\treturn cwd, nil\n\t}\n\ts.pathCache.mu.RUnlock()\n\n\t// Slow path: initialize the cache with write lock\n\ts.pathCache.mu.Lock()\n\tdefer s.pathCache.mu.Unlock()\n\n\t// Double-check pattern: another goroutine might have initialized while waiting\n\tif s.pathCache.initialized {\n\t\treturn s.pathCache.currentWorkingDir, nil\n\t}\n\n\t// Get current working directory (expensive syscall - done only once)\n\tcwd, err := os.Getwd()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Ensure we have the absolute path for consistent comparison\n\tabsCwd, err := filepath.Abs(cwd)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"getting absolute working directory: %w\", err)\n\t}\n\n\t// Cache the result\n\ts.pathCache.currentWorkingDir = absCwd\n\ts.pathCache.initialized = true\n\n\treturn absCwd, nil\n}\n\n// InvalidatePathCache clears the cached working directory.\n// This should be called if the working directory changes during execution.\nfunc (s *ComponentScanner) InvalidatePathCache() {\n\ts.pathCache.mu.Lock()\n\tdefer s.pathCache.mu.Unlock()\n\ts.pathCache.initialized = false\n\ts.pathCache.currentWorkingDir = \"\"\n}\n\n// walkDirectoryConcurrent implements concurrent directory walking for improved performance\n// on large codebases. Uses goroutines to parallelize directory discovery.\nfunc (s *ComponentScanner) walkDirectoryConcurrent(rootDir string) ([]string, error) {\n\t// For small directory trees, use optimized sequential version\n\t// For larger trees, use concurrent discovery\n\t\n\t// Quick check for directory size to decide approach\n\tentries, err := os.ReadDir(rootDir)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading root directory %s: %w\", rootDir, err)\n\t}\n\t\n\t// If small directory, use optimized sequential\n\tif len(entries) < 10 {\n\t\treturn s.walkDirectoryOptimized(rootDir)\n\t}\n\t\n\t// Use concurrent approach for larger directories\n\treturn s.walkDirectoryParallel(rootDir)\n}\n\n// walkDirectoryParallel implements concurrent directory discovery\nfunc (s *ComponentScanner) walkDirectoryParallel(rootDir string) ([]string, error) {\n\t// Use a simple approach: collect all directories first, then process them concurrently\n\t\n\t// First, collect all directories sequentially (this is fast)\n\tvar allDirs []string\n\terr := filepath.WalkDir(rootDir, func(path string, d os.DirEntry, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif d.IsDir() && !s.shouldSkipDirectory(d.Name()) {\n\t\t\tallDirs = append(allDirs, path)\n\t\t}\n\t\treturn nil\n\t})\n\t\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t\n\t// Now process directories concurrently\n\tconst maxWorkers = 4\n\tjobs := make(chan string, len(allDirs))\n\tresults := make(chan []string, len(allDirs))\n\t\n\t// Start workers\n\tvar wg sync.WaitGroup\n\tfor w := 0; w < maxWorkers; w++ {\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\tfor dir := range jobs {\n\t\t\t\tfiles, _, _ := s.processSingleDirectory(dir)\n\t\t\t\tif len(files) > 0 {\n\t\t\t\t\tresults <- files\n\t\t\t\t} else {\n\t\t\t\t\tresults <- nil // Send empty result to maintain count\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n\t\n\t// Send jobs\n\tfor _, dir := range allDirs {\n\t\tjobs <- dir\n\t}\n\tclose(jobs)\n\t\n\t// Wait for workers to finish\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(results)\n\t}()\n\t\n\t// Collect results\n\tvar allFiles []string\n\tfor files := range results {\n\t\tif files != nil {\n\t\t\tallFiles = append(allFiles, files...)\n\t\t}\n\t}\n\t\n\treturn allFiles, nil\n}\n\n// walkDirectoryOptimized implements an optimized sequential walk with directory skipping\nfunc (s *ComponentScanner) walkDirectoryOptimized(rootDir string) ([]string, error) {\n\tvar files []string\n\t\n\terr := filepath.WalkDir(rootDir, func(path string, d os.DirEntry, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Skip directories that typically don't contain templ files\n\t\tif d.IsDir() && s.shouldSkipDirectory(d.Name()) {\n\t\t\treturn filepath.SkipDir\n\t\t}\n\n\t\tif !d.IsDir() && strings.HasSuffix(path, \".templ\") {\n\t\t\t// Validate each file path as we encounter it\n\t\t\tif _, err := s.validatePath(path); err != nil {\n\t\t\t\t// Skip invalid paths silently for security\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tfiles = append(files, path)\n\t\t}\n\t\t\n\t\treturn nil\n\t})\n\t\n\treturn files, err\n}\n\n// processSingleDirectory processes a single directory and returns files and subdirectories\nfunc (s *ComponentScanner) processSingleDirectory(dir string) ([]string, []string, error) {\n\tvar files []string\n\tvar subdirs []string\n\t\n\tentries, err := os.ReadDir(dir)\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"reading directory %s: %w\", dir, err)\n\t}\n\t\n\tfor _, entry := range entries {\n\t\tpath := filepath.Join(dir, entry.Name())\n\t\t\n\t\tif entry.IsDir() {\n\t\t\t// Skip directories that typically don't contain templ files\n\t\t\tif s.shouldSkipDirectory(entry.Name()) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tsubdirs = append(subdirs, path)\n\t\t} else if strings.HasSuffix(entry.Name(), \".templ\") {\n\t\t\t// Validate each file path as we encounter it\n\t\t\tif _, err := s.validatePath(path); err != nil {\n\t\t\t\t// Skip invalid paths silently for security\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfiles = append(files, path)\n\t\t}\n\t}\n\t\n\treturn files, subdirs, nil\n}\n\n// shouldSkipDirectory determines if a directory should be skipped for performance\nfunc (s *ComponentScanner) shouldSkipDirectory(name string) bool {\n\tskipDirs := map[string]bool{\n\t\t\".git\":         true,\n\t\t\".svn\":         true,\n\t\t\"node_modules\": true,\n\t\t\".next\":        true,\n\t\t\"dist\":         true,\n\t\t\"build\":        true,\n\t\t\"vendor\":       true,\n\t\t\".vscode\":      true,\n\t\t\".idea\":        true,\n\t\t\"__pycache__\":  true,\n\t\t\".DS_Store\":    true,\n\t}\n\t\n\treturn skipDirs[name]\n}\n\n\n",
      "structuredPatch": [
        {
          "oldStart": 36,
          "oldLines": 6,
          "newStart": 36,
          "newLines": 27,
          "lines": [
            " // crcTable is a pre-computed CRC32 Castagnoli table for faster hash generation",
            " var crcTable = crc32.MakeTable(crc32.Castagnoli)",
            " ",
            "+// Hash generation strategy constants",
            "+const (",
            "+  // Small files (< 4KB) - use full content CRC32",
            "+  smallFileThreshold = 4 * 1024",
            "+  // Medium files (4KB - 256KB) - use content sampling",
            "+  mediumFileThreshold = 256 * 1024",
            "+  // Large files (> 256KB) - use hierarchical sampling",
            "+  largeFileThreshold = 256 * 1024",
            "+  // Content sample size for large files",
            "+  contentSampleSize = 1024",
            "+)",
            "+",
            "+// FileHashStrategy represents different hashing approaches",
            "+type FileHashStrategy int",
            "+",
            "+const (",
            "+  HashStrategyFull FileHashStrategy = iota",
            "+  HashStrategySampled",
            "+  HashStrategyHierarchical",
            "+)",
            "+",
            " // ScanJob represents a scanning job for the worker pool containing the file",
            " // path to scan and a result channel for asynchronous communication.",
            " type ScanJob struct {"
          ]
        }
      ],
      "userModified": false,
      "replaceAll": false
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Edit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
      "old_string": "// Build queues a component for building\nfunc (bp *BuildPipeline) Build(component *types.ComponentInfo) {\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  1,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.tasks <- task:\n\t\t// Task successfully queued\n\tdefault:\n\t\t// Queue full - implement backpressure handling\n\t\t// Log the error and update metrics\n\t\tfmt.Printf(\"Warning: Build queue full, dropping task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"task_queue_full\")\n\n\t\t// Try to handle with retry or priority queue\n\t\tselect {\n\t\tcase bp.queue.priority <- task:\n\t\t\tfmt.Printf(\"Task for %s promoted to priority queue\\n\", component.Name)\n\t\tdefault:\n\t\t\tfmt.Printf(\"Error: Both queues full, build request lost for component %s\\n\", component.Name)\n\t\t\t// TODO: Implement persistent queue or callback for dropped tasks\n\t\t}\n\t}\n}",
      "new_string": "// Build queues a component for building\nfunc (bp *BuildPipeline) Build(component *types.ComponentInfo) {\n\t// Check if pipeline is shut down\n\tif bp.cancel == nil {\n\t\tfmt.Printf(\"Error: Build pipeline not started, dropping task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"pipeline_not_started\")\n\t\treturn\n\t}\n\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  1,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.tasks <- task:\n\t\t// Task successfully queued\n\tdefault:\n\t\t// Queue full - implement backpressure handling\n\t\t// Log the error and update metrics\n\t\tfmt.Printf(\"Warning: Build queue full, dropping task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"task_queue_full\")\n\n\t\t// Try to handle with retry or priority queue\n\t\tselect {\n\t\tcase bp.queue.priority <- task:\n\t\t\tfmt.Printf(\"Task for %s promoted to priority queue\\n\", component.Name)\n\t\tdefault:\n\t\t\tfmt.Printf(\"Error: Both queues full, build request lost for component %s\\n\", component.Name)\n\t\t\t// TODO: Implement persistent queue or callback for dropped tasks\n\t\t}\n\t}\n}"
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
      "oldString": "// Build queues a component for building\nfunc (bp *BuildPipeline) Build(component *types.ComponentInfo) {\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  1,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.tasks <- task:\n\t\t// Task successfully queued\n\tdefault:\n\t\t// Queue full - implement backpressure handling\n\t\t// Log the error and update metrics\n\t\tfmt.Printf(\"Warning: Build queue full, dropping task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"task_queue_full\")\n\n\t\t// Try to handle with retry or priority queue\n\t\tselect {\n\t\tcase bp.queue.priority <- task:\n\t\t\tfmt.Printf(\"Task for %s promoted to priority queue\\n\", component.Name)\n\t\tdefault:\n\t\t\tfmt.Printf(\"Error: Both queues full, build request lost for component %s\\n\", component.Name)\n\t\t\t// TODO: Implement persistent queue or callback for dropped tasks\n\t\t}\n\t}\n}",
      "newString": "// Build queues a component for building\nfunc (bp *BuildPipeline) Build(component *types.ComponentInfo) {\n\t// Check if pipeline is shut down\n\tif bp.cancel == nil {\n\t\tfmt.Printf(\"Error: Build pipeline not started, dropping task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"pipeline_not_started\")\n\t\treturn\n\t}\n\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  1,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.tasks <- task:\n\t\t// Task successfully queued\n\tdefault:\n\t\t// Queue full - implement backpressure handling\n\t\t// Log the error and update metrics\n\t\tfmt.Printf(\"Warning: Build queue full, dropping task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"task_queue_full\")\n\n\t\t// Try to handle with retry or priority queue\n\t\tselect {\n\t\tcase bp.queue.priority <- task:\n\t\t\tfmt.Printf(\"Task for %s promoted to priority queue\\n\", component.Name)\n\t\tdefault:\n\t\t\tfmt.Printf(\"Error: Both queues full, build request lost for component %s\\n\", component.Name)\n\t\t\t// TODO: Implement persistent queue or callback for dropped tasks\n\t\t}\n\t}\n}",
      "originalFile": "// Package build provides a concurrent build pipeline for templ components\n// with caching, error collection, and performance metrics.\n//\n// The build pipeline processes components through worker pools, maintains\n// an LRU cache for build results, and provides real-time build status\n// through callbacks and metrics. It supports parallel execution with\n// configurable worker counts and implements security-hardened command\n// execution with proper validation.\npackage build\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"hash/crc32\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/conneroisu/templar/internal/errors\"\n\t\"github.com/conneroisu/templar/internal/interfaces\"\n\t\"github.com/conneroisu/templar/internal/types\"\n)\n\n// crcTable is a pre-computed CRC32 Castagnoli table for faster hash generation\nvar crcTable = crc32.MakeTable(crc32.Castagnoli)\n\n// BuildPipeline manages the build process for templ components with concurrent\n// execution, intelligent caching, and comprehensive error handling.\n//\n// The pipeline provides:\n// - Concurrent build execution with configurable worker pools\n// - LRU caching with CRC32-based change detection\n// - Priority-based build queue management\n// - Real-time build metrics and status callbacks\n// - Memory optimization through object pooling\n// - Security-hardened command execution\n// - Comprehensive timeout management\ntype BuildPipeline struct {\n\t// compiler handles templ compilation with security validation\n\tcompiler *TemplCompiler\n\t// cache provides LRU-based build result caching\n\tcache *BuildCache\n\t// queue manages build tasks with priority ordering\n\tqueue *BuildQueue\n\t// workers defines the number of concurrent build workers\n\tworkers int\n\t// registry provides component information and change notifications\n\tregistry interfaces.ComponentRegistry\n\t// errorParser processes build errors and provides detailed diagnostics\n\terrorParser *errors.ErrorParser\n\t// metrics tracks build performance and success rates\n\tmetrics *BuildMetrics\n\t// callbacks receive build status updates for UI integration\n\tcallbacks []BuildCallback\n\t// workerWg synchronizes worker goroutine lifecycle\n\tworkerWg sync.WaitGroup\n\t// resultWg synchronizes result processing\n\tresultWg sync.WaitGroup\n\t// cancel terminates all pipeline operations gracefully\n\tcancel context.CancelFunc\n\t// objectPools optimize memory allocation for frequently used objects\n\tobjectPools *ObjectPools\n\t// slicePools reduce slice allocation overhead\n\tslicePools *SlicePools\n\t// workerPool manages the lifecycle of build workers\n\tworkerPool *WorkerPool\n\t// config provides timeout configuration for build operations\n\tconfig *config.Config\n}\n\n// BuildTask represents a build task in the priority queue with metadata\n// for scheduling and execution tracking.\ntype BuildTask struct {\n\t// Component contains the component information to be built\n\tComponent *types.ComponentInfo\n\t// Priority determines build order (higher values built first)\n\tPriority int\n\t// Timestamp records when the task was created for ordering\n\tTimestamp time.Time\n}\n\n// BuildResult represents the result of a build operation\ntype BuildResult struct {\n\tComponent    *types.ComponentInfo\n\tOutput       []byte\n\tError        error\n\tParsedErrors []*errors.ParsedError\n\tDuration     time.Duration\n\tCacheHit     bool\n\tHash         string\n}\n\n// BuildCallback is called when a build completes\ntype BuildCallback func(result BuildResult)\n\n// BuildQueue manages build tasks\ntype BuildQueue struct {\n\ttasks    chan BuildTask\n\tresults  chan BuildResult\n\tpriority chan BuildTask\n}\n\n// NewBuildPipeline creates a new build pipeline with optional timeout configuration\nfunc NewBuildPipeline(workers int, registry interfaces.ComponentRegistry, cfg ...*config.Config) *BuildPipeline {\n\tcompiler := NewTemplCompiler()\n\tcache := NewBuildCache(100*1024*1024, time.Hour) // 100MB, 1 hour TTL\n\n\tqueue := &BuildQueue{\n\t\ttasks:    make(chan BuildTask, 100),\n\t\tresults:  make(chan BuildResult, 100),\n\t\tpriority: make(chan BuildTask, 10),\n\t}\n\n\tmetrics := NewBuildMetrics()\n\n\t// Use first config if provided, otherwise nil\n\tvar config *config.Config\n\tif len(cfg) > 0 {\n\t\tconfig = cfg[0]\n\t}\n\n\treturn &BuildPipeline{\n\t\tcompiler:    compiler,\n\t\tcache:       cache,\n\t\tqueue:       queue,\n\t\tworkers:     workers,\n\t\tregistry:    registry,\n\t\terrorParser: errors.NewErrorParser(),\n\t\tmetrics:     metrics,\n\t\tcallbacks:   make([]BuildCallback, 0),\n\t\t// Initialize object pools for memory optimization\n\t\tobjectPools: NewObjectPools(),\n\t\tslicePools:  NewSlicePools(),\n\t\tworkerPool:  NewWorkerPool(),\n\t\tconfig:      config,\n\t}\n}\n\n// Start starts the build pipeline\nfunc (bp *BuildPipeline) Start(ctx context.Context) {\n\t// Create cancellable context\n\tctx, bp.cancel = context.WithCancel(ctx)\n\n\t// Start workers\n\tfor i := 0; i < bp.workers; i++ {\n\t\tbp.workerWg.Add(1)\n\t\tgo bp.worker(ctx)\n\t}\n\n\t// Start result processor\n\tbp.resultWg.Add(1)\n\tgo bp.processResults(ctx)\n}\n\n// Stop stops the build pipeline and waits for all goroutines to finish\nfunc (bp *BuildPipeline) Stop() {\n\tif bp.cancel != nil {\n\t\tbp.cancel()\n\t}\n\n\t// Wait for all workers to finish\n\tbp.workerWg.Wait()\n\n\t// Wait for result processor to finish\n\tbp.resultWg.Wait()\n}\n\n// Build queues a component for building\nfunc (bp *BuildPipeline) Build(component *types.ComponentInfo) {\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  1,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.tasks <- task:\n\t\t// Task successfully queued\n\tdefault:\n\t\t// Queue full - implement backpressure handling\n\t\t// Log the error and update metrics\n\t\tfmt.Printf(\"Warning: Build queue full, dropping task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"task_queue_full\")\n\n\t\t// Try to handle with retry or priority queue\n\t\tselect {\n\t\tcase bp.queue.priority <- task:\n\t\t\tfmt.Printf(\"Task for %s promoted to priority queue\\n\", component.Name)\n\t\tdefault:\n\t\t\tfmt.Printf(\"Error: Both queues full, build request lost for component %s\\n\", component.Name)\n\t\t\t// TODO: Implement persistent queue or callback for dropped tasks\n\t\t}\n\t}\n}\n\n// BuildWithPriority queues a component for building with high priority\nfunc (bp *BuildPipeline) BuildWithPriority(component *types.ComponentInfo) {\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  10,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.priority <- task:\n\t\t// Priority task successfully queued\n\tdefault:\n\t\t// Priority queue also full - this is a critical error\n\t\tfmt.Printf(\"Critical: Priority queue full, dropping high-priority task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"priority_queue_full\")\n\n\t\t// Could implement emergency handling here (e.g., block briefly or expand queue)\n\t\t// For now, log the critical error\n\t}\n}\n\n// AddCallback adds a callback to be called when builds complete\nfunc (bp *BuildPipeline) AddCallback(callback BuildCallback) {\n\tbp.callbacks = append(bp.callbacks, callback)\n}\n\n// GetMetrics returns the current build metrics\nfunc (bp *BuildPipeline) GetMetrics() BuildMetrics {\n\treturn bp.metrics.GetSnapshot()\n}\n\n// ClearCache clears the build cache\nfunc (bp *BuildPipeline) ClearCache() {\n\tbp.cache.Clear()\n}\n\n// GetCacheStats returns cache statistics\nfunc (bp *BuildPipeline) GetCacheStats() (int, int64, int64) {\n\treturn bp.cache.GetStats()\n}\n\n// getBuildTimeout returns the configured timeout for build operations\nfunc (bp *BuildPipeline) getBuildTimeout() time.Duration {\n\tif bp.config != nil && bp.config.Timeouts.Build > 0 {\n\t\treturn bp.config.Timeouts.Build\n\t}\n\t// Default fallback timeout if no configuration is available\n\treturn 5 * time.Minute\n}\n\n// worker processes build tasks\nfunc (bp *BuildPipeline) worker(ctx context.Context) {\n\tdefer bp.workerWg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase task := <-bp.queue.priority:\n\t\t\tbp.processBuildTask(ctx, task)\n\t\tcase task := <-bp.queue.tasks:\n\t\t\tbp.processBuildTask(ctx, task)\n\t\t}\n\t}\n}\n\nfunc (bp *BuildPipeline) processBuildTask(ctx context.Context, task BuildTask) {\n\tstart := time.Now()\n\n\t// Generate content hash for caching\n\tcontentHash := bp.generateContentHash(task.Component)\n\n\t// Check if context is cancelled before starting work\n\tselect {\n\tcase <-ctx.Done():\n\t\t// Context cancelled, return error\n\t\tbuildResult := bp.objectPools.GetBuildResult()\n\t\tbuildResult.Component = task.Component\n\t\tbuildResult.Output = nil\n\t\tbuildResult.Error = ctx.Err()\n\t\tbuildResult.ParsedErrors = nil\n\t\tbuildResult.Duration = time.Since(start)\n\t\tbuildResult.CacheHit = false\n\t\tbuildResult.Hash = contentHash\n\n\t\t// Non-blocking send to results channel\n\t\tselect {\n\t\tcase bp.queue.results <- *buildResult:\n\t\tdefault:\n\t\t\tbp.metrics.RecordDroppedResult(task.Component.Name, \"results_queue_full_cancelled\")\n\t\t}\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\tdefault:\n\t}\n\n\t// Check cache first\n\tif result, found := bp.cache.Get(contentHash); found {\n\t\t// Use object pool for cache hit result\n\t\tbuildResult := bp.objectPools.GetBuildResult()\n\t\tbuildResult.Component = task.Component\n\t\tbuildResult.Output = result\n\t\tbuildResult.Error = nil\n\t\tbuildResult.ParsedErrors = nil\n\t\tbuildResult.Duration = time.Since(start)\n\t\tbuildResult.CacheHit = true\n\t\tbuildResult.Hash = contentHash\n\n\t\t// Non-blocking send to results channel to prevent worker hangs\n\t\tselect {\n\t\tcase bp.queue.results <- *buildResult:\n\t\t\t// Cache hit result successfully queued\n\t\tcase <-ctx.Done():\n\t\t\t// Context cancelled while sending result\n\t\t\tbuildResult.Error = ctx.Err()\n\t\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\t\treturn\n\t\tdefault:\n\t\t\t// Results queue full - this could cause result loss\n\t\t\tfmt.Printf(\"Warning: Results queue full, dropping cache hit result for component %s\\n\", buildResult.Component.Name)\n\t\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full_cache_hit\")\n\t\t}\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\t}\n\n\t// Create timeout context for build operation based on configuration\n\tbuildTimeout := bp.getBuildTimeout()\n\tbuildCtx, cancel := context.WithTimeout(ctx, buildTimeout)\n\tdefer cancel()\n\n\t// Execute build with pooled output buffer and context-based timeout\n\toutput, err := bp.compiler.CompileWithPools(buildCtx, task.Component, bp.objectPools)\n\n\t// Parse errors if build failed\n\tvar parsedErrors []*errors.ParsedError\n\tif err != nil {\n\t\t// Wrap the error with build context for better debugging\n\t\terr = errors.WrapBuild(err, errors.ErrCodeBuildFailed, \n\t\t\t\"component compilation failed\", task.Component.Name).\n\t\t\tWithLocation(task.Component.FilePath, 0, 0)\n\t\tparsedErrors = bp.errorParser.ParseError(string(output))\n\t}\n\n\t// Use object pool for build result\n\tbuildResult := bp.objectPools.GetBuildResult()\n\tbuildResult.Component = task.Component\n\tbuildResult.Output = output\n\tbuildResult.Error = err\n\tbuildResult.ParsedErrors = parsedErrors\n\tbuildResult.Duration = time.Since(start)\n\tbuildResult.CacheHit = false\n\tbuildResult.Hash = contentHash\n\n\t// Cache successful builds\n\tif err == nil {\n\t\tbp.cache.Set(contentHash, output)\n\t}\n\n\t// Non-blocking send to results channel to prevent worker hangs\n\tselect {\n\tcase bp.queue.results <- *buildResult:\n\t\t// Result successfully queued\n\tdefault:\n\t\t// Results queue full - this could cause result loss\n\t\tfmt.Printf(\"Warning: Results queue full, dropping result for component %s\\n\", buildResult.Component.Name)\n\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full\")\n\t}\n\tbp.objectPools.PutBuildResult(buildResult)\n}\n\nfunc (bp *BuildPipeline) processResults(ctx context.Context) {\n\tdefer bp.resultWg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase result := <-bp.queue.results:\n\t\t\tbp.handleBuildResult(result)\n\t\t}\n\t}\n}\n\nfunc (bp *BuildPipeline) handleBuildResult(result BuildResult) {\n\t// Update metrics\n\tbp.metrics.RecordBuild(result)\n\n\t// Print result\n\tif result.Error != nil {\n\t\tfmt.Printf(\"Build failed for %s: %v\\n\", result.Component.Name, result.Error)\n\t\tif len(result.ParsedErrors) > 0 {\n\t\t\tfmt.Println(\"Parsed errors:\")\n\t\t\tfor _, err := range result.ParsedErrors {\n\t\t\t\tfmt.Print(err.FormatError())\n\t\t\t}\n\t\t}\n\t} else {\n\t\tstatus := \"succeeded\"\n\t\tif result.CacheHit {\n\t\t\tstatus = \"cached\"\n\t\t}\n\t\tfmt.Printf(\"Build %s for %s in %v\\n\", status, result.Component.Name, result.Duration)\n\t}\n\n\t// Call callbacks\n\tfor _, callback := range bp.callbacks {\n\t\tcallback(result)\n\t}\n}\n\n// generateContentHash generates a hash for component content with optimized single I/O operation\nfunc (bp *BuildPipeline) generateContentHash(component *types.ComponentInfo) string {\n\t// OPTIMIZATION: Use Stat() first to get metadata without opening file\n\t// This reduces file I/O operations by 70-90% for cached files\n\tstat, err := os.Stat(component.FilePath)\n\tif err != nil {\n\t\t// File not accessible, return fallback hash\n\t\t// Note: We don't need to wrap this error as it's an internal optimization\n\t\treturn component.FilePath\n\t}\n\n\t// Create metadata-based hash key for cache lookup\n\tmetadataKey := fmt.Sprintf(\"%s:%d:%d\", component.FilePath, stat.ModTime().Unix(), stat.Size())\n\n\t// Two-tier cache system: Check metadata cache first (no file I/O)\n\tif hash, found := bp.cache.GetHash(metadataKey); found {\n\t\t// Cache hit - no file I/O needed, just return cached hash\n\t\treturn hash\n\t}\n\n\t// Cache miss: Now we need to read file content and generate hash\n\t// Only open file when we actually need to read content\n\tfile, err := os.Open(component.FilePath)\n\tif err != nil {\n\t\treturn component.FilePath\n\t}\n\tdefer file.Close()\n\n\t// Use mmap for large files (>64KB) for better performance\n\tvar content []byte\n\tif stat.Size() > 64*1024 {\n\t\t// Use mmap for large files\n\t\tcontent, err = bp.readFileWithMmap(file, stat.Size())\n\t\tif err != nil {\n\t\t\t// Fallback to regular read\n\t\t\tcontent, err = io.ReadAll(file)\n\t\t}\n\t} else {\n\t\t// Regular read for small files\n\t\tcontent, err = io.ReadAll(file)\n\t}\n\n\tif err != nil {\n\t\t// Fallback to metadata-based hash\n\t\treturn fmt.Sprintf(\"%s:%d\", component.FilePath, stat.ModTime().Unix())\n\t}\n\n\t// Generate content hash using CRC32 Castagnoli for faster file change detection\n\tcrcHash := crc32.Checksum(content, crcTable)\n\tcontentHash := strconv.FormatUint(uint64(crcHash), 16)\n\n\t// Cache the hash with metadata key for future lookups\n\tbp.cache.SetHash(metadataKey, contentHash)\n\n\treturn contentHash\n}\n\n// readFileWithMmap reads file content using memory mapping for better performance on large files\nfunc (bp *BuildPipeline) readFileWithMmap(file *os.File, size int64) ([]byte, error) {\n\t// Memory map the file for efficient reading\n\tmmap, err := syscall.Mmap(int(file.Fd()), 0, int(size), syscall.PROT_READ, syscall.MAP_SHARED)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Copy the mapped data to avoid keeping the mapping open\n\tcontent := make([]byte, size)\n\tcopy(content, mmap)\n\n\t// Unmap the memory\n\tif err := syscall.Munmap(mmap); err != nil {\n\t\t// Log warning but don't fail - we have the content\n\t\t// Could add logging here if logger is available\n\t}\n\n\treturn content, nil\n}\n\n// generateContentHashesBatch processes multiple components in a single batch for better I/O efficiency\nfunc (bp *BuildPipeline) generateContentHashesBatch(components []*types.ComponentInfo) map[string]string {\n\tresults := make(map[string]string, len(components))\n\n\t// Group components by whether they need content reading (cache misses)\n\tvar needsReading []*types.ComponentInfo\n\n\t// First pass: check metadata-based cache for all components (no file I/O)\n\tfor _, component := range components {\n\t\t// OPTIMIZATION: Use efficient Stat() + metadata cache check first\n\t\tif stat, err := os.Stat(component.FilePath); err == nil {\n\t\t\tmetadataKey := fmt.Sprintf(\"%s:%d:%d\", component.FilePath, stat.ModTime().Unix(), stat.Size())\n\n\t\t\t// Check cache with metadata key\n\t\t\tif hash, found := bp.cache.GetHash(metadataKey); found {\n\t\t\t\t// Cache hit - no file reading needed\n\t\t\t\tresults[component.FilePath] = hash\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// Cache miss - needs content reading\n\t\tneedsReading = append(needsReading, component)\n\t}\n\n\t// Second pass: batch process cache misses with optimized I/O\n\tif len(needsReading) > 0 {\n\t\thashResults := bp.batchReadAndHash(needsReading)\n\t\tfor filePath, hash := range hashResults {\n\t\t\tresults[filePath] = hash\n\t\t}\n\t}\n\n\treturn results\n}\n\n// batchReadAndHash reads and hashes multiple files efficiently\nfunc (bp *BuildPipeline) batchReadAndHash(components []*types.ComponentInfo) map[string]string {\n\tresults := make(map[string]string, len(components))\n\n\t// Process each component with optimized I/O\n\tfor _, component := range components {\n\t\thash := bp.generateContentHash(component)\n\t\tresults[component.FilePath] = hash\n\t}\n\n\treturn results\n}\n\n// FileDiscoveryResult represents the result of discovering files in a directory\ntype FileDiscoveryResult struct {\n\tFiles       []*types.ComponentInfo\n\tErrors      []error\n\tDuration    time.Duration\n\tDiscovered  int64\n\tSkipped     int64\n}\n\n// FileDiscoveryStats tracks file discovery performance metrics\ntype FileDiscoveryStats struct {\n\tTotalFiles     int64\n\tProcessedFiles int64\n\tSkippedFiles   int64\n\tErrors         int64\n\tDuration       time.Duration\n\tWorkerCount    int\n}\n\n// ParallelFileProcessor provides parallel file processing capabilities\ntype ParallelFileProcessor struct {\n\tworkerCount int\n\tmaxDepth    int\n\tfilters     []string\n\tstats       *FileDiscoveryStats\n}\n\n// NewParallelFileProcessor creates a new parallel file processor\nfunc NewParallelFileProcessor(workerCount int) *ParallelFileProcessor {\n\treturn &ParallelFileProcessor{\n\t\tworkerCount: workerCount,\n\t\tmaxDepth:    10, // Default max depth\n\t\tfilters:     []string{\".templ\"},\n\t\tstats:       &FileDiscoveryStats{},\n\t}\n}\n\n// DiscoverFiles discovers component files in parallel using filepath.WalkDir\nfunc (pfp *ParallelFileProcessor) DiscoverFiles(ctx context.Context, rootPaths []string) (*FileDiscoveryResult, error) {\n\tstart := time.Now()\n\tdefer func() {\n\t\tpfp.stats.Duration = time.Since(start)\n\t}()\n\n\t// Create channels for work distribution\n\tpathCh := make(chan string, len(rootPaths))\n\tresultCh := make(chan *types.ComponentInfo, 100)\n\terrorCh := make(chan error, 100)\n\n\t// Start workers\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < pfp.workerCount; i++ {\n\t\twg.Add(1)\n\t\tgo pfp.worker(ctx, pathCh, resultCh, errorCh, &wg)\n\t}\n\n\t// Send root paths to workers\n\tgo func() {\n\t\tdefer close(pathCh)\n\t\tfor _, path := range rootPaths {\n\t\t\tselect {\n\t\t\tcase pathCh <- path:\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Collect results\n\tvar files []*types.ComponentInfo\n\tvar errors []error\n\tvar discovered, skipped int64\n\n\t// Result collection goroutine\n\tdone := make(chan struct{})\n\tgo func() {\n\t\tdefer close(done)\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase component, ok := <-resultCh:\n\t\t\t\tif !ok {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tfiles = append(files, component)\n\t\t\t\tatomic.AddInt64(&discovered, 1)\n\t\t\tcase err, ok := <-errorCh:\n\t\t\t\tif !ok {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\terrors = append(errors, err)\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Wait for workers to complete\n\twg.Wait()\n\tclose(resultCh)\n\tclose(errorCh)\n\n\t// Wait for result collection to complete\n\t<-done\n\n\t// Update stats\n\tatomic.StoreInt64(&pfp.stats.ProcessedFiles, discovered)\n\tatomic.StoreInt64(&pfp.stats.SkippedFiles, skipped)\n\tatomic.StoreInt64(&pfp.stats.Errors, int64(len(errors)))\n\tpfp.stats.WorkerCount = pfp.workerCount\n\n\treturn &FileDiscoveryResult{\n\t\tFiles:      files,\n\t\tErrors:     errors,\n\t\tDuration:   time.Since(start),\n\t\tDiscovered: discovered,\n\t\tSkipped:    skipped,\n\t}, nil\n}\n\n// worker processes file discovery work\nfunc (pfp *ParallelFileProcessor) worker(ctx context.Context, pathCh <-chan string, resultCh chan<- *types.ComponentInfo, errorCh chan<- error, wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase rootPath, ok := <-pathCh:\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Walk directory tree using filepath.WalkDir for better performance\n\t\t\terr := filepath.WalkDir(rootPath, func(path string, d os.DirEntry, err error) error {\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\t// Skip directories\n\t\t\t\tif d.IsDir() {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\n\t\t\t\t// Check if file matches our filters\n\t\t\t\tif !pfp.matchesFilter(path) {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\n\t\t\t\t// Create component info\n\t\t\t\tcomponent := &types.ComponentInfo{\n\t\t\t\t\tName:       pfp.extractComponentName(path),\n\t\t\t\t\tFilePath:   path,\n\t\t\t\t\tPackage:    pfp.extractPackage(path),\n\t\t\t\t\tParameters: []types.ParameterInfo{},\n\t\t\t\t}\n\n\t\t\t\t// Send result non-blocking\n\t\t\t\tselect {\n\t\t\t\tcase resultCh <- component:\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn ctx.Err()\n\t\t\t\tdefault:\n\t\t\t\t\t// Channel full, skip this component\n\t\t\t\t\tatomic.AddInt64(&pfp.stats.SkippedFiles, 1)\n\t\t\t\t}\n\n\t\t\t\treturn nil\n\t\t\t})\n\n\t\t\tif err != nil {\n\t\t\t\tselect {\n\t\t\t\tcase errorCh <- err:\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\tdefault:\n\t\t\t\t\t// Error channel full, skip error\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// matchesFilter checks if a file path matches the processor's filters\nfunc (pfp *ParallelFileProcessor) matchesFilter(path string) bool {\n\tfor _, filter := range pfp.filters {\n\t\tif strings.HasSuffix(path, filter) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// extractComponentName extracts component name from file path\nfunc (pfp *ParallelFileProcessor) extractComponentName(path string) string {\n\tbase := filepath.Base(path)\n\text := filepath.Ext(base)\n\treturn strings.TrimSuffix(base, ext)\n}\n\n// extractPackage extracts package name from file path\nfunc (pfp *ParallelFileProcessor) extractPackage(path string) string {\n\tdir := filepath.Dir(path)\n\treturn filepath.Base(dir)\n}\n\n// ProcessFilesBatch processes multiple files in parallel batches\nfunc (bp *BuildPipeline) ProcessFilesBatch(ctx context.Context, components []*types.ComponentInfo, batchSize int) (*FileDiscoveryResult, error) {\n\tstart := time.Now()\n\tvar totalDiscovered, totalSkipped int64\n\tvar allErrors []error\n\tvar allResults []*types.ComponentInfo\n\n\t// Process components in batches\n\tfor i := 0; i < len(components); i += batchSize {\n\t\tend := i + batchSize\n\t\tif end > len(components) {\n\t\t\tend = len(components)\n\t\t}\n\n\t\tbatch := components[i:end]\n\t\thashes := bp.generateContentHashesBatch(batch)\n\n\t\t// Process batch with caching\n\t\tfor _, component := range batch {\n\t\t\thash, exists := hashes[component.FilePath]\n\t\t\tif !exists {\n\t\t\t\tatomic.AddInt64(&totalSkipped, 1)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check cache first\n\t\t\tif _, found := bp.cache.Get(hash); found {\n\t\t\t\t// Cache hit, no processing needed\n\t\t\t\tallResults = append(allResults, component)\n\t\t\t\tatomic.AddInt64(&totalDiscovered, 1)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Process component\n\t\t\tallResults = append(allResults, component)\n\t\t\tatomic.AddInt64(&totalDiscovered, 1)\n\t\t}\n\t}\n\n\treturn &FileDiscoveryResult{\n\t\tFiles:      allResults,\n\t\tErrors:     allErrors,\n\t\tDuration:   time.Since(start),\n\t\tDiscovered: totalDiscovered,\n\t\tSkipped:    totalSkipped,\n\t}, nil\n}\n\n// BuildDirectory builds all components in a directory using parallel processing\nfunc (bp *BuildPipeline) BuildDirectory(ctx context.Context, rootPath string) error {\n\t// Create parallel file processor\n\tprocessor := NewParallelFileProcessor(bp.workers)\n\n\t// Discover files\n\tresult, err := processor.DiscoverFiles(ctx, []string{rootPath})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to discover files: %w\", err)\n\t}\n\n\t// Queue all discovered components for building\n\tfor _, component := range result.Files {\n\t\tbp.Build(component)\n\t}\n\n\tfmt.Printf(\"Directory build queued: %d components discovered in %v\\n\", \n\t\tresult.Discovered, result.Duration)\n\n\treturn nil\n}\n\n// GetFileDiscoveryStats returns file discovery performance statistics\nfunc (pfp *ParallelFileProcessor) GetFileDiscoveryStats() FileDiscoveryStats {\n\treturn FileDiscoveryStats{\n\t\tTotalFiles:     atomic.LoadInt64(&pfp.stats.TotalFiles),\n\t\tProcessedFiles: atomic.LoadInt64(&pfp.stats.ProcessedFiles),\n\t\tSkippedFiles:   atomic.LoadInt64(&pfp.stats.SkippedFiles),\n\t\tErrors:         atomic.LoadInt64(&pfp.stats.Errors),\n\t\tDuration:       pfp.stats.Duration,\n\t\tWorkerCount:    pfp.stats.WorkerCount,\n\t}\n}\n",
      "structuredPatch": [
        {
          "oldStart": 174,
          "oldLines": 6,
          "newStart": 174,
          "newLines": 13,
          "lines": [
            " ",
            " // Build queues a component for building",
            " func (bp *BuildPipeline) Build(component *types.ComponentInfo) {",
            "+  // Check if pipeline is shut down",
            "+  if bp.cancel == nil {",
            "+    fmt.Printf(\"Error: Build pipeline not started, dropping task for component %s\\n\", component.Name)",
            "+    bp.metrics.RecordDroppedTask(component.Name, \"pipeline_not_started\")",
            "+    return",
            "+  }",
            "+",
            "   task := BuildTask{",
            "     Component: component,",
            "     Priority:  1,"
          ]
        }
      ],
      "userModified": false,
      "replaceAll": false
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Edit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/cmd/audit.go",
      "old_string": "\t// Load configuration\n\tcfg, err := config.LoadConfig()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to load configuration: %w\", err)\n\t}",
      "new_string": "\t// Load configuration\n\tcfg, err := config.Load()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to load configuration: %w\", err)\n\t}"
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/cmd/audit.go",
      "oldString": "\t// Load configuration\n\tcfg, err := config.LoadConfig()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to load configuration: %w\", err)\n\t}",
      "newString": "\t// Load configuration\n\tcfg, err := config.Load()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to load configuration: %w\", err)\n\t}",
      "originalFile": "package cmd\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/accessibility\"\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/conneroisu/templar/internal/interfaces\"\n\t\"github.com/conneroisu/templar/internal/logging\"\n\t\"github.com/conneroisu/templar/internal/registry\"\n\t\"github.com/conneroisu/templar/internal/renderer\"\n\t\"github.com/conneroisu/templar/internal/scanner\"\n\t\"github.com/spf13/cobra\"\n)\n\nvar (\n\tauditComponentName   string\n\tauditWCAGLevel       string\n\tauditOutputFormat    string\n\tauditOutputFile      string\n\tauditIncludeHTML     bool\n\tauditFixableOnly     bool\n\tauditSeverityFilter  string\n\tauditQuiet           bool\n\tauditVerbose         bool\n\tauditMaxViolations   int\n\tauditGenerateReport  bool\n\tauditShowSuggestions bool\n\tauditAutoFix         bool\n\tauditShowGuidance    bool\n\tauditGuidanceOnly    bool\n)\n\n// auditCmd represents the audit command\nvar auditCmd = &cobra.Command{\n\tUse:   \"audit [component-name]\",\n\tShort: \"Run accessibility audit on components\",\n\tLong: `Run comprehensive accessibility audits on templ components to identify\nWCAG compliance issues and get actionable suggestions for improvements.\n\nThe audit command can test individual components or all components in your project.\nIt provides detailed reports with severity levels, WCAG criteria mapping, and\nspecific suggestions for fixing accessibility issues.\n\nExamples:\n  # Audit all components\n  templar audit\n\n  # Audit specific component\n  templar audit Button\n\n  # Audit with specific WCAG level\n  templar audit --wcag-level AA\n\n  # Generate HTML report\n  templar audit --output html --output-file report.html\n\n  # Show only critical issues\n  templar audit --severity error\n\n  # Apply automatic fixes\n  templar audit --auto-fix`,\n\tValidArgsFunction: func(cmd *cobra.Command, args []string, toComplete string) ([]string, cobra.ShellCompDirective) {\n\t\treturn getComponentCompletions(toComplete), cobra.ShellCompDirectiveNoFileComp\n\t},\n\tRunE: runAuditCommand,\n}\n\nfunc init() {\n\trootCmd.AddCommand(auditCmd)\n\n\tauditCmd.Flags().StringVarP(&auditComponentName, \"component\", \"c\", \"\", \"Specific component to audit (if not provided as argument)\")\n\tauditCmd.Flags().StringVarP(&auditWCAGLevel, \"wcag-level\", \"w\", \"AA\", \"WCAG compliance level to test against (A, AA, AAA)\")\n\tauditCmd.Flags().StringVarP(&auditOutputFormat, \"output\", \"o\", \"console\", \"Output format (console, json, html, markdown)\")\n\tauditCmd.Flags().StringVarP(&auditOutputFile, \"output-file\", \"f\", \"\", \"Output file path (stdout if not specified)\")\n\tauditCmd.Flags().BoolVar(&auditIncludeHTML, \"include-html\", false, \"Include HTML snapshot in report\")\n\tauditCmd.Flags().BoolVar(&auditFixableOnly, \"fixable-only\", false, \"Show only issues that can be automatically fixed\")\n\tauditCmd.Flags().StringVarP(&auditSeverityFilter, \"severity\", \"s\", \"\", \"Filter by severity level (error, warning, info)\")\n\tauditCmd.Flags().BoolVarP(&auditQuiet, \"quiet\", \"q\", false, \"Suppress non-error output\")\n\tauditCmd.Flags().BoolVarP(&auditVerbose, \"verbose\", \"v\", false, \"Enable verbose output\")\n\tauditCmd.Flags().IntVarP(&auditMaxViolations, \"max-violations\", \"m\", 0, \"Maximum number of violations to report (0 = unlimited)\")\n\tauditCmd.Flags().BoolVar(&auditGenerateReport, \"generate-report\", false, \"Generate detailed accessibility report\")\n\tauditCmd.Flags().BoolVar(&auditShowSuggestions, \"show-suggestions\", true, \"Include suggestions in output\")\n\tauditCmd.Flags().BoolVar(&auditAutoFix, \"auto-fix\", false, \"Attempt to automatically fix issues where possible\")\n\tauditCmd.Flags().BoolVar(&auditShowGuidance, \"show-guidance\", false, \"Include detailed accessibility guidance\")\n\tauditCmd.Flags().BoolVar(&auditGuidanceOnly, \"guidance-only\", false, \"Show only guidance without running audit\")\n}\n\nfunc runAuditCommand(cmd *cobra.Command, args []string) error {\n\tctx := context.Background()\n\n\t// Determine component name from args or flag\n\tcomponentName := auditComponentName\n\tif len(args) > 0 {\n\t\tcomponentName = args[0]\n\t}\n\n\t// Handle guidance-only mode\n\tif auditGuidanceOnly {\n\t\treturn showGuidanceOnly(componentName)\n\t}\n\n\t// Initialize logging\n\tloggerConfig := &logging.LoggerConfig{\n\t\tLevel:     logging.LevelInfo,\n\t\tFormat:    \"text\", \n\t\tComponent: \"audit\",\n\t\tOutput:    os.Stdout,\n\t}\n\tif auditQuiet {\n\t\tloggerConfig.Level = logging.LevelError\n\t} else if auditVerbose {\n\t\tloggerConfig.Level = logging.LevelDebug\n\t}\n\tlogger := logging.NewLogger(loggerConfig)\n\n\t// Load configuration\n\tcfg, err := config.LoadConfig()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to load configuration: %w\", err)\n\t}\n\n\t// Initialize component registry and scanner\n\tcomponentRegistry := registry.NewComponentRegistry(logger)\n\tcomponentScanner := scanner.NewScanner(logger)\n\n\t// Scan components\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Scanning components...\")\n\t}\n\n\tcomponents, err := componentScanner.ScanDirectory(cfg.Components.ScanPaths[0])\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to scan components: %w\", err)\n\t}\n\n\t// Register components\n\tfor _, component := range components {\n\t\tcomponentRegistry.RegisterComponent(component)\n\t}\n\n\t// Create renderer\n\tcomponentRenderer := renderer.NewComponentRenderer(logger)\n\n\t// Initialize accessibility tester\n\ttesterConfig := accessibility.TesterConfig{\n\t\tDefaultWCAGLevel:    parseWCAGLevel(auditWCAGLevel),\n\t\tDefaultTimeout:      30 * time.Second,\n\t\tEnableRealTimeWarn:  false,\n\t\tMaxConcurrentTests:  1,\n\t}\n\n\ttester := accessibility.NewComponentAccessibilityTester(\n\t\tcomponentRegistry,\n\t\tcomponentRenderer,\n\t\tlogger,\n\t\ttesterConfig,\n\t)\n\n\t// Perform audit\n\tif componentName != \"\" {\n\t\treturn runSingleComponentAudit(ctx, tester, componentName, logger)\n\t} else {\n\t\treturn runAllComponentsAudit(ctx, tester, componentRegistry, logger)\n\t}\n}\n\nfunc runSingleComponentAudit(ctx context.Context, tester accessibility.AccessibilityTester, componentName string, logger logging.Logger) error {\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Running accessibility audit\", \"component\", componentName)\n\t}\n\n\t// Run accessibility test\n\treport, err := tester.TestComponent(ctx, componentName, nil)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"accessibility audit failed for %s: %w\", componentName, err)\n\t}\n\n\t// Apply filters\n\treport = applyReportFilters(report)\n\n\t// Apply auto-fixes if requested\n\tif auditAutoFix && len(report.Violations) > 0 {\n\t\tfixedCount, err := applyAutoFixes(ctx, tester, report)\n\t\tif err != nil {\n\t\t\tlogger.Warn(ctx, err, \"Failed to apply auto-fixes\")\n\t\t} else if fixedCount > 0 {\n\t\t\tlogger.Info(ctx, \"Applied automatic fixes\", \"count\", fixedCount)\n\t\t}\n\t}\n\n\t// Output results\n\treturn outputAuditResults([]*accessibility.AccessibilityReport{report}, logger)\n}\n\nfunc runAllComponentsAudit(ctx context.Context, tester accessibility.AccessibilityTester, registry interfaces.ComponentRegistry, logger logging.Logger) error {\n\tcomponents := registry.GetAllComponents()\n\t\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Running accessibility audit on all components\", \"count\", len(components))\n\t}\n\n\treports := []*accessibility.AccessibilityReport{}\n\ttotalViolations := 0\n\ttotalAutoFixes := 0\n\n\tfor i, component := range components {\n\t\tif auditVerbose {\n\t\t\tlogger.Info(ctx, \"Auditing component\", \n\t\t\t\t\"component\", component.Name,\n\t\t\t\t\"progress\", fmt.Sprintf(\"%d/%d\", i+1, len(components)))\n\t\t}\n\n\t\t// Run accessibility test\n\t\treport, err := tester.TestComponent(ctx, component.Name, nil)\n\t\tif err != nil {\n\t\t\tlogger.Warn(ctx, err, \"Failed to audit component\", \"component\", component.Name)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Apply filters\n\t\treport = applyReportFilters(report)\n\t\t\n\t\t// Apply auto-fixes if requested\n\t\tif auditAutoFix && len(report.Violations) > 0 {\n\t\t\tfixedCount, err := applyAutoFixes(ctx, tester, report)\n\t\t\tif err != nil {\n\t\t\t\tlogger.Warn(ctx, err, \"Failed to apply auto-fixes\", \"component\", component.Name)\n\t\t\t} else {\n\t\t\t\ttotalAutoFixes += fixedCount\n\t\t\t}\n\t\t}\n\n\t\treports = append(reports, report)\n\t\ttotalViolations += len(report.Violations)\n\t}\n\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Audit completed\",\n\t\t\t\"components\", len(reports),\n\t\t\t\"total_violations\", totalViolations)\n\t\t\n\t\tif auditAutoFix && totalAutoFixes > 0 {\n\t\t\tlogger.Info(ctx, \"Applied automatic fixes\", \"total_fixes\", totalAutoFixes)\n\t\t}\n\t}\n\n\t// Output results\n\treturn outputAuditResults(reports, logger)\n}\n\nfunc applyReportFilters(report *accessibility.AccessibilityReport) *accessibility.AccessibilityReport {\n\tfilteredViolations := []accessibility.AccessibilityViolation{}\n\n\tfor _, violation := range report.Violations {\n\t\t// Apply severity filter\n\t\tif auditSeverityFilter != \"\" {\n\t\t\texpectedSeverity := parseSeverity(auditSeverityFilter)\n\t\t\tif violation.Severity != expectedSeverity {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// Apply fixable filter\n\t\tif auditFixableOnly && !violation.CanAutoFix {\n\t\t\tcontinue\n\t\t}\n\n\t\tfilteredViolations = append(filteredViolations, violation)\n\t}\n\n\t// Apply max violations limit\n\tif auditMaxViolations > 0 && len(filteredViolations) > auditMaxViolations {\n\t\tfilteredViolations = filteredViolations[:auditMaxViolations]\n\t}\n\n\t// Update report\n\treport.Violations = filteredViolations\n\t\n\t// Recalculate summary\n\treport.Summary = calculateAccessibilitySummary(filteredViolations, report.Passed)\n\n\treturn report\n}\n\nfunc applyAutoFixes(ctx context.Context, tester accessibility.AccessibilityTester, report *accessibility.AccessibilityReport) (int, error) {\n\tif componentTester, ok := tester.(*accessibility.ComponentAccessibilityTester); ok {\n\t\tautoFixableViolations := []accessibility.AccessibilityViolation{}\n\t\tfor _, violation := range report.Violations {\n\t\t\tif violation.CanAutoFix {\n\t\t\t\tautoFixableViolations = append(autoFixableViolations, violation)\n\t\t\t}\n\t\t}\n\n\t\tif len(autoFixableViolations) == 0 {\n\t\t\treturn 0, nil\n\t\t}\n\n\t\t// Apply auto-fixes (this would need integration with file system)\n\t\t_, err := componentTester.AutoFix(ctx, report.HTMLSnapshot, autoFixableViolations)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\n\t\treturn len(autoFixableViolations), nil\n\t}\n\n\treturn 0, fmt.Errorf(\"auto-fix not supported for this tester type\")\n}\n\nfunc outputAuditResults(reports []*accessibility.AccessibilityReport, logger logging.Logger) error {\n\tswitch auditOutputFormat {\n\tcase \"json\":\n\t\treturn outputJSON(reports)\n\tcase \"html\":\n\t\treturn outputHTML(reports)\n\tcase \"markdown\":\n\t\treturn outputMarkdown(reports)\n\tcase \"console\":\n\t\tfallthrough\n\tdefault:\n\t\treturn outputConsole(reports, logger)\n\t}\n}\n\nfunc outputJSON(reports []*accessibility.AccessibilityReport) error {\n\toutput := reports\n\tif len(reports) == 1 {\n\t\toutput = reports[0] // Single component audit returns single report\n\t}\n\n\tjsonData, err := json.MarshalIndent(output, \"\", \"  \")\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to marshal JSON: %w\", err)\n\t}\n\n\treturn writeOutput(string(jsonData))\n}\n\nfunc outputHTML(reports []*accessibility.AccessibilityReport) error {\n\thtml := generateHTMLReport(reports)\n\treturn writeOutput(html)\n}\n\nfunc outputMarkdown(reports []*accessibility.AccessibilityReport) error {\n\tmarkdown := generateMarkdownReport(reports)\n\treturn writeOutput(markdown)\n}\n\nfunc outputConsole(reports []*accessibility.AccessibilityReport, logger logging.Logger) error {\n\tctx := context.Background()\n\t\n\tif len(reports) == 0 {\n\t\tfmt.Println(\"No components audited.\")\n\t\treturn nil\n\t}\n\n\t// Summary statistics\n\ttotalComponents := len(reports)\n\ttotalViolations := 0\n\tcriticalViolations := 0\n\tcomponentsWithIssues := 0\n\toverallScoreSum := 0.0\n\n\tfor _, report := range reports {\n\t\ttotalViolations += len(report.Violations)\n\t\toverallScoreSum += report.Summary.OverallScore\n\t\t\n\t\tif len(report.Violations) > 0 {\n\t\t\tcomponentsWithIssues++\n\t\t}\n\t\t\n\t\tfor _, violation := range report.Violations {\n\t\t\tif violation.Impact == accessibility.ImpactCritical {\n\t\t\t\tcriticalViolations++\n\t\t\t}\n\t\t}\n\t}\n\n\taverageScore := overallScoreSum / float64(totalComponents)\n\n\t// Print summary\n\tfmt.Printf(\"\\n\ud83d\udd0d Accessibility Audit Summary\\n\")\n\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\tfmt.Printf(\"Components audited:     %d\\n\", totalComponents)\n\tfmt.Printf(\"Components with issues: %d\\n\", componentsWithIssues)\n\tfmt.Printf(\"Total violations:       %d\\n\", totalViolations)\n\tfmt.Printf(\"Critical violations:    %d\\n\", criticalViolations)\n\tfmt.Printf(\"Average score:          %.1f/100\\n\", averageScore)\n\t\n\t// Overall status\n\tvar status string\n\tvar statusIcon string\n\tif criticalViolations > 0 {\n\t\tstatus = \"CRITICAL ISSUES FOUND\"\n\t\tstatusIcon = \"\ud83d\udea8\"\n\t} else if totalViolations > 0 {\n\t\tstatus = \"ISSUES FOUND\"\n\t\tstatusIcon = \"\u26a0\ufe0f\"\n\t} else {\n\t\tstatus = \"ALL CHECKS PASSED\"\n\t\tstatusIcon = \"\u2705\"\n\t}\n\t\n\tfmt.Printf(\"Status:                 %s %s\\n\\n\", statusIcon, status)\n\n\t// Detailed component results\n\tif auditVerbose || len(reports) == 1 {\n\t\tfor _, report := range reports {\n\t\t\toutputComponentDetails(report)\n\t\t}\n\t} else if totalViolations > 0 {\n\t\t// Show only components with issues\n\t\tfmt.Printf(\"Components with accessibility issues:\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tfor _, report := range reports {\n\t\t\tif len(report.Violations) > 0 {\n\t\t\t\toutputComponentSummary(report)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Show suggestions if enabled\n\tif auditShowSuggestions && totalViolations > 0 {\n\t\tfmt.Printf(\"\\n\ud83d\udca1 Top Suggestions\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tsuggestions := aggregateSuggestions(reports)\n\t\tfor i, suggestion := range suggestions {\n\t\t\tif i >= 5 { // Limit to top 5\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfmt.Printf(\"%d. %s\\n\", i+1, suggestion.Title)\n\t\t\tif suggestion.Description != \"\" {\n\t\t\t\tfmt.Printf(\"   %s\\n\", suggestion.Description)\n\t\t\t}\n\t\t\tfmt.Printf(\"\\n\")\n\t\t}\n\t}\n\t\n\t// Show detailed guidance if enabled and there are violations\n\tif totalViolations > 0 {\n\t\tallViolations := []accessibility.AccessibilityViolation{}\n\t\tfor _, report := range reports {\n\t\t\tallViolations = append(allViolations, report.Violations...)\n\t\t}\n\t\tshowGuidanceForViolations(allViolations)\n\t}\n\n\treturn nil\n}\n\nfunc outputComponentDetails(report *accessibility.AccessibilityReport) {\n\tcomponentName := report.ComponentName\n\tif componentName == \"\" {\n\t\tcomponentName = \"Unknown Component\"\n\t}\n\n\tscoreColor := getScoreColor(report.Summary.OverallScore)\n\t\n\tfmt.Printf(\"\ud83d\udce6 %s (Score: %s%.1f/100%s)\\n\", \n\t\tcomponentName, scoreColor, report.Summary.OverallScore, \"\\033[0m\")\n\tfmt.Printf(\"   File: %s\\n\", report.ComponentFile)\n\t\n\tif len(report.Violations) == 0 {\n\t\tfmt.Printf(\"   \u2705 No accessibility issues found\\n\\n\")\n\t\treturn\n\t}\n\n\t// Group violations by severity\n\terrorViolations := []accessibility.AccessibilityViolation{}\n\twarningViolations := []accessibility.AccessibilityViolation{}\n\tinfoViolations := []accessibility.AccessibilityViolation{}\n\n\tfor _, violation := range report.Violations {\n\t\tswitch violation.Severity {\n\t\tcase accessibility.SeverityError:\n\t\t\terrorViolations = append(errorViolations, violation)\n\t\tcase accessibility.SeverityWarning:\n\t\t\twarningViolations = append(warningViolations, violation)\n\t\tcase accessibility.SeverityInfo:\n\t\t\tinfoViolations = append(infoViolations, violation)\n\t\t}\n\t}\n\n\t// Output violations by severity\n\tif len(errorViolations) > 0 {\n\t\tfmt.Printf(\"   \ud83d\udea8 Errors (%d):\\n\", len(errorViolations))\n\t\tfor _, violation := range errorViolations {\n\t\t\toutputViolation(violation, \"     \")\n\t\t}\n\t}\n\t\n\tif len(warningViolations) > 0 {\n\t\tfmt.Printf(\"   \u26a0\ufe0f  Warnings (%d):\\n\", len(warningViolations))\n\t\tfor _, violation := range warningViolations {\n\t\t\toutputViolation(violation, \"     \")\n\t\t}\n\t}\n\t\n\tif len(infoViolations) > 0 && auditVerbose {\n\t\tfmt.Printf(\"   \u2139\ufe0f  Info (%d):\\n\", len(infoViolations))\n\t\tfor _, violation := range infoViolations {\n\t\t\toutputViolation(violation, \"     \")\n\t\t}\n\t}\n\n\tfmt.Printf(\"\\n\")\n}\n\nfunc outputComponentSummary(report *accessibility.AccessibilityReport) {\n\tcomponentName := report.ComponentName\n\tif componentName == \"\" {\n\t\tcomponentName = \"Unknown Component\"\n\t}\n\n\terrorCount := 0\n\twarningCount := 0\n\tcriticalCount := 0\n\n\tfor _, violation := range report.Violations {\n\t\tswitch violation.Severity {\n\t\tcase accessibility.SeverityError:\n\t\t\terrorCount++\n\t\tcase accessibility.SeverityWarning:\n\t\t\twarningCount++\n\t\t}\n\t\t\n\t\tif violation.Impact == accessibility.ImpactCritical {\n\t\t\tcriticalCount++\n\t\t}\n\t}\n\n\tscoreColor := getScoreColor(report.Summary.OverallScore)\n\t\n\tfmt.Printf(\"\ud83d\udce6 %s %s(%.1f/100)%s\\n\", componentName, scoreColor, report.Summary.OverallScore, \"\\033[0m\")\n\t\n\tif criticalCount > 0 {\n\t\tfmt.Printf(\"   \ud83d\udea8 %d critical issue(s)\\n\", criticalCount)\n\t}\n\tif errorCount > 0 {\n\t\tfmt.Printf(\"   \u274c %d error(s)\\n\", errorCount)\n\t}\n\tif warningCount > 0 {\n\t\tfmt.Printf(\"   \u26a0\ufe0f  %d warning(s)\\n\", warningCount)\n\t}\n\t\n\tfmt.Printf(\"\\n\")\n}\n\nfunc outputViolation(violation accessibility.AccessibilityViolation, indent string) {\n\tfmt.Printf(\"%s\u2022 %s\\n\", indent, violation.Message)\n\tfmt.Printf(\"%s  Rule: %s | WCAG: %s %s\\n\", \n\t\tindent, violation.Rule, violation.WCAG.Level, violation.WCAG.Criteria)\n\t\n\tif violation.Element != \"\" {\n\t\tfmt.Printf(\"%s  Element: <%s>\\n\", indent, violation.Element)\n\t}\n\t\n\tif auditShowSuggestions && len(violation.Suggestions) > 0 {\n\t\tfmt.Printf(\"%s  \ud83d\udca1 %s\\n\", indent, violation.Suggestions[0].Title)\n\t\tif violation.Suggestions[0].Code != \"\" && auditVerbose {\n\t\t\tfmt.Printf(\"%s     Code: %s\\n\", indent, violation.Suggestions[0].Code)\n\t\t}\n\t}\n\t\n\tif violation.CanAutoFix {\n\t\tfmt.Printf(\"%s  \ud83d\udd27 Auto-fixable\\n\", indent)\n\t}\n\t\n\tfmt.Printf(\"\\n\")\n}\n\nfunc writeOutput(content string) error {\n\tif auditOutputFile != \"\" {\n\t\t// Ensure output directory exists\n\t\tdir := filepath.Dir(auditOutputFile)\n\t\tif err := os.MkdirAll(dir, 0755); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to create output directory: %w\", err)\n\t\t}\n\t\t\n\t\t// Write to file\n\t\tif err := os.WriteFile(auditOutputFile, []byte(content), 0644); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to write output file: %w\", err)\n\t\t}\n\t\t\n\t\tfmt.Printf(\"Report written to: %s\\n\", auditOutputFile)\n\t\treturn nil\n\t}\n\t\n\t// Write to stdout\n\tfmt.Print(content)\n\treturn nil\n}\n\n// Helper functions\nfunc parseWCAGLevel(level string) accessibility.WCAGLevel {\n\tswitch strings.ToUpper(level) {\n\tcase \"A\":\n\t\treturn accessibility.WCAGLevelA\n\tcase \"AA\":\n\t\treturn accessibility.WCAGLevelAA\n\tcase \"AAA\":\n\t\treturn accessibility.WCAGLevelAAA\n\tdefault:\n\t\treturn accessibility.WCAGLevelAA\n\t}\n}\n\nfunc parseSeverity(severity string) accessibility.ViolationSeverity {\n\tswitch strings.ToLower(severity) {\n\tcase \"error\":\n\t\treturn accessibility.SeverityError\n\tcase \"warning\":\n\t\treturn accessibility.SeverityWarning\n\tcase \"info\":\n\t\treturn accessibility.SeverityInfo\n\tdefault:\n\t\treturn accessibility.SeverityWarning\n\t}\n}\n\nfunc getScoreColor(score float64) string {\n\tif score >= 90 {\n\t\treturn \"\\033[32m\" // Green\n\t} else if score >= 70 {\n\t\treturn \"\\033[33m\" // Yellow\n\t} else {\n\t\treturn \"\\033[31m\" // Red\n\t}\n}\n\nfunc aggregateSuggestions(reports []*accessibility.AccessibilityReport) []accessibility.AccessibilitySuggestion {\n\tsuggestionMap := make(map[string]*accessibility.AccessibilitySuggestion)\n\tsuggestionCounts := make(map[string]int)\n\t\n\tfor _, report := range reports {\n\t\tfor _, violation := range report.Violations {\n\t\t\tfor _, suggestion := range violation.Suggestions {\n\t\t\t\tkey := fmt.Sprintf(\"%s_%s\", suggestion.Type, suggestion.Title)\n\t\t\t\tsuggestionCounts[key]++\n\t\t\t\t\n\t\t\t\tif existing, exists := suggestionMap[key]; !exists || suggestion.Priority < existing.Priority {\n\t\t\t\t\tsuggestionCopy := suggestion\n\t\t\t\t\tsuggestionMap[key] = &suggestionCopy\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t\n\t// Convert to slice and sort by frequency and priority\n\tsuggestions := []accessibility.AccessibilitySuggestion{}\n\tfor key, suggestion := range suggestionMap {\n\t\t// Adjust priority based on frequency (more frequent = higher priority)\n\t\tsuggestion.Priority -= suggestionCounts[key] // Lower number = higher priority\n\t\tsuggestions = append(suggestions, *suggestion)\n\t}\n\t\n\t// Sort by priority\n\tsort.Slice(suggestions, func(i, j int) bool {\n\t\treturn suggestions[i].Priority < suggestions[j].Priority\n\t})\n\t\n\treturn suggestions\n}\n\nfunc calculateAccessibilitySummary(violations []accessibility.AccessibilityViolation, passed []accessibility.AccessibilityRule) accessibility.AccessibilitySummary {\n\tsummary := accessibility.AccessibilitySummary{\n\t\tTotalRules:      len(passed) + len(violations),\n\t\tPassedRules:     len(passed),\n\t\tFailedRules:     len(violations),\n\t\tTotalViolations: len(violations),\n\t}\n\t\n\tfor _, violation := range violations {\n\t\tswitch violation.Severity {\n\t\tcase accessibility.SeverityError:\n\t\t\tsummary.ErrorViolations++\n\t\tcase accessibility.SeverityWarning:\n\t\t\tsummary.WarnViolations++\n\t\tcase accessibility.SeverityInfo:\n\t\t\tsummary.InfoViolations++\n\t\t}\n\t\t\n\t\tswitch violation.Impact {\n\t\tcase accessibility.ImpactCritical:\n\t\t\tsummary.CriticalImpact++\n\t\tcase accessibility.ImpactSerious:\n\t\t\tsummary.SeriousImpact++\n\t\tcase accessibility.ImpactModerate:\n\t\t\tsummary.ModerateImpact++\n\t\tcase accessibility.ImpactMinor:\n\t\t\tsummary.MinorImpact++\n\t\t}\n\t}\n\t\n\t// Calculate overall score\n\tif summary.TotalRules > 0 {\n\t\tsummary.OverallScore = float64(summary.PassedRules) / float64(summary.TotalRules) * 100\n\t}\n\t\n\treturn summary\n}\n\nfunc generateHTMLReport(reports []*accessibility.AccessibilityReport) string {\n\t// This would generate a comprehensive HTML report\n\t// For brevity, returning a simplified version\n\thtml := `<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Accessibility Audit Report</title>\n    <style>\n        body { font-family: Arial, sans-serif; margin: 40px; }\n        .header { background: #f5f5f5; padding: 20px; border-radius: 8px; }\n        .component { margin: 20px 0; padding: 20px; border: 1px solid #ddd; border-radius: 8px; }\n        .violation { margin: 10px 0; padding: 10px; background: #fff3cd; border-left: 4px solid #ffc107; }\n        .error { background: #f8d7da; border-left-color: #dc3545; }\n        .success { background: #d4edda; border-left-color: #28a745; }\n    </style>\n</head>\n<body>\n    <div class=\"header\">\n        <h1>Accessibility Audit Report</h1>\n        <p>Generated on: ` + time.Now().Format(\"2006-01-02 15:04:05\") + `</p>\n    </div>`\n\n\tfor _, report := range reports {\n\t\thtml += fmt.Sprintf(`\n    <div class=\"component\">\n        <h2>%s</h2>\n        <p>Score: %.1f/100</p>\n        <p>Violations: %d</p>\n    </div>`, report.ComponentName, report.Summary.OverallScore, len(report.Violations))\n\t}\n\n\thtml += `\n</body>\n</html>`\n\n\treturn html\n}\n\nfunc generateMarkdownReport(reports []*accessibility.AccessibilityReport) string {\n\tmd := fmt.Sprintf(\"# Accessibility Audit Report\\n\\nGenerated on: %s\\n\\n\", time.Now().Format(\"2006-01-02 15:04:05\"))\n\t\n\tfor _, report := range reports {\n\t\tmd += fmt.Sprintf(\"## %s\\n\\n\", report.ComponentName)\n\t\tmd += fmt.Sprintf(\"- **Score**: %.1f/100\\n\", report.Summary.OverallScore)\n\t\tmd += fmt.Sprintf(\"- **Violations**: %d\\n\\n\", len(report.Violations))\n\t\t\n\t\tif len(report.Violations) > 0 {\n\t\t\tmd += \"### Issues Found\\n\\n\"\n\t\t\tfor _, violation := range report.Violations {\n\t\t\t\tmd += fmt.Sprintf(\"- **%s**: %s\\n\", violation.Rule, violation.Message)\n\t\t\t}\n\t\t\tmd += \"\\n\"\n\t\t}\n\t}\n\t\n\treturn md\n}\n\nfunc getComponentCompletions(toComplete string) []string {\n\t// This would integrate with the component registry to provide completions\n\t// For now, returning empty slice\n\treturn []string{}\n}\n\n// showGuidanceOnly displays accessibility guidance without running an audit\nfunc showGuidanceOnly(componentName string) error {\n\tguide := accessibility.NewAccessibilityGuide()\n\t\n\tif componentName != \"\" {\n\t\t// Show component-specific guidance\n\t\tfmt.Printf(\"\ud83c\udfaf Accessibility Guidance for %s Component\\n\", componentName)\n\t\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\t\t\n\t\tguidanceText := guide.GetComponentGuidanceText(componentName)\n\t\tfmt.Print(guidanceText)\n\t\t\n\t\t// Also show general guidance applicable to all components\n\t\tfmt.Printf(\"\\n\ud83d\udccb General Accessibility Guidelines\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tquickStart := guide.GetQuickStartGuide()\n\t\tfor i, item := range quickStart {\n\t\t\tif i >= 3 { // Limit to top 3 for brevity\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfmt.Printf(\"%d. %s\\n\", i+1, item.Title)\n\t\t\tfmt.Printf(\"   %s\\n\\n\", item.Description)\n\t\t}\n\t\t\n\t} else {\n\t\t// Show general accessibility guidance\n\t\tfmt.Printf(\"\ud83c\udf1f Accessibility Quick Start Guide\\n\")\n\t\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\t\t\n\t\tquickStart := guide.GetQuickStartGuide()\n\t\tfor i, item := range quickStart {\n\t\t\tfmt.Printf(\"%d. %s\\n\", i+1, item.Title)\n\t\t\tfmt.Printf(\"   %s\\n\", item.Description)\n\t\t\t\n\t\t\tif len(item.Examples) > 0 {\n\t\t\t\texample := item.Examples[0]\n\t\t\t\tif example.BadCode != \"\" {\n\t\t\t\t\tfmt.Printf(\"   \u274c Avoid: %s\\n\", strings.ReplaceAll(example.BadCode, \"\\n\", \" \"))\n\t\t\t\t}\n\t\t\t\tif example.GoodCode != \"\" {\n\t\t\t\t\tfmt.Printf(\"   \u2705 Use: %s\\n\", strings.ReplaceAll(example.GoodCode, \"\\n\", \" \"))\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Printf(\"\\n\")\n\t\t}\n\t\t\n\t\tfmt.Printf(\"\ud83d\udca1 Advanced Guidelines\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tbestPractices := guide.GetBestPracticesGuide()\n\t\tfor i, item := range bestPractices {\n\t\t\tif i >= 3 { // Limit for readability\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfmt.Printf(\"\u2022 %s\\n\", item.Title)\n\t\t\tfmt.Printf(\"  %s\\n\\n\", item.Description)\n\t\t}\n\t\t\n\t\tfmt.Printf(\"\ud83d\udcda Additional Resources\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\")\n\t\tfmt.Printf(\"\u2022 WCAG Quick Reference: https://www.w3.org/WAI/WCAG21/quickref/\\n\")\n\t\tfmt.Printf(\"\u2022 WebAIM Guidelines: https://webaim.org/\\n\")\n\t\tfmt.Printf(\"\u2022 A11y Project: https://www.a11yproject.com/\\n\")\n\t\tfmt.Printf(\"\u2022 MDN Accessibility: https://developer.mozilla.org/en-US/docs/Web/Accessibility\\n\\n\")\n\t\t\n\t\tfmt.Printf(\"\ud83d\udd27 To audit your components, run:\\n\")\n\t\tfmt.Printf(\"   templar audit              # Audit all components\\n\")\n\t\tfmt.Printf(\"   templar audit Button       # Audit specific component\\n\")\n\t\tfmt.Printf(\"   templar audit --help        # See all options\\n\")\n\t}\n\t\n\treturn nil\n}\n\n// showGuidanceForViolations displays guidance for specific accessibility violations\nfunc showGuidanceForViolations(violations []accessibility.AccessibilityViolation) {\n\tif !auditShowGuidance || len(violations) == 0 {\n\t\treturn\n\t}\n\t\n\tfmt.Printf(\"\\n\ud83c\udf93 Accessibility Guidance\\n\")\n\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\t\n\tguide := accessibility.NewAccessibilityGuide()\n\t\n\t// Group violations by rule to avoid duplicate guidance\n\truleMap := make(map[string]bool)\n\tuniqueRules := []string{}\n\t\n\tfor _, violation := range violations {\n\t\tif !ruleMap[violation.Rule] {\n\t\t\truleMap[violation.Rule] = true\n\t\t\tuniqueRules = append(uniqueRules, violation.Rule)\n\t\t}\n\t}\n\t\n\t// Show guidance for each unique rule\n\tfor i, rule := range uniqueRules {\n\t\tif i > 0 {\n\t\t\tfmt.Printf(\"\\n\" + strings.Repeat(\"\u2500\", 60) + \"\\n\\n\")\n\t\t}\n\t\t\n\t\tguidanceText := guide.GetGuidanceText(rule)\n\t\tfmt.Print(guidanceText)\n\t}\n}",
      "structuredPatch": [
        {
          "oldStart": 122,
          "oldLines": 7,
          "newStart": 122,
          "newLines": 7,
          "lines": [
            "   logger := logging.NewLogger(loggerConfig)",
            " ",
            "   // Load configuration",
            "-  cfg, err := config.LoadConfig()",
            "+  cfg, err := config.Load()",
            "   if err != nil {",
            "     return fmt.Errorf(\"failed to load configuration: %w\", err)",
            "   }"
          ]
        }
      ],
      "userModified": false,
      "replaceAll": false
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Edit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
      "old_string": "// BuildWithPriority queues a component for building with high priority\nfunc (bp *BuildPipeline) BuildWithPriority(component *types.ComponentInfo) {\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  10,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.priority <- task:\n\t\t// Priority task successfully queued\n\tdefault:\n\t\t// Priority queue also full - this is a critical error\n\t\tfmt.Printf(\"Critical: Priority queue full, dropping high-priority task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"priority_queue_full\")\n\n\t\t// Could implement emergency handling here (e.g., block briefly or expand queue)\n\t\t// For now, log the critical error\n\t}\n}",
      "new_string": "// BuildWithPriority queues a component for building with high priority\nfunc (bp *BuildPipeline) BuildWithPriority(component *types.ComponentInfo) {\n\t// Check if pipeline is shut down\n\tif bp.cancel == nil {\n\t\tfmt.Printf(\"Error: Build pipeline not started, dropping priority task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"pipeline_not_started\")\n\t\treturn\n\t}\n\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  10,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.priority <- task:\n\t\t// Priority task successfully queued\n\tdefault:\n\t\t// Priority queue also full - this is a critical error\n\t\tfmt.Printf(\"Critical: Priority queue full, dropping high-priority task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"priority_queue_full\")\n\n\t\t// Could implement emergency handling here (e.g., block briefly or expand queue)\n\t\t// For now, log the critical error\n\t}\n}"
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
      "oldString": "// BuildWithPriority queues a component for building with high priority\nfunc (bp *BuildPipeline) BuildWithPriority(component *types.ComponentInfo) {\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  10,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.priority <- task:\n\t\t// Priority task successfully queued\n\tdefault:\n\t\t// Priority queue also full - this is a critical error\n\t\tfmt.Printf(\"Critical: Priority queue full, dropping high-priority task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"priority_queue_full\")\n\n\t\t// Could implement emergency handling here (e.g., block briefly or expand queue)\n\t\t// For now, log the critical error\n\t}\n}",
      "newString": "// BuildWithPriority queues a component for building with high priority\nfunc (bp *BuildPipeline) BuildWithPriority(component *types.ComponentInfo) {\n\t// Check if pipeline is shut down\n\tif bp.cancel == nil {\n\t\tfmt.Printf(\"Error: Build pipeline not started, dropping priority task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"pipeline_not_started\")\n\t\treturn\n\t}\n\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  10,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.priority <- task:\n\t\t// Priority task successfully queued\n\tdefault:\n\t\t// Priority queue also full - this is a critical error\n\t\tfmt.Printf(\"Critical: Priority queue full, dropping high-priority task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"priority_queue_full\")\n\n\t\t// Could implement emergency handling here (e.g., block briefly or expand queue)\n\t\t// For now, log the critical error\n\t}\n}",
      "originalFile": "// Package build provides a concurrent build pipeline for templ components\n// with caching, error collection, and performance metrics.\n//\n// The build pipeline processes components through worker pools, maintains\n// an LRU cache for build results, and provides real-time build status\n// through callbacks and metrics. It supports parallel execution with\n// configurable worker counts and implements security-hardened command\n// execution with proper validation.\npackage build\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"hash/crc32\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/conneroisu/templar/internal/errors\"\n\t\"github.com/conneroisu/templar/internal/interfaces\"\n\t\"github.com/conneroisu/templar/internal/types\"\n)\n\n// crcTable is a pre-computed CRC32 Castagnoli table for faster hash generation\nvar crcTable = crc32.MakeTable(crc32.Castagnoli)\n\n// BuildPipeline manages the build process for templ components with concurrent\n// execution, intelligent caching, and comprehensive error handling.\n//\n// The pipeline provides:\n// - Concurrent build execution with configurable worker pools\n// - LRU caching with CRC32-based change detection\n// - Priority-based build queue management\n// - Real-time build metrics and status callbacks\n// - Memory optimization through object pooling\n// - Security-hardened command execution\n// - Comprehensive timeout management\ntype BuildPipeline struct {\n\t// compiler handles templ compilation with security validation\n\tcompiler *TemplCompiler\n\t// cache provides LRU-based build result caching\n\tcache *BuildCache\n\t// queue manages build tasks with priority ordering\n\tqueue *BuildQueue\n\t// workers defines the number of concurrent build workers\n\tworkers int\n\t// registry provides component information and change notifications\n\tregistry interfaces.ComponentRegistry\n\t// errorParser processes build errors and provides detailed diagnostics\n\terrorParser *errors.ErrorParser\n\t// metrics tracks build performance and success rates\n\tmetrics *BuildMetrics\n\t// callbacks receive build status updates for UI integration\n\tcallbacks []BuildCallback\n\t// workerWg synchronizes worker goroutine lifecycle\n\tworkerWg sync.WaitGroup\n\t// resultWg synchronizes result processing\n\tresultWg sync.WaitGroup\n\t// cancel terminates all pipeline operations gracefully\n\tcancel context.CancelFunc\n\t// objectPools optimize memory allocation for frequently used objects\n\tobjectPools *ObjectPools\n\t// slicePools reduce slice allocation overhead\n\tslicePools *SlicePools\n\t// workerPool manages the lifecycle of build workers\n\tworkerPool *WorkerPool\n\t// config provides timeout configuration for build operations\n\tconfig *config.Config\n}\n\n// BuildTask represents a build task in the priority queue with metadata\n// for scheduling and execution tracking.\ntype BuildTask struct {\n\t// Component contains the component information to be built\n\tComponent *types.ComponentInfo\n\t// Priority determines build order (higher values built first)\n\tPriority int\n\t// Timestamp records when the task was created for ordering\n\tTimestamp time.Time\n}\n\n// BuildResult represents the result of a build operation\ntype BuildResult struct {\n\tComponent    *types.ComponentInfo\n\tOutput       []byte\n\tError        error\n\tParsedErrors []*errors.ParsedError\n\tDuration     time.Duration\n\tCacheHit     bool\n\tHash         string\n}\n\n// BuildCallback is called when a build completes\ntype BuildCallback func(result BuildResult)\n\n// BuildQueue manages build tasks\ntype BuildQueue struct {\n\ttasks    chan BuildTask\n\tresults  chan BuildResult\n\tpriority chan BuildTask\n}\n\n// NewBuildPipeline creates a new build pipeline with optional timeout configuration\nfunc NewBuildPipeline(workers int, registry interfaces.ComponentRegistry, cfg ...*config.Config) *BuildPipeline {\n\tcompiler := NewTemplCompiler()\n\tcache := NewBuildCache(100*1024*1024, time.Hour) // 100MB, 1 hour TTL\n\n\tqueue := &BuildQueue{\n\t\ttasks:    make(chan BuildTask, 100),\n\t\tresults:  make(chan BuildResult, 100),\n\t\tpriority: make(chan BuildTask, 10),\n\t}\n\n\tmetrics := NewBuildMetrics()\n\n\t// Use first config if provided, otherwise nil\n\tvar config *config.Config\n\tif len(cfg) > 0 {\n\t\tconfig = cfg[0]\n\t}\n\n\treturn &BuildPipeline{\n\t\tcompiler:    compiler,\n\t\tcache:       cache,\n\t\tqueue:       queue,\n\t\tworkers:     workers,\n\t\tregistry:    registry,\n\t\terrorParser: errors.NewErrorParser(),\n\t\tmetrics:     metrics,\n\t\tcallbacks:   make([]BuildCallback, 0),\n\t\t// Initialize object pools for memory optimization\n\t\tobjectPools: NewObjectPools(),\n\t\tslicePools:  NewSlicePools(),\n\t\tworkerPool:  NewWorkerPool(),\n\t\tconfig:      config,\n\t}\n}\n\n// Start starts the build pipeline\nfunc (bp *BuildPipeline) Start(ctx context.Context) {\n\t// Create cancellable context\n\tctx, bp.cancel = context.WithCancel(ctx)\n\n\t// Start workers\n\tfor i := 0; i < bp.workers; i++ {\n\t\tbp.workerWg.Add(1)\n\t\tgo bp.worker(ctx)\n\t}\n\n\t// Start result processor\n\tbp.resultWg.Add(1)\n\tgo bp.processResults(ctx)\n}\n\n// Stop stops the build pipeline and waits for all goroutines to finish\nfunc (bp *BuildPipeline) Stop() {\n\tif bp.cancel != nil {\n\t\tbp.cancel()\n\t}\n\n\t// Wait for all workers to finish\n\tbp.workerWg.Wait()\n\n\t// Wait for result processor to finish\n\tbp.resultWg.Wait()\n}\n\n// Build queues a component for building\nfunc (bp *BuildPipeline) Build(component *types.ComponentInfo) {\n\t// Check if pipeline is shut down\n\tif bp.cancel == nil {\n\t\tfmt.Printf(\"Error: Build pipeline not started, dropping task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"pipeline_not_started\")\n\t\treturn\n\t}\n\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  1,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.tasks <- task:\n\t\t// Task successfully queued\n\tdefault:\n\t\t// Queue full - implement backpressure handling\n\t\t// Log the error and update metrics\n\t\tfmt.Printf(\"Warning: Build queue full, dropping task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"task_queue_full\")\n\n\t\t// Try to handle with retry or priority queue\n\t\tselect {\n\t\tcase bp.queue.priority <- task:\n\t\t\tfmt.Printf(\"Task for %s promoted to priority queue\\n\", component.Name)\n\t\tdefault:\n\t\t\tfmt.Printf(\"Error: Both queues full, build request lost for component %s\\n\", component.Name)\n\t\t\t// TODO: Implement persistent queue or callback for dropped tasks\n\t\t}\n\t}\n}\n\n// BuildWithPriority queues a component for building with high priority\nfunc (bp *BuildPipeline) BuildWithPriority(component *types.ComponentInfo) {\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  10,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.priority <- task:\n\t\t// Priority task successfully queued\n\tdefault:\n\t\t// Priority queue also full - this is a critical error\n\t\tfmt.Printf(\"Critical: Priority queue full, dropping high-priority task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"priority_queue_full\")\n\n\t\t// Could implement emergency handling here (e.g., block briefly or expand queue)\n\t\t// For now, log the critical error\n\t}\n}\n\n// AddCallback adds a callback to be called when builds complete\nfunc (bp *BuildPipeline) AddCallback(callback BuildCallback) {\n\tbp.callbacks = append(bp.callbacks, callback)\n}\n\n// GetMetrics returns the current build metrics\nfunc (bp *BuildPipeline) GetMetrics() BuildMetrics {\n\treturn bp.metrics.GetSnapshot()\n}\n\n// ClearCache clears the build cache\nfunc (bp *BuildPipeline) ClearCache() {\n\tbp.cache.Clear()\n}\n\n// GetCacheStats returns cache statistics\nfunc (bp *BuildPipeline) GetCacheStats() (int, int64, int64) {\n\treturn bp.cache.GetStats()\n}\n\n// getBuildTimeout returns the configured timeout for build operations\nfunc (bp *BuildPipeline) getBuildTimeout() time.Duration {\n\tif bp.config != nil && bp.config.Timeouts.Build > 0 {\n\t\treturn bp.config.Timeouts.Build\n\t}\n\t// Default fallback timeout if no configuration is available\n\treturn 5 * time.Minute\n}\n\n// worker processes build tasks\nfunc (bp *BuildPipeline) worker(ctx context.Context) {\n\tdefer bp.workerWg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase task := <-bp.queue.priority:\n\t\t\tbp.processBuildTask(ctx, task)\n\t\tcase task := <-bp.queue.tasks:\n\t\t\tbp.processBuildTask(ctx, task)\n\t\t}\n\t}\n}\n\nfunc (bp *BuildPipeline) processBuildTask(ctx context.Context, task BuildTask) {\n\tstart := time.Now()\n\n\t// Generate content hash for caching\n\tcontentHash := bp.generateContentHash(task.Component)\n\n\t// Check if context is cancelled before starting work\n\tselect {\n\tcase <-ctx.Done():\n\t\t// Context cancelled, return error\n\t\tbuildResult := bp.objectPools.GetBuildResult()\n\t\tbuildResult.Component = task.Component\n\t\tbuildResult.Output = nil\n\t\tbuildResult.Error = ctx.Err()\n\t\tbuildResult.ParsedErrors = nil\n\t\tbuildResult.Duration = time.Since(start)\n\t\tbuildResult.CacheHit = false\n\t\tbuildResult.Hash = contentHash\n\n\t\t// Non-blocking send to results channel\n\t\tselect {\n\t\tcase bp.queue.results <- *buildResult:\n\t\tdefault:\n\t\t\tbp.metrics.RecordDroppedResult(task.Component.Name, \"results_queue_full_cancelled\")\n\t\t}\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\tdefault:\n\t}\n\n\t// Check cache first\n\tif result, found := bp.cache.Get(contentHash); found {\n\t\t// Use object pool for cache hit result\n\t\tbuildResult := bp.objectPools.GetBuildResult()\n\t\tbuildResult.Component = task.Component\n\t\tbuildResult.Output = result\n\t\tbuildResult.Error = nil\n\t\tbuildResult.ParsedErrors = nil\n\t\tbuildResult.Duration = time.Since(start)\n\t\tbuildResult.CacheHit = true\n\t\tbuildResult.Hash = contentHash\n\n\t\t// Non-blocking send to results channel to prevent worker hangs\n\t\tselect {\n\t\tcase bp.queue.results <- *buildResult:\n\t\t\t// Cache hit result successfully queued\n\t\tcase <-ctx.Done():\n\t\t\t// Context cancelled while sending result\n\t\t\tbuildResult.Error = ctx.Err()\n\t\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\t\treturn\n\t\tdefault:\n\t\t\t// Results queue full - this could cause result loss\n\t\t\tfmt.Printf(\"Warning: Results queue full, dropping cache hit result for component %s\\n\", buildResult.Component.Name)\n\t\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full_cache_hit\")\n\t\t}\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\t}\n\n\t// Create timeout context for build operation based on configuration\n\tbuildTimeout := bp.getBuildTimeout()\n\tbuildCtx, cancel := context.WithTimeout(ctx, buildTimeout)\n\tdefer cancel()\n\n\t// Execute build with pooled output buffer and context-based timeout\n\toutput, err := bp.compiler.CompileWithPools(buildCtx, task.Component, bp.objectPools)\n\n\t// Parse errors if build failed\n\tvar parsedErrors []*errors.ParsedError\n\tif err != nil {\n\t\t// Wrap the error with build context for better debugging\n\t\terr = errors.WrapBuild(err, errors.ErrCodeBuildFailed, \n\t\t\t\"component compilation failed\", task.Component.Name).\n\t\t\tWithLocation(task.Component.FilePath, 0, 0)\n\t\tparsedErrors = bp.errorParser.ParseError(string(output))\n\t}\n\n\t// Use object pool for build result\n\tbuildResult := bp.objectPools.GetBuildResult()\n\tbuildResult.Component = task.Component\n\tbuildResult.Output = output\n\tbuildResult.Error = err\n\tbuildResult.ParsedErrors = parsedErrors\n\tbuildResult.Duration = time.Since(start)\n\tbuildResult.CacheHit = false\n\tbuildResult.Hash = contentHash\n\n\t// Cache successful builds\n\tif err == nil {\n\t\tbp.cache.Set(contentHash, output)\n\t}\n\n\t// Non-blocking send to results channel to prevent worker hangs\n\tselect {\n\tcase bp.queue.results <- *buildResult:\n\t\t// Result successfully queued\n\tdefault:\n\t\t// Results queue full - this could cause result loss\n\t\tfmt.Printf(\"Warning: Results queue full, dropping result for component %s\\n\", buildResult.Component.Name)\n\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full\")\n\t}\n\tbp.objectPools.PutBuildResult(buildResult)\n}\n\nfunc (bp *BuildPipeline) processResults(ctx context.Context) {\n\tdefer bp.resultWg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase result := <-bp.queue.results:\n\t\t\tbp.handleBuildResult(result)\n\t\t}\n\t}\n}\n\nfunc (bp *BuildPipeline) handleBuildResult(result BuildResult) {\n\t// Update metrics\n\tbp.metrics.RecordBuild(result)\n\n\t// Print result\n\tif result.Error != nil {\n\t\tfmt.Printf(\"Build failed for %s: %v\\n\", result.Component.Name, result.Error)\n\t\tif len(result.ParsedErrors) > 0 {\n\t\t\tfmt.Println(\"Parsed errors:\")\n\t\t\tfor _, err := range result.ParsedErrors {\n\t\t\t\tfmt.Print(err.FormatError())\n\t\t\t}\n\t\t}\n\t} else {\n\t\tstatus := \"succeeded\"\n\t\tif result.CacheHit {\n\t\t\tstatus = \"cached\"\n\t\t}\n\t\tfmt.Printf(\"Build %s for %s in %v\\n\", status, result.Component.Name, result.Duration)\n\t}\n\n\t// Call callbacks\n\tfor _, callback := range bp.callbacks {\n\t\tcallback(result)\n\t}\n}\n\n// generateContentHash generates a hash for component content with optimized single I/O operation\nfunc (bp *BuildPipeline) generateContentHash(component *types.ComponentInfo) string {\n\t// OPTIMIZATION: Use Stat() first to get metadata without opening file\n\t// This reduces file I/O operations by 70-90% for cached files\n\tstat, err := os.Stat(component.FilePath)\n\tif err != nil {\n\t\t// File not accessible, return fallback hash\n\t\t// Note: We don't need to wrap this error as it's an internal optimization\n\t\treturn component.FilePath\n\t}\n\n\t// Create metadata-based hash key for cache lookup\n\tmetadataKey := fmt.Sprintf(\"%s:%d:%d\", component.FilePath, stat.ModTime().Unix(), stat.Size())\n\n\t// Two-tier cache system: Check metadata cache first (no file I/O)\n\tif hash, found := bp.cache.GetHash(metadataKey); found {\n\t\t// Cache hit - no file I/O needed, just return cached hash\n\t\treturn hash\n\t}\n\n\t// Cache miss: Now we need to read file content and generate hash\n\t// Only open file when we actually need to read content\n\tfile, err := os.Open(component.FilePath)\n\tif err != nil {\n\t\treturn component.FilePath\n\t}\n\tdefer file.Close()\n\n\t// Use mmap for large files (>64KB) for better performance\n\tvar content []byte\n\tif stat.Size() > 64*1024 {\n\t\t// Use mmap for large files\n\t\tcontent, err = bp.readFileWithMmap(file, stat.Size())\n\t\tif err != nil {\n\t\t\t// Fallback to regular read\n\t\t\tcontent, err = io.ReadAll(file)\n\t\t}\n\t} else {\n\t\t// Regular read for small files\n\t\tcontent, err = io.ReadAll(file)\n\t}\n\n\tif err != nil {\n\t\t// Fallback to metadata-based hash\n\t\treturn fmt.Sprintf(\"%s:%d\", component.FilePath, stat.ModTime().Unix())\n\t}\n\n\t// Generate content hash using CRC32 Castagnoli for faster file change detection\n\tcrcHash := crc32.Checksum(content, crcTable)\n\tcontentHash := strconv.FormatUint(uint64(crcHash), 16)\n\n\t// Cache the hash with metadata key for future lookups\n\tbp.cache.SetHash(metadataKey, contentHash)\n\n\treturn contentHash\n}\n\n// readFileWithMmap reads file content using memory mapping for better performance on large files\nfunc (bp *BuildPipeline) readFileWithMmap(file *os.File, size int64) ([]byte, error) {\n\t// Memory map the file for efficient reading\n\tmmap, err := syscall.Mmap(int(file.Fd()), 0, int(size), syscall.PROT_READ, syscall.MAP_SHARED)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Copy the mapped data to avoid keeping the mapping open\n\tcontent := make([]byte, size)\n\tcopy(content, mmap)\n\n\t// Unmap the memory\n\tif err := syscall.Munmap(mmap); err != nil {\n\t\t// Log warning but don't fail - we have the content\n\t\t// Could add logging here if logger is available\n\t}\n\n\treturn content, nil\n}\n\n// generateContentHashesBatch processes multiple components in a single batch for better I/O efficiency\nfunc (bp *BuildPipeline) generateContentHashesBatch(components []*types.ComponentInfo) map[string]string {\n\tresults := make(map[string]string, len(components))\n\n\t// Group components by whether they need content reading (cache misses)\n\tvar needsReading []*types.ComponentInfo\n\n\t// First pass: check metadata-based cache for all components (no file I/O)\n\tfor _, component := range components {\n\t\t// OPTIMIZATION: Use efficient Stat() + metadata cache check first\n\t\tif stat, err := os.Stat(component.FilePath); err == nil {\n\t\t\tmetadataKey := fmt.Sprintf(\"%s:%d:%d\", component.FilePath, stat.ModTime().Unix(), stat.Size())\n\n\t\t\t// Check cache with metadata key\n\t\t\tif hash, found := bp.cache.GetHash(metadataKey); found {\n\t\t\t\t// Cache hit - no file reading needed\n\t\t\t\tresults[component.FilePath] = hash\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// Cache miss - needs content reading\n\t\tneedsReading = append(needsReading, component)\n\t}\n\n\t// Second pass: batch process cache misses with optimized I/O\n\tif len(needsReading) > 0 {\n\t\thashResults := bp.batchReadAndHash(needsReading)\n\t\tfor filePath, hash := range hashResults {\n\t\t\tresults[filePath] = hash\n\t\t}\n\t}\n\n\treturn results\n}\n\n// batchReadAndHash reads and hashes multiple files efficiently\nfunc (bp *BuildPipeline) batchReadAndHash(components []*types.ComponentInfo) map[string]string {\n\tresults := make(map[string]string, len(components))\n\n\t// Process each component with optimized I/O\n\tfor _, component := range components {\n\t\thash := bp.generateContentHash(component)\n\t\tresults[component.FilePath] = hash\n\t}\n\n\treturn results\n}\n\n// FileDiscoveryResult represents the result of discovering files in a directory\ntype FileDiscoveryResult struct {\n\tFiles       []*types.ComponentInfo\n\tErrors      []error\n\tDuration    time.Duration\n\tDiscovered  int64\n\tSkipped     int64\n}\n\n// FileDiscoveryStats tracks file discovery performance metrics\ntype FileDiscoveryStats struct {\n\tTotalFiles     int64\n\tProcessedFiles int64\n\tSkippedFiles   int64\n\tErrors         int64\n\tDuration       time.Duration\n\tWorkerCount    int\n}\n\n// ParallelFileProcessor provides parallel file processing capabilities\ntype ParallelFileProcessor struct {\n\tworkerCount int\n\tmaxDepth    int\n\tfilters     []string\n\tstats       *FileDiscoveryStats\n}\n\n// NewParallelFileProcessor creates a new parallel file processor\nfunc NewParallelFileProcessor(workerCount int) *ParallelFileProcessor {\n\treturn &ParallelFileProcessor{\n\t\tworkerCount: workerCount,\n\t\tmaxDepth:    10, // Default max depth\n\t\tfilters:     []string{\".templ\"},\n\t\tstats:       &FileDiscoveryStats{},\n\t}\n}\n\n// DiscoverFiles discovers component files in parallel using filepath.WalkDir\nfunc (pfp *ParallelFileProcessor) DiscoverFiles(ctx context.Context, rootPaths []string) (*FileDiscoveryResult, error) {\n\tstart := time.Now()\n\tdefer func() {\n\t\tpfp.stats.Duration = time.Since(start)\n\t}()\n\n\t// Create channels for work distribution\n\tpathCh := make(chan string, len(rootPaths))\n\tresultCh := make(chan *types.ComponentInfo, 100)\n\terrorCh := make(chan error, 100)\n\n\t// Start workers\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < pfp.workerCount; i++ {\n\t\twg.Add(1)\n\t\tgo pfp.worker(ctx, pathCh, resultCh, errorCh, &wg)\n\t}\n\n\t// Send root paths to workers\n\tgo func() {\n\t\tdefer close(pathCh)\n\t\tfor _, path := range rootPaths {\n\t\t\tselect {\n\t\t\tcase pathCh <- path:\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Collect results\n\tvar files []*types.ComponentInfo\n\tvar errors []error\n\tvar discovered, skipped int64\n\n\t// Result collection goroutine\n\tdone := make(chan struct{})\n\tgo func() {\n\t\tdefer close(done)\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase component, ok := <-resultCh:\n\t\t\t\tif !ok {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tfiles = append(files, component)\n\t\t\t\tatomic.AddInt64(&discovered, 1)\n\t\t\tcase err, ok := <-errorCh:\n\t\t\t\tif !ok {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\terrors = append(errors, err)\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Wait for workers to complete\n\twg.Wait()\n\tclose(resultCh)\n\tclose(errorCh)\n\n\t// Wait for result collection to complete\n\t<-done\n\n\t// Update stats\n\tatomic.StoreInt64(&pfp.stats.ProcessedFiles, discovered)\n\tatomic.StoreInt64(&pfp.stats.SkippedFiles, skipped)\n\tatomic.StoreInt64(&pfp.stats.Errors, int64(len(errors)))\n\tpfp.stats.WorkerCount = pfp.workerCount\n\n\treturn &FileDiscoveryResult{\n\t\tFiles:      files,\n\t\tErrors:     errors,\n\t\tDuration:   time.Since(start),\n\t\tDiscovered: discovered,\n\t\tSkipped:    skipped,\n\t}, nil\n}\n\n// worker processes file discovery work\nfunc (pfp *ParallelFileProcessor) worker(ctx context.Context, pathCh <-chan string, resultCh chan<- *types.ComponentInfo, errorCh chan<- error, wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase rootPath, ok := <-pathCh:\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Walk directory tree using filepath.WalkDir for better performance\n\t\t\terr := filepath.WalkDir(rootPath, func(path string, d os.DirEntry, err error) error {\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\t// Skip directories\n\t\t\t\tif d.IsDir() {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\n\t\t\t\t// Check if file matches our filters\n\t\t\t\tif !pfp.matchesFilter(path) {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\n\t\t\t\t// Create component info\n\t\t\t\tcomponent := &types.ComponentInfo{\n\t\t\t\t\tName:       pfp.extractComponentName(path),\n\t\t\t\t\tFilePath:   path,\n\t\t\t\t\tPackage:    pfp.extractPackage(path),\n\t\t\t\t\tParameters: []types.ParameterInfo{},\n\t\t\t\t}\n\n\t\t\t\t// Send result non-blocking\n\t\t\t\tselect {\n\t\t\t\tcase resultCh <- component:\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn ctx.Err()\n\t\t\t\tdefault:\n\t\t\t\t\t// Channel full, skip this component\n\t\t\t\t\tatomic.AddInt64(&pfp.stats.SkippedFiles, 1)\n\t\t\t\t}\n\n\t\t\t\treturn nil\n\t\t\t})\n\n\t\t\tif err != nil {\n\t\t\t\tselect {\n\t\t\t\tcase errorCh <- err:\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\tdefault:\n\t\t\t\t\t// Error channel full, skip error\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// matchesFilter checks if a file path matches the processor's filters\nfunc (pfp *ParallelFileProcessor) matchesFilter(path string) bool {\n\tfor _, filter := range pfp.filters {\n\t\tif strings.HasSuffix(path, filter) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// extractComponentName extracts component name from file path\nfunc (pfp *ParallelFileProcessor) extractComponentName(path string) string {\n\tbase := filepath.Base(path)\n\text := filepath.Ext(base)\n\treturn strings.TrimSuffix(base, ext)\n}\n\n// extractPackage extracts package name from file path\nfunc (pfp *ParallelFileProcessor) extractPackage(path string) string {\n\tdir := filepath.Dir(path)\n\treturn filepath.Base(dir)\n}\n\n// ProcessFilesBatch processes multiple files in parallel batches\nfunc (bp *BuildPipeline) ProcessFilesBatch(ctx context.Context, components []*types.ComponentInfo, batchSize int) (*FileDiscoveryResult, error) {\n\tstart := time.Now()\n\tvar totalDiscovered, totalSkipped int64\n\tvar allErrors []error\n\tvar allResults []*types.ComponentInfo\n\n\t// Process components in batches\n\tfor i := 0; i < len(components); i += batchSize {\n\t\tend := i + batchSize\n\t\tif end > len(components) {\n\t\t\tend = len(components)\n\t\t}\n\n\t\tbatch := components[i:end]\n\t\thashes := bp.generateContentHashesBatch(batch)\n\n\t\t// Process batch with caching\n\t\tfor _, component := range batch {\n\t\t\thash, exists := hashes[component.FilePath]\n\t\t\tif !exists {\n\t\t\t\tatomic.AddInt64(&totalSkipped, 1)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check cache first\n\t\t\tif _, found := bp.cache.Get(hash); found {\n\t\t\t\t// Cache hit, no processing needed\n\t\t\t\tallResults = append(allResults, component)\n\t\t\t\tatomic.AddInt64(&totalDiscovered, 1)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Process component\n\t\t\tallResults = append(allResults, component)\n\t\t\tatomic.AddInt64(&totalDiscovered, 1)\n\t\t}\n\t}\n\n\treturn &FileDiscoveryResult{\n\t\tFiles:      allResults,\n\t\tErrors:     allErrors,\n\t\tDuration:   time.Since(start),\n\t\tDiscovered: totalDiscovered,\n\t\tSkipped:    totalSkipped,\n\t}, nil\n}\n\n// BuildDirectory builds all components in a directory using parallel processing\nfunc (bp *BuildPipeline) BuildDirectory(ctx context.Context, rootPath string) error {\n\t// Create parallel file processor\n\tprocessor := NewParallelFileProcessor(bp.workers)\n\n\t// Discover files\n\tresult, err := processor.DiscoverFiles(ctx, []string{rootPath})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to discover files: %w\", err)\n\t}\n\n\t// Queue all discovered components for building\n\tfor _, component := range result.Files {\n\t\tbp.Build(component)\n\t}\n\n\tfmt.Printf(\"Directory build queued: %d components discovered in %v\\n\", \n\t\tresult.Discovered, result.Duration)\n\n\treturn nil\n}\n\n// GetFileDiscoveryStats returns file discovery performance statistics\nfunc (pfp *ParallelFileProcessor) GetFileDiscoveryStats() FileDiscoveryStats {\n\treturn FileDiscoveryStats{\n\t\tTotalFiles:     atomic.LoadInt64(&pfp.stats.TotalFiles),\n\t\tProcessedFiles: atomic.LoadInt64(&pfp.stats.ProcessedFiles),\n\t\tSkippedFiles:   atomic.LoadInt64(&pfp.stats.SkippedFiles),\n\t\tErrors:         atomic.LoadInt64(&pfp.stats.Errors),\n\t\tDuration:       pfp.stats.Duration,\n\t\tWorkerCount:    pfp.stats.WorkerCount,\n\t}\n}\n",
      "structuredPatch": [
        {
          "oldStart": 209,
          "oldLines": 6,
          "newStart": 209,
          "newLines": 13,
          "lines": [
            " ",
            " // BuildWithPriority queues a component for building with high priority",
            " func (bp *BuildPipeline) BuildWithPriority(component *types.ComponentInfo) {",
            "+  // Check if pipeline is shut down",
            "+  if bp.cancel == nil {",
            "+    fmt.Printf(\"Error: Build pipeline not started, dropping priority task for component %s\\n\", component.Name)",
            "+    bp.metrics.RecordDroppedTask(component.Name, \"pipeline_not_started\")",
            "+    return",
            "+  }",
            "+",
            "   task := BuildTask{",
            "     Component: component,",
            "     Priority:  10,"
          ]
        }
      ],
      "userModified": false,
      "replaceAll": false
    }
  },
  {
    "session_id": "42e03a9d-fb13-4416-96b7-b2ff5d56bda2",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/42e03a9d-fb13-4416-96b7-b2ff5d56bda2.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Write",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/server/websocket_memory_fix.go",
      "content": "package server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"net/http\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/coder/websocket\"\n\t\"github.com/conneroisu/templar/internal/config\"\n)\n\n// MemoryLeakPreventionManager provides comprehensive memory leak prevention for WebSocket connections\n// \n// This manager addresses common WebSocket memory leak sources:\n// 1. Goroutine leaks from improperly terminated read/write pumps\n// 2. Channel leaks from unclosed send channels\n// 3. Connection leaks from abandoned WebSocket connections\n// 4. Client map unbounded growth from stale connections\n// 5. Race conditions in concurrent client management\n//\n// Key Features:\n// - Automatic stale connection cleanup with configurable intervals\n// - Goroutine lifecycle tracking and termination guarantees\n// - Channel resource management with proper cleanup\n// - Connection timeout enforcement with activity monitoring\n// - Resource limit enforcement to prevent unbounded growth\n// - Comprehensive metrics for memory leak detection\ntype MemoryLeakPreventionManager struct {\n\t// Resource tracking\n\tactiveConnections int64                    // Atomic counter for active connections\n\ttotalConnections  int64                    // Atomic counter for total connections created\n\tcleanupCount      int64                    // Atomic counter for cleanup operations\n\t\n\t// Connection lifecycle management\n\tconnectionRegistry map[string]*ConnectionInfo  // Registry of all connections with metadata\n\tregistryMutex     sync.RWMutex                 // Protects connection registry\n\t\n\t// Resource limits and configuration\n\tmaxConnections    int                          // Maximum allowed concurrent connections\n\tcleanupInterval   time.Duration               // Interval for periodic cleanup operations\n\tconnectionTimeout time.Duration               // Timeout for idle connections\n\t\n\t// Cleanup coordination\n\tcleanupTimer      *time.Timer                 // Timer for periodic cleanup\n\tstopCleanup       chan struct{}               // Channel to stop cleanup goroutine\n\tcleanupWg         sync.WaitGroup              // Ensures cleanup goroutine termination\n\t\n\t// Metrics and monitoring\n\tmetrics           *WebSocketMemoryMetrics     // Detailed memory and resource metrics\n\t\n\t// Configuration\n\tconfig            *config.Config              // Configuration for timeout values\n\t\n\t// Lifecycle management\n\tctx               context.Context             // Context for coordinated cancellation\n\tcancel            context.CancelFunc          // Cancellation function\n\tshutdownOnce      sync.Once                   // Ensures shutdown happens exactly once\n\tisShutdown        bool                        // Shutdown state indicator\n}\n\n// ConnectionInfo tracks metadata for each WebSocket connection to prevent leaks\ntype ConnectionInfo struct {\n\tID            string              // Unique connection identifier\n\tRemoteAddr    string              // Client remote address\n\tCreatedAt     time.Time           // Connection creation timestamp\n\tLastActivity  time.Time           // Last activity timestamp for timeout detection\n\tConn          *websocket.Conn     // WebSocket connection reference\n\tSend          chan []byte         // Send channel reference for cleanup\n\tContext       context.Context     // Connection-specific context\n\tCancel        context.CancelFunc  // Connection cancellation function\n\tGoroutineIDs  []string           // Track associated goroutines for leak detection\n\tIsActive      bool               // Active state flag\n\tmutex         sync.RWMutex       // Protects connection info fields\n}\n\n// WebSocketMemoryMetrics provides comprehensive metrics for memory leak detection\ntype WebSocketMemoryMetrics struct {\n\t// Connection metrics\n\tActiveConnections    int64    // Current active connections\n\tTotalConnections     int64    // Total connections created\n\tCleanedConnections   int64    // Connections cleaned up\n\tFailedConnections    int64    // Failed connection attempts\n\tTimeoutConnections   int64    // Connections closed due to timeout\n\t\n\t// Resource metrics\n\tActiveGoroutines     int64    // Active goroutines count\n\tActiveChannels       int64    // Active send channels count\n\tMemoryUsageBytes     int64    // Estimated memory usage\n\t\n\t// Performance metrics\n\tAverageConnectionAge time.Duration  // Average connection lifetime\n\tCleanupOperations    int64          // Total cleanup operations performed\n\tLastCleanupTime      time.Time      // Last cleanup operation timestamp\n\t\n\t// Error metrics\n\tLeakPrevented        int64          // Number of potential leaks prevented\n\tForceCleanups        int64          // Number of forced cleanups performed\n\t\n\tmutex                sync.RWMutex   // Protects metrics updates\n}\n\n// NewMemoryLeakPreventionManager creates a comprehensive memory leak prevention system\nfunc NewMemoryLeakPreventionManager(cfg ...*config.Config) *MemoryLeakPreventionManager {\n\t// Create cancellable context\n\tctx, cancel := context.WithCancel(context.Background())\n\t\n\t// Use first config if provided, otherwise use defaults\n\tvar config *config.Config\n\tif len(cfg) > 0 {\n\t\tconfig = cfg[0]\n\t}\n\t\n\tmanager := &MemoryLeakPreventionManager{\n\t\tconnectionRegistry: make(map[string]*ConnectionInfo),\n\t\tmaxConnections:     getMaxConnections(config),\n\t\tcleanupInterval:    getCleanupInterval(config),\n\t\tconnectionTimeout:  getConnectionTimeout(config),\n\t\tstopCleanup:        make(chan struct{}),\n\t\tmetrics:           &WebSocketMemoryMetrics{},\n\t\tconfig:            config,\n\t\tctx:               ctx,\n\t\tcancel:            cancel,\n\t\tisShutdown:        false,\n\t}\n\t\n\t// Start periodic cleanup goroutine\n\tmanager.startCleanupRoutine()\n\t\n\treturn manager\n}\n\n// RegisterConnection registers a new WebSocket connection for memory leak prevention\nfunc (m *MemoryLeakPreventionManager) RegisterConnection(\n\tconn *websocket.Conn,\n\tremoteAddr string,\n) (*ConnectionInfo, error) {\n\t// Check if shutdown\n\tif m.isShutdown {\n\t\treturn nil, fmt.Errorf(\"manager is shutting down\")\n\t}\n\t\n\t// Check connection limit\n\tcurrentCount := atomic.LoadInt64(&m.activeConnections)\n\tif currentCount >= int64(m.maxConnections) {\n\t\tatomic.AddInt64(&m.metrics.FailedConnections, 1)\n\t\treturn nil, fmt.Errorf(\"connection limit exceeded: %d/%d\", currentCount, m.maxConnections)\n\t}\n\t\n\t// Create connection-specific context\n\tconnCtx, connCancel := context.WithCancel(m.ctx)\n\t\n\t// Generate unique connection ID\n\tconnID := m.generateConnectionID(remoteAddr)\n\t\n\t// Create connection info with comprehensive tracking\n\tconnInfo := &ConnectionInfo{\n\t\tID:           connID,\n\t\tRemoteAddr:   remoteAddr,\n\t\tCreatedAt:    time.Now(),\n\t\tLastActivity: time.Now(),\n\t\tConn:         conn,\n\t\tSend:         make(chan []byte, 256), // Buffered channel\n\t\tContext:      connCtx,\n\t\tCancel:       connCancel,\n\t\tGoroutineIDs: make([]string, 0, 2),  // Expect read and write goroutines\n\t\tIsActive:     true,\n\t}\n\t\n\t// Register connection in thread-safe manner\n\tm.registryMutex.Lock()\n\tm.connectionRegistry[connID] = connInfo\n\tm.registryMutex.Unlock()\n\t\n\t// Update metrics atomically\n\tatomic.AddInt64(&m.activeConnections, 1)\n\tatomic.AddInt64(&m.totalConnections, 1)\n\tm.updateConnectionMetrics()\n\t\n\tlog.Printf(\"WebSocket connection registered: %s (total: %d)\", connID, atomic.LoadInt64(&m.activeConnections))\n\t\n\treturn connInfo, nil\n}\n\n// UnregisterConnection safely removes a connection and prevents memory leaks\nfunc (m *MemoryLeakPreventionManager) UnregisterConnection(connID string) error {\n\tm.registryMutex.Lock()\n\tconnInfo, exists := m.connectionRegistry[connID]\n\tif exists {\n\t\tdelete(m.connectionRegistry, connID)\n\t}\n\tm.registryMutex.Unlock()\n\t\n\tif !exists {\n\t\tlog.Printf(\"Warning: Attempted to unregister unknown connection: %s\", connID)\n\t\treturn fmt.Errorf(\"connection not found: %s\", connID)\n\t}\n\t\n\t// Perform comprehensive cleanup\n\tm.cleanupConnection(connInfo)\n\t\n\t// Update metrics\n\tatomic.AddInt64(&m.activeConnections, -1)\n\tatomic.AddInt64(&m.metrics.CleanedConnections, 1)\n\tm.updateConnectionMetrics()\n\t\n\tlog.Printf(\"WebSocket connection unregistered: %s (remaining: %d)\", \n\t\tconnID, atomic.LoadInt64(&m.activeConnections))\n\t\n\treturn nil\n}\n\n// UpdateConnectionActivity updates the last activity timestamp for a connection\nfunc (m *MemoryLeakPreventionManager) UpdateConnectionActivity(connID string) {\n\tm.registryMutex.RLock()\n\tconnInfo, exists := m.connectionRegistry[connID]\n\tm.registryMutex.RUnlock()\n\t\n\tif exists {\n\t\tconnInfo.mutex.Lock()\n\t\tconnInfo.LastActivity = time.Now()\n\t\tconnInfo.mutex.Unlock()\n\t}\n}\n\n// startCleanupRoutine starts the periodic cleanup goroutine for stale connections\nfunc (m *MemoryLeakPreventionManager) startCleanupRoutine() {\n\tm.cleanupWg.Add(1)\n\tgo func() {\n\t\tdefer m.cleanupWg.Done()\n\t\t\n\t\tticker := time.NewTicker(m.cleanupInterval)\n\t\tdefer ticker.Stop()\n\t\t\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ticker.C:\n\t\t\t\tm.performCleanup()\n\t\t\t\t\n\t\t\tcase <-m.stopCleanup:\n\t\t\t\treturn\n\t\t\t\t\n\t\t\tcase <-m.ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n}\n\n// performCleanup identifies and removes stale connections to prevent memory leaks\nfunc (m *MemoryLeakPreventionManager) performCleanup() {\n\tnow := time.Now()\n\tvar staleConnections []string\n\t\n\t// Identify stale connections\n\tm.registryMutex.RLock()\n\tfor connID, connInfo := range m.connectionRegistry {\n\t\tconnInfo.mutex.RLock()\n\t\tisStale := now.Sub(connInfo.LastActivity) > m.connectionTimeout\n\t\tisInactive := !connInfo.IsActive\n\t\tconnInfo.mutex.RUnlock()\n\t\t\n\t\tif isStale || isInactive {\n\t\t\tstaleConnections = append(staleConnections, connID)\n\t\t}\n\t}\n\tm.registryMutex.RUnlock()\n\t\n\t// Clean up stale connections\n\tcleanedCount := 0\n\tfor _, connID := range staleConnections {\n\t\tif err := m.UnregisterConnection(connID); err == nil {\n\t\t\tcleanedCount++\n\t\t\tatomic.AddInt64(&m.metrics.TimeoutConnections, 1)\n\t\t}\n\t}\n\t\n\tif cleanedCount > 0 {\n\t\tlog.Printf(\"Cleanup completed: removed %d stale connections\", cleanedCount)\n\t\tatomic.AddInt64(&m.metrics.CleanupOperations, 1)\n\t}\n\t\n\t// Update cleanup metrics\n\tm.metrics.mutex.Lock()\n\tm.metrics.LastCleanupTime = now\n\tm.metrics.mutex.Unlock()\n\t\n\tatomic.AddInt64(&m.cleanupCount, 1)\n}\n\n// cleanupConnection performs comprehensive cleanup for a single connection\nfunc (m *MemoryLeakPreventionManager) cleanupConnection(connInfo *ConnectionInfo) {\n\tif connInfo == nil {\n\t\treturn\n\t}\n\t\n\t// Mark as inactive to prevent further use\n\tconnInfo.mutex.Lock()\n\tconnInfo.IsActive = false\n\tconnInfo.mutex.Unlock()\n\t\n\t// Cancel connection context to terminate associated goroutines\n\tif connInfo.Cancel != nil {\n\t\tconnInfo.Cancel()\n\t}\n\t\n\t// Close send channel safely\n\tif connInfo.Send != nil {\n\t\tselect {\n\t\tcase <-connInfo.Send: // Drain any pending messages\n\t\tdefault:\n\t\t}\n\t\tclose(connInfo.Send)\n\t\tatomic.AddInt64(&m.metrics.ActiveChannels, -1)\n\t}\n\t\n\t// Close WebSocket connection\n\tif connInfo.Conn != nil {\n\t\tconnInfo.Conn.Close(websocket.StatusNormalClosure, \"Cleanup\")\n\t}\n\t\n\tlog.Printf(\"Connection cleanup completed for: %s\", connInfo.ID)\n}\n\n// ForceCleanupStaleConnections immediately cleans up all stale connections\nfunc (m *MemoryLeakPreventionManager) ForceCleanupStaleConnections() int {\n\tm.performCleanup()\n\tatomic.AddInt64(&m.metrics.ForceCleanups, 1)\n\treturn int(atomic.LoadInt64(&m.metrics.TimeoutConnections))\n}\n\n// GetMemoryMetrics returns current memory and resource metrics\nfunc (m *MemoryLeakPreventionManager) GetMemoryMetrics() WebSocketMemoryMetrics {\n\tm.updateConnectionMetrics()\n\t\n\tm.metrics.mutex.RLock()\n\tdefer m.metrics.mutex.RUnlock()\n\t\n\treturn WebSocketMemoryMetrics{\n\t\tActiveConnections:    atomic.LoadInt64(&m.activeConnections),\n\t\tTotalConnections:     atomic.LoadInt64(&m.totalConnections),\n\t\tCleanedConnections:   atomic.LoadInt64(&m.metrics.CleanedConnections),\n\t\tFailedConnections:    atomic.LoadInt64(&m.metrics.FailedConnections),\n\t\tTimeoutConnections:   atomic.LoadInt64(&m.metrics.TimeoutConnections),\n\t\tActiveGoroutines:     atomic.LoadInt64(&m.metrics.ActiveGoroutines),\n\t\tActiveChannels:       atomic.LoadInt64(&m.metrics.ActiveChannels),\n\t\tMemoryUsageBytes:     atomic.LoadInt64(&m.metrics.MemoryUsageBytes),\n\t\tAverageConnectionAge: m.metrics.AverageConnectionAge,\n\t\tCleanupOperations:    atomic.LoadInt64(&m.metrics.CleanupOperations),\n\t\tLastCleanupTime:      m.metrics.LastCleanupTime,\n\t\tLeakPrevented:        atomic.LoadInt64(&m.metrics.LeakPrevented),\n\t\tForceCleanups:        atomic.LoadInt64(&m.metrics.ForceCleanups),\n\t}\n}\n\n// updateConnectionMetrics calculates and updates connection-related metrics\nfunc (m *MemoryLeakPreventionManager) updateConnectionMetrics() {\n\tnow := time.Now()\n\tvar totalAge time.Duration\n\tactiveCount := int64(0)\n\t\n\tm.registryMutex.RLock()\n\tfor _, connInfo := range m.connectionRegistry {\n\t\tconnInfo.mutex.RLock()\n\t\tif connInfo.IsActive {\n\t\t\ttotalAge += now.Sub(connInfo.CreatedAt)\n\t\t\tactiveCount++\n\t\t}\n\t\tconnInfo.mutex.RUnlock()\n\t}\n\tm.registryMutex.RUnlock()\n\t\n\t// Calculate average connection age\n\tvar avgAge time.Duration\n\tif activeCount > 0 {\n\t\tavgAge = totalAge / time.Duration(activeCount)\n\t}\n\t\n\t// Update metrics\n\tm.metrics.mutex.Lock()\n\tm.metrics.ActiveConnections = activeCount\n\tm.metrics.AverageConnectionAge = avgAge\n\tm.metrics.ActiveGoroutines = activeCount * 2 // Assume read + write goroutines per connection\n\tm.metrics.MemoryUsageBytes = m.estimateMemoryUsage()\n\tm.metrics.mutex.Unlock()\n}\n\n// estimateMemoryUsage provides rough memory usage estimation\nfunc (m *MemoryLeakPreventionManager) estimateMemoryUsage() int64 {\n\t// Rough estimation: connection struct + channels + buffers\n\tconnectionSize := int64(1024)  // Approximate size per connection\n\tchannelSize := int64(256 * 8)  // 256 buffer * 8 bytes per message pointer\n\t\n\tactiveConnections := atomic.LoadInt64(&m.activeConnections)\n\treturn activeConnections * (connectionSize + channelSize)\n}\n\n// generateConnectionID creates a unique identifier for connections\nfunc (m *MemoryLeakPreventionManager) generateConnectionID(remoteAddr string) string {\n\ttimestamp := time.Now().UnixNano()\n\treturn fmt.Sprintf(\"conn-%d-%s\", timestamp, remoteAddr)\n}\n\n// Shutdown gracefully shuts down the memory leak prevention manager\nfunc (m *MemoryLeakPreventionManager) Shutdown(ctx context.Context) error {\n\tvar shutdownErr error\n\t\n\tm.shutdownOnce.Do(func() {\n\t\tm.isShutdown = true\n\t\t\n\t\tlog.Printf(\"Shutting down WebSocket memory leak prevention manager...\")\n\t\t\n\t\t// Stop cleanup routine\n\t\tclose(m.stopCleanup)\n\t\tm.cleanupWg.Wait()\n\t\t\n\t\t// Cancel all connections\n\t\tm.cancel()\n\t\t\n\t\t// Clean up all remaining connections\n\t\tm.registryMutex.Lock()\n\t\tconnectionCount := len(m.connectionRegistry)\n\t\tfor connID, connInfo := range m.connectionRegistry {\n\t\t\tm.cleanupConnection(connInfo)\n\t\t\tdelete(m.connectionRegistry, connID)\n\t\t}\n\t\tm.registryMutex.Unlock()\n\t\t\n\t\t// Reset counters\n\t\tatomic.StoreInt64(&m.activeConnections, 0)\n\t\t\n\t\tlog.Printf(\"WebSocket memory leak prevention manager shutdown completed. Cleaned up %d connections.\", connectionCount)\n\t})\n\t\n\treturn shutdownErr\n}\n\n// IsHealthy returns whether the manager is operating within healthy parameters\nfunc (m *MemoryLeakPreventionManager) IsHealthy() bool {\n\tmetrics := m.GetMemoryMetrics()\n\t\n\t// Check various health indicators\n\tconnectionRatio := float64(metrics.ActiveConnections) / float64(m.maxConnections)\n\thasRecentCleanup := time.Since(metrics.LastCleanupTime) < m.cleanupInterval*2\n\t\n\treturn !m.isShutdown && \n\t\tconnectionRatio < 0.9 && // Less than 90% of max connections\n\t\thasRecentCleanup &&      // Recent cleanup activity\n\t\tmetrics.FailedConnections < 100 // Reasonable failure count\n}\n\n// GetConnectionInfo returns information about a specific connection\nfunc (m *MemoryLeakPreventionManager) GetConnectionInfo(connID string) (*ConnectionInfo, bool) {\n\tm.registryMutex.RLock()\n\tdefer m.registryMutex.RUnlock()\n\t\n\tconnInfo, exists := m.connectionRegistry[connID]\n\treturn connInfo, exists\n}\n\n// ListActiveConnections returns a list of all active connection IDs\nfunc (m *MemoryLeakPreventionManager) ListActiveConnections() []string {\n\tm.registryMutex.RLock()\n\tdefer m.registryMutex.RUnlock()\n\t\n\tconnections := make([]string, 0, len(m.connectionRegistry))\n\tfor connID, connInfo := range m.connectionRegistry {\n\t\tconnInfo.mutex.RLock()\n\t\tif connInfo.IsActive {\n\t\t\tconnections = append(connections, connID)\n\t\t}\n\t\tconnInfo.mutex.RUnlock()\n\t}\n\t\n\treturn connections\n}\n\n// Configuration helper functions\n\nfunc getMaxConnections(cfg *config.Config) int {\n\tif cfg != nil && cfg.Server.MaxWebSocketConnections > 0 {\n\t\treturn cfg.Server.MaxWebSocketConnections\n\t}\n\treturn 100 // Default maximum connections\n}\n\nfunc getCleanupInterval(cfg *config.Config) time.Duration {\n\tif cfg != nil && cfg.Timeouts.WebSocketCleanup > 0 {\n\t\treturn cfg.Timeouts.WebSocketCleanup\n\t}\n\treturn 5 * time.Minute // Default cleanup interval\n}\n\nfunc getConnectionTimeout(cfg *config.Config) time.Duration {\n\tif cfg != nil && cfg.Timeouts.WebSocket > 0 {\n\t\treturn cfg.Timeouts.WebSocket\n\t}\n\treturn 10 * time.Minute // Default connection timeout\n}"
    },
    "tool_response": {
      "type": "create",
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/server/websocket_memory_fix.go",
      "content": "package server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"net/http\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/coder/websocket\"\n\t\"github.com/conneroisu/templar/internal/config\"\n)\n\n// MemoryLeakPreventionManager provides comprehensive memory leak prevention for WebSocket connections\n// \n// This manager addresses common WebSocket memory leak sources:\n// 1. Goroutine leaks from improperly terminated read/write pumps\n// 2. Channel leaks from unclosed send channels\n// 3. Connection leaks from abandoned WebSocket connections\n// 4. Client map unbounded growth from stale connections\n// 5. Race conditions in concurrent client management\n//\n// Key Features:\n// - Automatic stale connection cleanup with configurable intervals\n// - Goroutine lifecycle tracking and termination guarantees\n// - Channel resource management with proper cleanup\n// - Connection timeout enforcement with activity monitoring\n// - Resource limit enforcement to prevent unbounded growth\n// - Comprehensive metrics for memory leak detection\ntype MemoryLeakPreventionManager struct {\n\t// Resource tracking\n\tactiveConnections int64                    // Atomic counter for active connections\n\ttotalConnections  int64                    // Atomic counter for total connections created\n\tcleanupCount      int64                    // Atomic counter for cleanup operations\n\t\n\t// Connection lifecycle management\n\tconnectionRegistry map[string]*ConnectionInfo  // Registry of all connections with metadata\n\tregistryMutex     sync.RWMutex                 // Protects connection registry\n\t\n\t// Resource limits and configuration\n\tmaxConnections    int                          // Maximum allowed concurrent connections\n\tcleanupInterval   time.Duration               // Interval for periodic cleanup operations\n\tconnectionTimeout time.Duration               // Timeout for idle connections\n\t\n\t// Cleanup coordination\n\tcleanupTimer      *time.Timer                 // Timer for periodic cleanup\n\tstopCleanup       chan struct{}               // Channel to stop cleanup goroutine\n\tcleanupWg         sync.WaitGroup              // Ensures cleanup goroutine termination\n\t\n\t// Metrics and monitoring\n\tmetrics           *WebSocketMemoryMetrics     // Detailed memory and resource metrics\n\t\n\t// Configuration\n\tconfig            *config.Config              // Configuration for timeout values\n\t\n\t// Lifecycle management\n\tctx               context.Context             // Context for coordinated cancellation\n\tcancel            context.CancelFunc          // Cancellation function\n\tshutdownOnce      sync.Once                   // Ensures shutdown happens exactly once\n\tisShutdown        bool                        // Shutdown state indicator\n}\n\n// ConnectionInfo tracks metadata for each WebSocket connection to prevent leaks\ntype ConnectionInfo struct {\n\tID            string              // Unique connection identifier\n\tRemoteAddr    string              // Client remote address\n\tCreatedAt     time.Time           // Connection creation timestamp\n\tLastActivity  time.Time           // Last activity timestamp for timeout detection\n\tConn          *websocket.Conn     // WebSocket connection reference\n\tSend          chan []byte         // Send channel reference for cleanup\n\tContext       context.Context     // Connection-specific context\n\tCancel        context.CancelFunc  // Connection cancellation function\n\tGoroutineIDs  []string           // Track associated goroutines for leak detection\n\tIsActive      bool               // Active state flag\n\tmutex         sync.RWMutex       // Protects connection info fields\n}\n\n// WebSocketMemoryMetrics provides comprehensive metrics for memory leak detection\ntype WebSocketMemoryMetrics struct {\n\t// Connection metrics\n\tActiveConnections    int64    // Current active connections\n\tTotalConnections     int64    // Total connections created\n\tCleanedConnections   int64    // Connections cleaned up\n\tFailedConnections    int64    // Failed connection attempts\n\tTimeoutConnections   int64    // Connections closed due to timeout\n\t\n\t// Resource metrics\n\tActiveGoroutines     int64    // Active goroutines count\n\tActiveChannels       int64    // Active send channels count\n\tMemoryUsageBytes     int64    // Estimated memory usage\n\t\n\t// Performance metrics\n\tAverageConnectionAge time.Duration  // Average connection lifetime\n\tCleanupOperations    int64          // Total cleanup operations performed\n\tLastCleanupTime      time.Time      // Last cleanup operation timestamp\n\t\n\t// Error metrics\n\tLeakPrevented        int64          // Number of potential leaks prevented\n\tForceCleanups        int64          // Number of forced cleanups performed\n\t\n\tmutex                sync.RWMutex   // Protects metrics updates\n}\n\n// NewMemoryLeakPreventionManager creates a comprehensive memory leak prevention system\nfunc NewMemoryLeakPreventionManager(cfg ...*config.Config) *MemoryLeakPreventionManager {\n\t// Create cancellable context\n\tctx, cancel := context.WithCancel(context.Background())\n\t\n\t// Use first config if provided, otherwise use defaults\n\tvar config *config.Config\n\tif len(cfg) > 0 {\n\t\tconfig = cfg[0]\n\t}\n\t\n\tmanager := &MemoryLeakPreventionManager{\n\t\tconnectionRegistry: make(map[string]*ConnectionInfo),\n\t\tmaxConnections:     getMaxConnections(config),\n\t\tcleanupInterval:    getCleanupInterval(config),\n\t\tconnectionTimeout:  getConnectionTimeout(config),\n\t\tstopCleanup:        make(chan struct{}),\n\t\tmetrics:           &WebSocketMemoryMetrics{},\n\t\tconfig:            config,\n\t\tctx:               ctx,\n\t\tcancel:            cancel,\n\t\tisShutdown:        false,\n\t}\n\t\n\t// Start periodic cleanup goroutine\n\tmanager.startCleanupRoutine()\n\t\n\treturn manager\n}\n\n// RegisterConnection registers a new WebSocket connection for memory leak prevention\nfunc (m *MemoryLeakPreventionManager) RegisterConnection(\n\tconn *websocket.Conn,\n\tremoteAddr string,\n) (*ConnectionInfo, error) {\n\t// Check if shutdown\n\tif m.isShutdown {\n\t\treturn nil, fmt.Errorf(\"manager is shutting down\")\n\t}\n\t\n\t// Check connection limit\n\tcurrentCount := atomic.LoadInt64(&m.activeConnections)\n\tif currentCount >= int64(m.maxConnections) {\n\t\tatomic.AddInt64(&m.metrics.FailedConnections, 1)\n\t\treturn nil, fmt.Errorf(\"connection limit exceeded: %d/%d\", currentCount, m.maxConnections)\n\t}\n\t\n\t// Create connection-specific context\n\tconnCtx, connCancel := context.WithCancel(m.ctx)\n\t\n\t// Generate unique connection ID\n\tconnID := m.generateConnectionID(remoteAddr)\n\t\n\t// Create connection info with comprehensive tracking\n\tconnInfo := &ConnectionInfo{\n\t\tID:           connID,\n\t\tRemoteAddr:   remoteAddr,\n\t\tCreatedAt:    time.Now(),\n\t\tLastActivity: time.Now(),\n\t\tConn:         conn,\n\t\tSend:         make(chan []byte, 256), // Buffered channel\n\t\tContext:      connCtx,\n\t\tCancel:       connCancel,\n\t\tGoroutineIDs: make([]string, 0, 2),  // Expect read and write goroutines\n\t\tIsActive:     true,\n\t}\n\t\n\t// Register connection in thread-safe manner\n\tm.registryMutex.Lock()\n\tm.connectionRegistry[connID] = connInfo\n\tm.registryMutex.Unlock()\n\t\n\t// Update metrics atomically\n\tatomic.AddInt64(&m.activeConnections, 1)\n\tatomic.AddInt64(&m.totalConnections, 1)\n\tm.updateConnectionMetrics()\n\t\n\tlog.Printf(\"WebSocket connection registered: %s (total: %d)\", connID, atomic.LoadInt64(&m.activeConnections))\n\t\n\treturn connInfo, nil\n}\n\n// UnregisterConnection safely removes a connection and prevents memory leaks\nfunc (m *MemoryLeakPreventionManager) UnregisterConnection(connID string) error {\n\tm.registryMutex.Lock()\n\tconnInfo, exists := m.connectionRegistry[connID]\n\tif exists {\n\t\tdelete(m.connectionRegistry, connID)\n\t}\n\tm.registryMutex.Unlock()\n\t\n\tif !exists {\n\t\tlog.Printf(\"Warning: Attempted to unregister unknown connection: %s\", connID)\n\t\treturn fmt.Errorf(\"connection not found: %s\", connID)\n\t}\n\t\n\t// Perform comprehensive cleanup\n\tm.cleanupConnection(connInfo)\n\t\n\t// Update metrics\n\tatomic.AddInt64(&m.activeConnections, -1)\n\tatomic.AddInt64(&m.metrics.CleanedConnections, 1)\n\tm.updateConnectionMetrics()\n\t\n\tlog.Printf(\"WebSocket connection unregistered: %s (remaining: %d)\", \n\t\tconnID, atomic.LoadInt64(&m.activeConnections))\n\t\n\treturn nil\n}\n\n// UpdateConnectionActivity updates the last activity timestamp for a connection\nfunc (m *MemoryLeakPreventionManager) UpdateConnectionActivity(connID string) {\n\tm.registryMutex.RLock()\n\tconnInfo, exists := m.connectionRegistry[connID]\n\tm.registryMutex.RUnlock()\n\t\n\tif exists {\n\t\tconnInfo.mutex.Lock()\n\t\tconnInfo.LastActivity = time.Now()\n\t\tconnInfo.mutex.Unlock()\n\t}\n}\n\n// startCleanupRoutine starts the periodic cleanup goroutine for stale connections\nfunc (m *MemoryLeakPreventionManager) startCleanupRoutine() {\n\tm.cleanupWg.Add(1)\n\tgo func() {\n\t\tdefer m.cleanupWg.Done()\n\t\t\n\t\tticker := time.NewTicker(m.cleanupInterval)\n\t\tdefer ticker.Stop()\n\t\t\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase <-ticker.C:\n\t\t\t\tm.performCleanup()\n\t\t\t\t\n\t\t\tcase <-m.stopCleanup:\n\t\t\t\treturn\n\t\t\t\t\n\t\t\tcase <-m.ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n}\n\n// performCleanup identifies and removes stale connections to prevent memory leaks\nfunc (m *MemoryLeakPreventionManager) performCleanup() {\n\tnow := time.Now()\n\tvar staleConnections []string\n\t\n\t// Identify stale connections\n\tm.registryMutex.RLock()\n\tfor connID, connInfo := range m.connectionRegistry {\n\t\tconnInfo.mutex.RLock()\n\t\tisStale := now.Sub(connInfo.LastActivity) > m.connectionTimeout\n\t\tisInactive := !connInfo.IsActive\n\t\tconnInfo.mutex.RUnlock()\n\t\t\n\t\tif isStale || isInactive {\n\t\t\tstaleConnections = append(staleConnections, connID)\n\t\t}\n\t}\n\tm.registryMutex.RUnlock()\n\t\n\t// Clean up stale connections\n\tcleanedCount := 0\n\tfor _, connID := range staleConnections {\n\t\tif err := m.UnregisterConnection(connID); err == nil {\n\t\t\tcleanedCount++\n\t\t\tatomic.AddInt64(&m.metrics.TimeoutConnections, 1)\n\t\t}\n\t}\n\t\n\tif cleanedCount > 0 {\n\t\tlog.Printf(\"Cleanup completed: removed %d stale connections\", cleanedCount)\n\t\tatomic.AddInt64(&m.metrics.CleanupOperations, 1)\n\t}\n\t\n\t// Update cleanup metrics\n\tm.metrics.mutex.Lock()\n\tm.metrics.LastCleanupTime = now\n\tm.metrics.mutex.Unlock()\n\t\n\tatomic.AddInt64(&m.cleanupCount, 1)\n}\n\n// cleanupConnection performs comprehensive cleanup for a single connection\nfunc (m *MemoryLeakPreventionManager) cleanupConnection(connInfo *ConnectionInfo) {\n\tif connInfo == nil {\n\t\treturn\n\t}\n\t\n\t// Mark as inactive to prevent further use\n\tconnInfo.mutex.Lock()\n\tconnInfo.IsActive = false\n\tconnInfo.mutex.Unlock()\n\t\n\t// Cancel connection context to terminate associated goroutines\n\tif connInfo.Cancel != nil {\n\t\tconnInfo.Cancel()\n\t}\n\t\n\t// Close send channel safely\n\tif connInfo.Send != nil {\n\t\tselect {\n\t\tcase <-connInfo.Send: // Drain any pending messages\n\t\tdefault:\n\t\t}\n\t\tclose(connInfo.Send)\n\t\tatomic.AddInt64(&m.metrics.ActiveChannels, -1)\n\t}\n\t\n\t// Close WebSocket connection\n\tif connInfo.Conn != nil {\n\t\tconnInfo.Conn.Close(websocket.StatusNormalClosure, \"Cleanup\")\n\t}\n\t\n\tlog.Printf(\"Connection cleanup completed for: %s\", connInfo.ID)\n}\n\n// ForceCleanupStaleConnections immediately cleans up all stale connections\nfunc (m *MemoryLeakPreventionManager) ForceCleanupStaleConnections() int {\n\tm.performCleanup()\n\tatomic.AddInt64(&m.metrics.ForceCleanups, 1)\n\treturn int(atomic.LoadInt64(&m.metrics.TimeoutConnections))\n}\n\n// GetMemoryMetrics returns current memory and resource metrics\nfunc (m *MemoryLeakPreventionManager) GetMemoryMetrics() WebSocketMemoryMetrics {\n\tm.updateConnectionMetrics()\n\t\n\tm.metrics.mutex.RLock()\n\tdefer m.metrics.mutex.RUnlock()\n\t\n\treturn WebSocketMemoryMetrics{\n\t\tActiveConnections:    atomic.LoadInt64(&m.activeConnections),\n\t\tTotalConnections:     atomic.LoadInt64(&m.totalConnections),\n\t\tCleanedConnections:   atomic.LoadInt64(&m.metrics.CleanedConnections),\n\t\tFailedConnections:    atomic.LoadInt64(&m.metrics.FailedConnections),\n\t\tTimeoutConnections:   atomic.LoadInt64(&m.metrics.TimeoutConnections),\n\t\tActiveGoroutines:     atomic.LoadInt64(&m.metrics.ActiveGoroutines),\n\t\tActiveChannels:       atomic.LoadInt64(&m.metrics.ActiveChannels),\n\t\tMemoryUsageBytes:     atomic.LoadInt64(&m.metrics.MemoryUsageBytes),\n\t\tAverageConnectionAge: m.metrics.AverageConnectionAge,\n\t\tCleanupOperations:    atomic.LoadInt64(&m.metrics.CleanupOperations),\n\t\tLastCleanupTime:      m.metrics.LastCleanupTime,\n\t\tLeakPrevented:        atomic.LoadInt64(&m.metrics.LeakPrevented),\n\t\tForceCleanups:        atomic.LoadInt64(&m.metrics.ForceCleanups),\n\t}\n}\n\n// updateConnectionMetrics calculates and updates connection-related metrics\nfunc (m *MemoryLeakPreventionManager) updateConnectionMetrics() {\n\tnow := time.Now()\n\tvar totalAge time.Duration\n\tactiveCount := int64(0)\n\t\n\tm.registryMutex.RLock()\n\tfor _, connInfo := range m.connectionRegistry {\n\t\tconnInfo.mutex.RLock()\n\t\tif connInfo.IsActive {\n\t\t\ttotalAge += now.Sub(connInfo.CreatedAt)\n\t\t\tactiveCount++\n\t\t}\n\t\tconnInfo.mutex.RUnlock()\n\t}\n\tm.registryMutex.RUnlock()\n\t\n\t// Calculate average connection age\n\tvar avgAge time.Duration\n\tif activeCount > 0 {\n\t\tavgAge = totalAge / time.Duration(activeCount)\n\t}\n\t\n\t// Update metrics\n\tm.metrics.mutex.Lock()\n\tm.metrics.ActiveConnections = activeCount\n\tm.metrics.AverageConnectionAge = avgAge\n\tm.metrics.ActiveGoroutines = activeCount * 2 // Assume read + write goroutines per connection\n\tm.metrics.MemoryUsageBytes = m.estimateMemoryUsage()\n\tm.metrics.mutex.Unlock()\n}\n\n// estimateMemoryUsage provides rough memory usage estimation\nfunc (m *MemoryLeakPreventionManager) estimateMemoryUsage() int64 {\n\t// Rough estimation: connection struct + channels + buffers\n\tconnectionSize := int64(1024)  // Approximate size per connection\n\tchannelSize := int64(256 * 8)  // 256 buffer * 8 bytes per message pointer\n\t\n\tactiveConnections := atomic.LoadInt64(&m.activeConnections)\n\treturn activeConnections * (connectionSize + channelSize)\n}\n\n// generateConnectionID creates a unique identifier for connections\nfunc (m *MemoryLeakPreventionManager) generateConnectionID(remoteAddr string) string {\n\ttimestamp := time.Now().UnixNano()\n\treturn fmt.Sprintf(\"conn-%d-%s\", timestamp, remoteAddr)\n}\n\n// Shutdown gracefully shuts down the memory leak prevention manager\nfunc (m *MemoryLeakPreventionManager) Shutdown(ctx context.Context) error {\n\tvar shutdownErr error\n\t\n\tm.shutdownOnce.Do(func() {\n\t\tm.isShutdown = true\n\t\t\n\t\tlog.Printf(\"Shutting down WebSocket memory leak prevention manager...\")\n\t\t\n\t\t// Stop cleanup routine\n\t\tclose(m.stopCleanup)\n\t\tm.cleanupWg.Wait()\n\t\t\n\t\t// Cancel all connections\n\t\tm.cancel()\n\t\t\n\t\t// Clean up all remaining connections\n\t\tm.registryMutex.Lock()\n\t\tconnectionCount := len(m.connectionRegistry)\n\t\tfor connID, connInfo := range m.connectionRegistry {\n\t\t\tm.cleanupConnection(connInfo)\n\t\t\tdelete(m.connectionRegistry, connID)\n\t\t}\n\t\tm.registryMutex.Unlock()\n\t\t\n\t\t// Reset counters\n\t\tatomic.StoreInt64(&m.activeConnections, 0)\n\t\t\n\t\tlog.Printf(\"WebSocket memory leak prevention manager shutdown completed. Cleaned up %d connections.\", connectionCount)\n\t})\n\t\n\treturn shutdownErr\n}\n\n// IsHealthy returns whether the manager is operating within healthy parameters\nfunc (m *MemoryLeakPreventionManager) IsHealthy() bool {\n\tmetrics := m.GetMemoryMetrics()\n\t\n\t// Check various health indicators\n\tconnectionRatio := float64(metrics.ActiveConnections) / float64(m.maxConnections)\n\thasRecentCleanup := time.Since(metrics.LastCleanupTime) < m.cleanupInterval*2\n\t\n\treturn !m.isShutdown && \n\t\tconnectionRatio < 0.9 && // Less than 90% of max connections\n\t\thasRecentCleanup &&      // Recent cleanup activity\n\t\tmetrics.FailedConnections < 100 // Reasonable failure count\n}\n\n// GetConnectionInfo returns information about a specific connection\nfunc (m *MemoryLeakPreventionManager) GetConnectionInfo(connID string) (*ConnectionInfo, bool) {\n\tm.registryMutex.RLock()\n\tdefer m.registryMutex.RUnlock()\n\t\n\tconnInfo, exists := m.connectionRegistry[connID]\n\treturn connInfo, exists\n}\n\n// ListActiveConnections returns a list of all active connection IDs\nfunc (m *MemoryLeakPreventionManager) ListActiveConnections() []string {\n\tm.registryMutex.RLock()\n\tdefer m.registryMutex.RUnlock()\n\t\n\tconnections := make([]string, 0, len(m.connectionRegistry))\n\tfor connID, connInfo := range m.connectionRegistry {\n\t\tconnInfo.mutex.RLock()\n\t\tif connInfo.IsActive {\n\t\t\tconnections = append(connections, connID)\n\t\t}\n\t\tconnInfo.mutex.RUnlock()\n\t}\n\t\n\treturn connections\n}\n\n// Configuration helper functions\n\nfunc getMaxConnections(cfg *config.Config) int {\n\tif cfg != nil && cfg.Server.MaxWebSocketConnections > 0 {\n\t\treturn cfg.Server.MaxWebSocketConnections\n\t}\n\treturn 100 // Default maximum connections\n}\n\nfunc getCleanupInterval(cfg *config.Config) time.Duration {\n\tif cfg != nil && cfg.Timeouts.WebSocketCleanup > 0 {\n\t\treturn cfg.Timeouts.WebSocketCleanup\n\t}\n\treturn 5 * time.Minute // Default cleanup interval\n}\n\nfunc getConnectionTimeout(cfg *config.Config) time.Duration {\n\tif cfg != nil && cfg.Timeouts.WebSocket > 0 {\n\t\treturn cfg.Timeouts.WebSocket\n\t}\n\treturn 10 * time.Minute // Default connection timeout\n}",
      "structuredPatch": []
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Edit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
      "old_string": "// Stop stops the build pipeline and waits for all goroutines to finish\nfunc (bp *BuildPipeline) Stop() {\n\tif bp.cancel != nil {\n\t\tbp.cancel()\n\t}\n\n\t// Wait for all workers to finish\n\tbp.workerWg.Wait()\n\n\t// Wait for result processor to finish\n\tbp.resultWg.Wait()\n}",
      "new_string": "// Stop stops the build pipeline and waits for all goroutines to finish\nfunc (bp *BuildPipeline) Stop() {\n\tif bp.cancel != nil {\n\t\tbp.cancel()\n\t}\n\n\t// Wait for all workers to finish\n\tbp.workerWg.Wait()\n\n\t// Wait for result processor to finish\n\tbp.resultWg.Wait()\n}\n\n// StopWithTimeout stops the build pipeline with a timeout for graceful shutdown\nfunc (bp *BuildPipeline) StopWithTimeout(timeout time.Duration) error {\n\tif bp.cancel != nil {\n\t\tbp.cancel()\n\t}\n\n\t// Use a channel to signal completion\n\tdone := make(chan struct{})\n\tgo func() {\n\t\t// Wait for all workers to finish\n\t\tbp.workerWg.Wait()\n\t\t// Wait for result processor to finish\n\t\tbp.resultWg.Wait()\n\t\tclose(done)\n\t}()\n\n\t// Wait for completion or timeout\n\tselect {\n\tcase <-done:\n\t\treturn nil\n\tcase <-time.After(timeout):\n\t\treturn fmt.Errorf(\"build pipeline shutdown timed out after %v\", timeout)\n\t}\n}"
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
      "oldString": "// Stop stops the build pipeline and waits for all goroutines to finish\nfunc (bp *BuildPipeline) Stop() {\n\tif bp.cancel != nil {\n\t\tbp.cancel()\n\t}\n\n\t// Wait for all workers to finish\n\tbp.workerWg.Wait()\n\n\t// Wait for result processor to finish\n\tbp.resultWg.Wait()\n}",
      "newString": "// Stop stops the build pipeline and waits for all goroutines to finish\nfunc (bp *BuildPipeline) Stop() {\n\tif bp.cancel != nil {\n\t\tbp.cancel()\n\t}\n\n\t// Wait for all workers to finish\n\tbp.workerWg.Wait()\n\n\t// Wait for result processor to finish\n\tbp.resultWg.Wait()\n}\n\n// StopWithTimeout stops the build pipeline with a timeout for graceful shutdown\nfunc (bp *BuildPipeline) StopWithTimeout(timeout time.Duration) error {\n\tif bp.cancel != nil {\n\t\tbp.cancel()\n\t}\n\n\t// Use a channel to signal completion\n\tdone := make(chan struct{})\n\tgo func() {\n\t\t// Wait for all workers to finish\n\t\tbp.workerWg.Wait()\n\t\t// Wait for result processor to finish\n\t\tbp.resultWg.Wait()\n\t\tclose(done)\n\t}()\n\n\t// Wait for completion or timeout\n\tselect {\n\tcase <-done:\n\t\treturn nil\n\tcase <-time.After(timeout):\n\t\treturn fmt.Errorf(\"build pipeline shutdown timed out after %v\", timeout)\n\t}\n}",
      "originalFile": "// Package build provides a concurrent build pipeline for templ components\n// with caching, error collection, and performance metrics.\n//\n// The build pipeline processes components through worker pools, maintains\n// an LRU cache for build results, and provides real-time build status\n// through callbacks and metrics. It supports parallel execution with\n// configurable worker counts and implements security-hardened command\n// execution with proper validation.\npackage build\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"hash/crc32\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/conneroisu/templar/internal/errors\"\n\t\"github.com/conneroisu/templar/internal/interfaces\"\n\t\"github.com/conneroisu/templar/internal/types\"\n)\n\n// crcTable is a pre-computed CRC32 Castagnoli table for faster hash generation\nvar crcTable = crc32.MakeTable(crc32.Castagnoli)\n\n// BuildPipeline manages the build process for templ components with concurrent\n// execution, intelligent caching, and comprehensive error handling.\n//\n// The pipeline provides:\n// - Concurrent build execution with configurable worker pools\n// - LRU caching with CRC32-based change detection\n// - Priority-based build queue management\n// - Real-time build metrics and status callbacks\n// - Memory optimization through object pooling\n// - Security-hardened command execution\n// - Comprehensive timeout management\ntype BuildPipeline struct {\n\t// compiler handles templ compilation with security validation\n\tcompiler *TemplCompiler\n\t// cache provides LRU-based build result caching\n\tcache *BuildCache\n\t// queue manages build tasks with priority ordering\n\tqueue *BuildQueue\n\t// workers defines the number of concurrent build workers\n\tworkers int\n\t// registry provides component information and change notifications\n\tregistry interfaces.ComponentRegistry\n\t// errorParser processes build errors and provides detailed diagnostics\n\terrorParser *errors.ErrorParser\n\t// metrics tracks build performance and success rates\n\tmetrics *BuildMetrics\n\t// callbacks receive build status updates for UI integration\n\tcallbacks []BuildCallback\n\t// workerWg synchronizes worker goroutine lifecycle\n\tworkerWg sync.WaitGroup\n\t// resultWg synchronizes result processing\n\tresultWg sync.WaitGroup\n\t// cancel terminates all pipeline operations gracefully\n\tcancel context.CancelFunc\n\t// objectPools optimize memory allocation for frequently used objects\n\tobjectPools *ObjectPools\n\t// slicePools reduce slice allocation overhead\n\tslicePools *SlicePools\n\t// workerPool manages the lifecycle of build workers\n\tworkerPool *WorkerPool\n\t// config provides timeout configuration for build operations\n\tconfig *config.Config\n}\n\n// BuildTask represents a build task in the priority queue with metadata\n// for scheduling and execution tracking.\ntype BuildTask struct {\n\t// Component contains the component information to be built\n\tComponent *types.ComponentInfo\n\t// Priority determines build order (higher values built first)\n\tPriority int\n\t// Timestamp records when the task was created for ordering\n\tTimestamp time.Time\n}\n\n// BuildResult represents the result of a build operation\ntype BuildResult struct {\n\tComponent    *types.ComponentInfo\n\tOutput       []byte\n\tError        error\n\tParsedErrors []*errors.ParsedError\n\tDuration     time.Duration\n\tCacheHit     bool\n\tHash         string\n}\n\n// BuildCallback is called when a build completes\ntype BuildCallback func(result BuildResult)\n\n// BuildQueue manages build tasks\ntype BuildQueue struct {\n\ttasks    chan BuildTask\n\tresults  chan BuildResult\n\tpriority chan BuildTask\n}\n\n// NewBuildPipeline creates a new build pipeline with optional timeout configuration\nfunc NewBuildPipeline(workers int, registry interfaces.ComponentRegistry, cfg ...*config.Config) *BuildPipeline {\n\tcompiler := NewTemplCompiler()\n\tcache := NewBuildCache(100*1024*1024, time.Hour) // 100MB, 1 hour TTL\n\n\tqueue := &BuildQueue{\n\t\ttasks:    make(chan BuildTask, 100),\n\t\tresults:  make(chan BuildResult, 100),\n\t\tpriority: make(chan BuildTask, 10),\n\t}\n\n\tmetrics := NewBuildMetrics()\n\n\t// Use first config if provided, otherwise nil\n\tvar config *config.Config\n\tif len(cfg) > 0 {\n\t\tconfig = cfg[0]\n\t}\n\n\treturn &BuildPipeline{\n\t\tcompiler:    compiler,\n\t\tcache:       cache,\n\t\tqueue:       queue,\n\t\tworkers:     workers,\n\t\tregistry:    registry,\n\t\terrorParser: errors.NewErrorParser(),\n\t\tmetrics:     metrics,\n\t\tcallbacks:   make([]BuildCallback, 0),\n\t\t// Initialize object pools for memory optimization\n\t\tobjectPools: NewObjectPools(),\n\t\tslicePools:  NewSlicePools(),\n\t\tworkerPool:  NewWorkerPool(),\n\t\tconfig:      config,\n\t}\n}\n\n// Start starts the build pipeline\nfunc (bp *BuildPipeline) Start(ctx context.Context) {\n\t// Create cancellable context\n\tctx, bp.cancel = context.WithCancel(ctx)\n\n\t// Start workers\n\tfor i := 0; i < bp.workers; i++ {\n\t\tbp.workerWg.Add(1)\n\t\tgo bp.worker(ctx)\n\t}\n\n\t// Start result processor\n\tbp.resultWg.Add(1)\n\tgo bp.processResults(ctx)\n}\n\n// Stop stops the build pipeline and waits for all goroutines to finish\nfunc (bp *BuildPipeline) Stop() {\n\tif bp.cancel != nil {\n\t\tbp.cancel()\n\t}\n\n\t// Wait for all workers to finish\n\tbp.workerWg.Wait()\n\n\t// Wait for result processor to finish\n\tbp.resultWg.Wait()\n}\n\n// Build queues a component for building\nfunc (bp *BuildPipeline) Build(component *types.ComponentInfo) {\n\t// Check if pipeline is shut down\n\tif bp.cancel == nil {\n\t\tfmt.Printf(\"Error: Build pipeline not started, dropping task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"pipeline_not_started\")\n\t\treturn\n\t}\n\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  1,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.tasks <- task:\n\t\t// Task successfully queued\n\tdefault:\n\t\t// Queue full - implement backpressure handling\n\t\t// Log the error and update metrics\n\t\tfmt.Printf(\"Warning: Build queue full, dropping task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"task_queue_full\")\n\n\t\t// Try to handle with retry or priority queue\n\t\tselect {\n\t\tcase bp.queue.priority <- task:\n\t\t\tfmt.Printf(\"Task for %s promoted to priority queue\\n\", component.Name)\n\t\tdefault:\n\t\t\tfmt.Printf(\"Error: Both queues full, build request lost for component %s\\n\", component.Name)\n\t\t\t// TODO: Implement persistent queue or callback for dropped tasks\n\t\t}\n\t}\n}\n\n// BuildWithPriority queues a component for building with high priority\nfunc (bp *BuildPipeline) BuildWithPriority(component *types.ComponentInfo) {\n\t// Check if pipeline is shut down\n\tif bp.cancel == nil {\n\t\tfmt.Printf(\"Error: Build pipeline not started, dropping priority task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"pipeline_not_started\")\n\t\treturn\n\t}\n\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  10,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.priority <- task:\n\t\t// Priority task successfully queued\n\tdefault:\n\t\t// Priority queue also full - this is a critical error\n\t\tfmt.Printf(\"Critical: Priority queue full, dropping high-priority task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"priority_queue_full\")\n\n\t\t// Could implement emergency handling here (e.g., block briefly or expand queue)\n\t\t// For now, log the critical error\n\t}\n}\n\n// AddCallback adds a callback to be called when builds complete\nfunc (bp *BuildPipeline) AddCallback(callback BuildCallback) {\n\tbp.callbacks = append(bp.callbacks, callback)\n}\n\n// GetMetrics returns the current build metrics\nfunc (bp *BuildPipeline) GetMetrics() BuildMetrics {\n\treturn bp.metrics.GetSnapshot()\n}\n\n// ClearCache clears the build cache\nfunc (bp *BuildPipeline) ClearCache() {\n\tbp.cache.Clear()\n}\n\n// GetCacheStats returns cache statistics\nfunc (bp *BuildPipeline) GetCacheStats() (int, int64, int64) {\n\treturn bp.cache.GetStats()\n}\n\n// getBuildTimeout returns the configured timeout for build operations\nfunc (bp *BuildPipeline) getBuildTimeout() time.Duration {\n\tif bp.config != nil && bp.config.Timeouts.Build > 0 {\n\t\treturn bp.config.Timeouts.Build\n\t}\n\t// Default fallback timeout if no configuration is available\n\treturn 5 * time.Minute\n}\n\n// worker processes build tasks\nfunc (bp *BuildPipeline) worker(ctx context.Context) {\n\tdefer bp.workerWg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase task := <-bp.queue.priority:\n\t\t\tbp.processBuildTask(ctx, task)\n\t\tcase task := <-bp.queue.tasks:\n\t\t\tbp.processBuildTask(ctx, task)\n\t\t}\n\t}\n}\n\nfunc (bp *BuildPipeline) processBuildTask(ctx context.Context, task BuildTask) {\n\tstart := time.Now()\n\n\t// Generate content hash for caching\n\tcontentHash := bp.generateContentHash(task.Component)\n\n\t// Check if context is cancelled before starting work\n\tselect {\n\tcase <-ctx.Done():\n\t\t// Context cancelled, return error\n\t\tbuildResult := bp.objectPools.GetBuildResult()\n\t\tbuildResult.Component = task.Component\n\t\tbuildResult.Output = nil\n\t\tbuildResult.Error = ctx.Err()\n\t\tbuildResult.ParsedErrors = nil\n\t\tbuildResult.Duration = time.Since(start)\n\t\tbuildResult.CacheHit = false\n\t\tbuildResult.Hash = contentHash\n\n\t\t// Non-blocking send to results channel\n\t\tselect {\n\t\tcase bp.queue.results <- *buildResult:\n\t\tdefault:\n\t\t\tbp.metrics.RecordDroppedResult(task.Component.Name, \"results_queue_full_cancelled\")\n\t\t}\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\tdefault:\n\t}\n\n\t// Check cache first\n\tif result, found := bp.cache.Get(contentHash); found {\n\t\t// Use object pool for cache hit result\n\t\tbuildResult := bp.objectPools.GetBuildResult()\n\t\tbuildResult.Component = task.Component\n\t\tbuildResult.Output = result\n\t\tbuildResult.Error = nil\n\t\tbuildResult.ParsedErrors = nil\n\t\tbuildResult.Duration = time.Since(start)\n\t\tbuildResult.CacheHit = true\n\t\tbuildResult.Hash = contentHash\n\n\t\t// Non-blocking send to results channel to prevent worker hangs\n\t\tselect {\n\t\tcase bp.queue.results <- *buildResult:\n\t\t\t// Cache hit result successfully queued\n\t\tcase <-ctx.Done():\n\t\t\t// Context cancelled while sending result\n\t\t\tbuildResult.Error = ctx.Err()\n\t\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\t\treturn\n\t\tdefault:\n\t\t\t// Results queue full - this could cause result loss\n\t\t\tfmt.Printf(\"Warning: Results queue full, dropping cache hit result for component %s\\n\", buildResult.Component.Name)\n\t\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full_cache_hit\")\n\t\t}\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\t}\n\n\t// Create timeout context for build operation based on configuration\n\tbuildTimeout := bp.getBuildTimeout()\n\tbuildCtx, cancel := context.WithTimeout(ctx, buildTimeout)\n\tdefer cancel()\n\n\t// Execute build with pooled output buffer and context-based timeout\n\toutput, err := bp.compiler.CompileWithPools(buildCtx, task.Component, bp.objectPools)\n\n\t// Parse errors if build failed\n\tvar parsedErrors []*errors.ParsedError\n\tif err != nil {\n\t\t// Wrap the error with build context for better debugging\n\t\terr = errors.WrapBuild(err, errors.ErrCodeBuildFailed, \n\t\t\t\"component compilation failed\", task.Component.Name).\n\t\t\tWithLocation(task.Component.FilePath, 0, 0)\n\t\tparsedErrors = bp.errorParser.ParseError(string(output))\n\t}\n\n\t// Use object pool for build result\n\tbuildResult := bp.objectPools.GetBuildResult()\n\tbuildResult.Component = task.Component\n\tbuildResult.Output = output\n\tbuildResult.Error = err\n\tbuildResult.ParsedErrors = parsedErrors\n\tbuildResult.Duration = time.Since(start)\n\tbuildResult.CacheHit = false\n\tbuildResult.Hash = contentHash\n\n\t// Cache successful builds\n\tif err == nil {\n\t\tbp.cache.Set(contentHash, output)\n\t}\n\n\t// Non-blocking send to results channel to prevent worker hangs\n\tselect {\n\tcase bp.queue.results <- *buildResult:\n\t\t// Result successfully queued\n\tdefault:\n\t\t// Results queue full - this could cause result loss\n\t\tfmt.Printf(\"Warning: Results queue full, dropping result for component %s\\n\", buildResult.Component.Name)\n\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full\")\n\t}\n\tbp.objectPools.PutBuildResult(buildResult)\n}\n\nfunc (bp *BuildPipeline) processResults(ctx context.Context) {\n\tdefer bp.resultWg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase result := <-bp.queue.results:\n\t\t\tbp.handleBuildResult(result)\n\t\t}\n\t}\n}\n\nfunc (bp *BuildPipeline) handleBuildResult(result BuildResult) {\n\t// Update metrics\n\tbp.metrics.RecordBuild(result)\n\n\t// Print result\n\tif result.Error != nil {\n\t\tfmt.Printf(\"Build failed for %s: %v\\n\", result.Component.Name, result.Error)\n\t\tif len(result.ParsedErrors) > 0 {\n\t\t\tfmt.Println(\"Parsed errors:\")\n\t\t\tfor _, err := range result.ParsedErrors {\n\t\t\t\tfmt.Print(err.FormatError())\n\t\t\t}\n\t\t}\n\t} else {\n\t\tstatus := \"succeeded\"\n\t\tif result.CacheHit {\n\t\t\tstatus = \"cached\"\n\t\t}\n\t\tfmt.Printf(\"Build %s for %s in %v\\n\", status, result.Component.Name, result.Duration)\n\t}\n\n\t// Call callbacks\n\tfor _, callback := range bp.callbacks {\n\t\tcallback(result)\n\t}\n}\n\n// generateContentHash generates a hash for component content with optimized single I/O operation\nfunc (bp *BuildPipeline) generateContentHash(component *types.ComponentInfo) string {\n\t// OPTIMIZATION: Use Stat() first to get metadata without opening file\n\t// This reduces file I/O operations by 70-90% for cached files\n\tstat, err := os.Stat(component.FilePath)\n\tif err != nil {\n\t\t// File not accessible, return fallback hash\n\t\t// Note: We don't need to wrap this error as it's an internal optimization\n\t\treturn component.FilePath\n\t}\n\n\t// Create metadata-based hash key for cache lookup\n\tmetadataKey := fmt.Sprintf(\"%s:%d:%d\", component.FilePath, stat.ModTime().Unix(), stat.Size())\n\n\t// Two-tier cache system: Check metadata cache first (no file I/O)\n\tif hash, found := bp.cache.GetHash(metadataKey); found {\n\t\t// Cache hit - no file I/O needed, just return cached hash\n\t\treturn hash\n\t}\n\n\t// Cache miss: Now we need to read file content and generate hash\n\t// Only open file when we actually need to read content\n\tfile, err := os.Open(component.FilePath)\n\tif err != nil {\n\t\treturn component.FilePath\n\t}\n\tdefer file.Close()\n\n\t// Use mmap for large files (>64KB) for better performance\n\tvar content []byte\n\tif stat.Size() > 64*1024 {\n\t\t// Use mmap for large files\n\t\tcontent, err = bp.readFileWithMmap(file, stat.Size())\n\t\tif err != nil {\n\t\t\t// Fallback to regular read\n\t\t\tcontent, err = io.ReadAll(file)\n\t\t}\n\t} else {\n\t\t// Regular read for small files\n\t\tcontent, err = io.ReadAll(file)\n\t}\n\n\tif err != nil {\n\t\t// Fallback to metadata-based hash\n\t\treturn fmt.Sprintf(\"%s:%d\", component.FilePath, stat.ModTime().Unix())\n\t}\n\n\t// Generate content hash using CRC32 Castagnoli for faster file change detection\n\tcrcHash := crc32.Checksum(content, crcTable)\n\tcontentHash := strconv.FormatUint(uint64(crcHash), 16)\n\n\t// Cache the hash with metadata key for future lookups\n\tbp.cache.SetHash(metadataKey, contentHash)\n\n\treturn contentHash\n}\n\n// readFileWithMmap reads file content using memory mapping for better performance on large files\nfunc (bp *BuildPipeline) readFileWithMmap(file *os.File, size int64) ([]byte, error) {\n\t// Memory map the file for efficient reading\n\tmmap, err := syscall.Mmap(int(file.Fd()), 0, int(size), syscall.PROT_READ, syscall.MAP_SHARED)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Copy the mapped data to avoid keeping the mapping open\n\tcontent := make([]byte, size)\n\tcopy(content, mmap)\n\n\t// Unmap the memory\n\tif err := syscall.Munmap(mmap); err != nil {\n\t\t// Log warning but don't fail - we have the content\n\t\t// Could add logging here if logger is available\n\t}\n\n\treturn content, nil\n}\n\n// generateContentHashesBatch processes multiple components in a single batch for better I/O efficiency\nfunc (bp *BuildPipeline) generateContentHashesBatch(components []*types.ComponentInfo) map[string]string {\n\tresults := make(map[string]string, len(components))\n\n\t// Group components by whether they need content reading (cache misses)\n\tvar needsReading []*types.ComponentInfo\n\n\t// First pass: check metadata-based cache for all components (no file I/O)\n\tfor _, component := range components {\n\t\t// OPTIMIZATION: Use efficient Stat() + metadata cache check first\n\t\tif stat, err := os.Stat(component.FilePath); err == nil {\n\t\t\tmetadataKey := fmt.Sprintf(\"%s:%d:%d\", component.FilePath, stat.ModTime().Unix(), stat.Size())\n\n\t\t\t// Check cache with metadata key\n\t\t\tif hash, found := bp.cache.GetHash(metadataKey); found {\n\t\t\t\t// Cache hit - no file reading needed\n\t\t\t\tresults[component.FilePath] = hash\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// Cache miss - needs content reading\n\t\tneedsReading = append(needsReading, component)\n\t}\n\n\t// Second pass: batch process cache misses with optimized I/O\n\tif len(needsReading) > 0 {\n\t\thashResults := bp.batchReadAndHash(needsReading)\n\t\tfor filePath, hash := range hashResults {\n\t\t\tresults[filePath] = hash\n\t\t}\n\t}\n\n\treturn results\n}\n\n// batchReadAndHash reads and hashes multiple files efficiently\nfunc (bp *BuildPipeline) batchReadAndHash(components []*types.ComponentInfo) map[string]string {\n\tresults := make(map[string]string, len(components))\n\n\t// Process each component with optimized I/O\n\tfor _, component := range components {\n\t\thash := bp.generateContentHash(component)\n\t\tresults[component.FilePath] = hash\n\t}\n\n\treturn results\n}\n\n// FileDiscoveryResult represents the result of discovering files in a directory\ntype FileDiscoveryResult struct {\n\tFiles       []*types.ComponentInfo\n\tErrors      []error\n\tDuration    time.Duration\n\tDiscovered  int64\n\tSkipped     int64\n}\n\n// FileDiscoveryStats tracks file discovery performance metrics\ntype FileDiscoveryStats struct {\n\tTotalFiles     int64\n\tProcessedFiles int64\n\tSkippedFiles   int64\n\tErrors         int64\n\tDuration       time.Duration\n\tWorkerCount    int\n}\n\n// ParallelFileProcessor provides parallel file processing capabilities\ntype ParallelFileProcessor struct {\n\tworkerCount int\n\tmaxDepth    int\n\tfilters     []string\n\tstats       *FileDiscoveryStats\n}\n\n// NewParallelFileProcessor creates a new parallel file processor\nfunc NewParallelFileProcessor(workerCount int) *ParallelFileProcessor {\n\treturn &ParallelFileProcessor{\n\t\tworkerCount: workerCount,\n\t\tmaxDepth:    10, // Default max depth\n\t\tfilters:     []string{\".templ\"},\n\t\tstats:       &FileDiscoveryStats{},\n\t}\n}\n\n// DiscoverFiles discovers component files in parallel using filepath.WalkDir\nfunc (pfp *ParallelFileProcessor) DiscoverFiles(ctx context.Context, rootPaths []string) (*FileDiscoveryResult, error) {\n\tstart := time.Now()\n\tdefer func() {\n\t\tpfp.stats.Duration = time.Since(start)\n\t}()\n\n\t// Create channels for work distribution\n\tpathCh := make(chan string, len(rootPaths))\n\tresultCh := make(chan *types.ComponentInfo, 100)\n\terrorCh := make(chan error, 100)\n\n\t// Start workers\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < pfp.workerCount; i++ {\n\t\twg.Add(1)\n\t\tgo pfp.worker(ctx, pathCh, resultCh, errorCh, &wg)\n\t}\n\n\t// Send root paths to workers\n\tgo func() {\n\t\tdefer close(pathCh)\n\t\tfor _, path := range rootPaths {\n\t\t\tselect {\n\t\t\tcase pathCh <- path:\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Collect results\n\tvar files []*types.ComponentInfo\n\tvar errors []error\n\tvar discovered, skipped int64\n\n\t// Result collection goroutine\n\tdone := make(chan struct{})\n\tgo func() {\n\t\tdefer close(done)\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase component, ok := <-resultCh:\n\t\t\t\tif !ok {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tfiles = append(files, component)\n\t\t\t\tatomic.AddInt64(&discovered, 1)\n\t\t\tcase err, ok := <-errorCh:\n\t\t\t\tif !ok {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\terrors = append(errors, err)\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Wait for workers to complete\n\twg.Wait()\n\tclose(resultCh)\n\tclose(errorCh)\n\n\t// Wait for result collection to complete\n\t<-done\n\n\t// Update stats\n\tatomic.StoreInt64(&pfp.stats.ProcessedFiles, discovered)\n\tatomic.StoreInt64(&pfp.stats.SkippedFiles, skipped)\n\tatomic.StoreInt64(&pfp.stats.Errors, int64(len(errors)))\n\tpfp.stats.WorkerCount = pfp.workerCount\n\n\treturn &FileDiscoveryResult{\n\t\tFiles:      files,\n\t\tErrors:     errors,\n\t\tDuration:   time.Since(start),\n\t\tDiscovered: discovered,\n\t\tSkipped:    skipped,\n\t}, nil\n}\n\n// worker processes file discovery work\nfunc (pfp *ParallelFileProcessor) worker(ctx context.Context, pathCh <-chan string, resultCh chan<- *types.ComponentInfo, errorCh chan<- error, wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase rootPath, ok := <-pathCh:\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Walk directory tree using filepath.WalkDir for better performance\n\t\t\terr := filepath.WalkDir(rootPath, func(path string, d os.DirEntry, err error) error {\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\t// Skip directories\n\t\t\t\tif d.IsDir() {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\n\t\t\t\t// Check if file matches our filters\n\t\t\t\tif !pfp.matchesFilter(path) {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\n\t\t\t\t// Create component info\n\t\t\t\tcomponent := &types.ComponentInfo{\n\t\t\t\t\tName:       pfp.extractComponentName(path),\n\t\t\t\t\tFilePath:   path,\n\t\t\t\t\tPackage:    pfp.extractPackage(path),\n\t\t\t\t\tParameters: []types.ParameterInfo{},\n\t\t\t\t}\n\n\t\t\t\t// Send result non-blocking\n\t\t\t\tselect {\n\t\t\t\tcase resultCh <- component:\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn ctx.Err()\n\t\t\t\tdefault:\n\t\t\t\t\t// Channel full, skip this component\n\t\t\t\t\tatomic.AddInt64(&pfp.stats.SkippedFiles, 1)\n\t\t\t\t}\n\n\t\t\t\treturn nil\n\t\t\t})\n\n\t\t\tif err != nil {\n\t\t\t\tselect {\n\t\t\t\tcase errorCh <- err:\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\tdefault:\n\t\t\t\t\t// Error channel full, skip error\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// matchesFilter checks if a file path matches the processor's filters\nfunc (pfp *ParallelFileProcessor) matchesFilter(path string) bool {\n\tfor _, filter := range pfp.filters {\n\t\tif strings.HasSuffix(path, filter) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// extractComponentName extracts component name from file path\nfunc (pfp *ParallelFileProcessor) extractComponentName(path string) string {\n\tbase := filepath.Base(path)\n\text := filepath.Ext(base)\n\treturn strings.TrimSuffix(base, ext)\n}\n\n// extractPackage extracts package name from file path\nfunc (pfp *ParallelFileProcessor) extractPackage(path string) string {\n\tdir := filepath.Dir(path)\n\treturn filepath.Base(dir)\n}\n\n// ProcessFilesBatch processes multiple files in parallel batches\nfunc (bp *BuildPipeline) ProcessFilesBatch(ctx context.Context, components []*types.ComponentInfo, batchSize int) (*FileDiscoveryResult, error) {\n\tstart := time.Now()\n\tvar totalDiscovered, totalSkipped int64\n\tvar allErrors []error\n\tvar allResults []*types.ComponentInfo\n\n\t// Process components in batches\n\tfor i := 0; i < len(components); i += batchSize {\n\t\tend := i + batchSize\n\t\tif end > len(components) {\n\t\t\tend = len(components)\n\t\t}\n\n\t\tbatch := components[i:end]\n\t\thashes := bp.generateContentHashesBatch(batch)\n\n\t\t// Process batch with caching\n\t\tfor _, component := range batch {\n\t\t\thash, exists := hashes[component.FilePath]\n\t\t\tif !exists {\n\t\t\t\tatomic.AddInt64(&totalSkipped, 1)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check cache first\n\t\t\tif _, found := bp.cache.Get(hash); found {\n\t\t\t\t// Cache hit, no processing needed\n\t\t\t\tallResults = append(allResults, component)\n\t\t\t\tatomic.AddInt64(&totalDiscovered, 1)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Process component\n\t\t\tallResults = append(allResults, component)\n\t\t\tatomic.AddInt64(&totalDiscovered, 1)\n\t\t}\n\t}\n\n\treturn &FileDiscoveryResult{\n\t\tFiles:      allResults,\n\t\tErrors:     allErrors,\n\t\tDuration:   time.Since(start),\n\t\tDiscovered: totalDiscovered,\n\t\tSkipped:    totalSkipped,\n\t}, nil\n}\n\n// BuildDirectory builds all components in a directory using parallel processing\nfunc (bp *BuildPipeline) BuildDirectory(ctx context.Context, rootPath string) error {\n\t// Create parallel file processor\n\tprocessor := NewParallelFileProcessor(bp.workers)\n\n\t// Discover files\n\tresult, err := processor.DiscoverFiles(ctx, []string{rootPath})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to discover files: %w\", err)\n\t}\n\n\t// Queue all discovered components for building\n\tfor _, component := range result.Files {\n\t\tbp.Build(component)\n\t}\n\n\tfmt.Printf(\"Directory build queued: %d components discovered in %v\\n\", \n\t\tresult.Discovered, result.Duration)\n\n\treturn nil\n}\n\n// GetFileDiscoveryStats returns file discovery performance statistics\nfunc (pfp *ParallelFileProcessor) GetFileDiscoveryStats() FileDiscoveryStats {\n\treturn FileDiscoveryStats{\n\t\tTotalFiles:     atomic.LoadInt64(&pfp.stats.TotalFiles),\n\t\tProcessedFiles: atomic.LoadInt64(&pfp.stats.ProcessedFiles),\n\t\tSkippedFiles:   atomic.LoadInt64(&pfp.stats.SkippedFiles),\n\t\tErrors:         atomic.LoadInt64(&pfp.stats.Errors),\n\t\tDuration:       pfp.stats.Duration,\n\t\tWorkerCount:    pfp.stats.WorkerCount,\n\t}\n}\n",
      "structuredPatch": [
        {
          "oldStart": 172,
          "oldLines": 6,
          "newStart": 172,
          "newLines": 31,
          "lines": [
            "   bp.resultWg.Wait()",
            " }",
            " ",
            "+// StopWithTimeout stops the build pipeline with a timeout for graceful shutdown",
            "+func (bp *BuildPipeline) StopWithTimeout(timeout time.Duration) error {",
            "+  if bp.cancel != nil {",
            "+    bp.cancel()",
            "+  }",
            "+",
            "+  // Use a channel to signal completion",
            "+  done := make(chan struct{})",
            "+  go func() {",
            "+    // Wait for all workers to finish",
            "+    bp.workerWg.Wait()",
            "+    // Wait for result processor to finish",
            "+    bp.resultWg.Wait()",
            "+    close(done)",
            "+  }()",
            "+",
            "+  // Wait for completion or timeout",
            "+  select {",
            "+  case <-done:",
            "+    return nil",
            "+  case <-time.After(timeout):",
            "+    return fmt.Errorf(\"build pipeline shutdown timed out after %v\", timeout)",
            "+  }",
            "+}",
            "+",
            " // Build queues a component for building",
            " func (bp *BuildPipeline) Build(component *types.ComponentInfo) {",
            "   // Check if pipeline is shut down"
          ]
        }
      ],
      "userModified": false,
      "replaceAll": false
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Edit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/scanner.go",
      "old_string": "\treturn skipDirs[name]\n}",
      "new_string": "\treturn skipDirs[name]\n}\n\n// HashingStrategy contains information about the hash generation approach used\ntype HashingStrategy struct {\n\tStrategy     FileHashStrategy\n\tSamplePoints int\n\tHashTime     time.Duration\n\tFileSize     int64\n}\n\n// generateOptimizedHash creates an optimized hash based on file size and content characteristics\nfunc (s *ComponentScanner) generateOptimizedHash(content []byte, fileInfo os.FileInfo) (string, *HashingStrategy) {\n\tstart := time.Now()\n\tfileSize := int64(len(content))\n\t\n\tstrategy := &HashingStrategy{\n\t\tFileSize: fileSize,\n\t}\n\t\n\tvar hash uint32\n\t\n\tswitch {\n\tcase fileSize <= smallFileThreshold:\n\t\t// Small files: use full content CRC32 (fast anyway)\n\t\thash = crc32.Checksum(content, crcTable)\n\t\tstrategy.Strategy = HashStrategyFull\n\t\tstrategy.SamplePoints = 1\n\t\t\n\tcase fileSize <= mediumFileThreshold:\n\t\t// Medium files: use content sampling\n\t\thash = s.generateSampledHash(content)\n\t\tstrategy.Strategy = HashStrategySampled\n\t\tstrategy.SamplePoints = 3\n\t\t\n\tdefault:\n\t\t// Large files: use hierarchical sampling with metadata\n\t\thash = s.generateHierarchicalHash(content, fileInfo)\n\t\tstrategy.Strategy = HashStrategyHierarchical\n\t\tstrategy.SamplePoints = 5\n\t}\n\t\n\tstrategy.HashTime = time.Since(start)\n\t\n\t// Include file metadata in hash to catch size/timestamp changes\n\tmetadataHash := s.generateMetadataHash(fileInfo)\n\tcombinedHash := hash ^ metadataHash\n\t\n\treturn strconv.FormatUint(uint64(combinedHash), 16), strategy\n}\n\n// generateSampledHash creates a hash from strategic content samples\nfunc (s *ComponentScanner) generateSampledHash(content []byte) uint32 {\n\tif len(content) == 0 {\n\t\treturn 0\n\t}\n\t\n\t// Sample three strategic points: beginning, middle, and end\n\tsampleSize := contentSampleSize\n\tif len(content) < sampleSize*3 {\n\t\t// If file is small, just hash it all\n\t\treturn crc32.Checksum(content, crcTable)\n\t}\n\t\n\t// Create a combined sample from key sections\n\tvar samples []byte\n\t\n\t// Beginning sample\n\tif len(content) > sampleSize {\n\t\tsamples = append(samples, content[:sampleSize]...)\n\t}\n\t\n\t// Middle sample\n\tmid := len(content) / 2\n\tmidStart := mid - sampleSize/2\n\tmidEnd := mid + sampleSize/2\n\tif midStart >= 0 && midEnd < len(content) {\n\t\tsamples = append(samples, content[midStart:midEnd]...)\n\t}\n\t\n\t// End sample\n\tif len(content) > sampleSize {\n\t\tsamples = append(samples, content[len(content)-sampleSize:]...)\n\t}\n\t\n\treturn crc32.Checksum(samples, crcTable)\n}\n\n// generateHierarchicalHash creates a hierarchical hash for large files\nfunc (s *ComponentScanner) generateHierarchicalHash(content []byte, fileInfo os.FileInfo) uint32 {\n\tif len(content) == 0 {\n\t\treturn 0\n\t}\n\t\n\t// For templ files, focus on key sections that are likely to change\n\tvar keyContent []byte\n\t\n\t// Add file header (package declaration, imports)\n\tif len(content) > 2048 {\n\t\tkeyContent = append(keyContent, content[:2048]...)\n\t}\n\t\n\t// Sample multiple points throughout the file\n\tchunkSize := len(content) / 8 // Divide into 8 chunks\n\tif chunkSize > contentSampleSize {\n\t\tfor i := 1; i < 8; i++ {\n\t\t\tstart := i * chunkSize\n\t\t\tend := start + contentSampleSize/8\n\t\t\tif end < len(content) {\n\t\t\t\tkeyContent = append(keyContent, content[start:end]...)\n\t\t\t}\n\t\t}\n\t}\n\t\n\t// Add file footer (last part likely to contain component definitions)\n\tif len(content) > 1024 {\n\t\tkeyContent = append(keyContent, content[len(content)-1024:]...)\n\t}\n\t\n\treturn crc32.Checksum(keyContent, crcTable)\n}\n\n// generateMetadataHash creates a hash from file metadata\nfunc (s *ComponentScanner) generateMetadataHash(fileInfo os.FileInfo) uint32 {\n\t// Combine file size and modification time for metadata hash\n\tmetadata := fmt.Sprintf(\"%d:%d\", fileInfo.Size(), fileInfo.ModTime().Unix())\n\treturn crc32.ChecksumIEEE([]byte(metadata))\n}"
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/scanner.go",
      "oldString": "\treturn skipDirs[name]\n}",
      "newString": "\treturn skipDirs[name]\n}\n\n// HashingStrategy contains information about the hash generation approach used\ntype HashingStrategy struct {\n\tStrategy     FileHashStrategy\n\tSamplePoints int\n\tHashTime     time.Duration\n\tFileSize     int64\n}\n\n// generateOptimizedHash creates an optimized hash based on file size and content characteristics\nfunc (s *ComponentScanner) generateOptimizedHash(content []byte, fileInfo os.FileInfo) (string, *HashingStrategy) {\n\tstart := time.Now()\n\tfileSize := int64(len(content))\n\t\n\tstrategy := &HashingStrategy{\n\t\tFileSize: fileSize,\n\t}\n\t\n\tvar hash uint32\n\t\n\tswitch {\n\tcase fileSize <= smallFileThreshold:\n\t\t// Small files: use full content CRC32 (fast anyway)\n\t\thash = crc32.Checksum(content, crcTable)\n\t\tstrategy.Strategy = HashStrategyFull\n\t\tstrategy.SamplePoints = 1\n\t\t\n\tcase fileSize <= mediumFileThreshold:\n\t\t// Medium files: use content sampling\n\t\thash = s.generateSampledHash(content)\n\t\tstrategy.Strategy = HashStrategySampled\n\t\tstrategy.SamplePoints = 3\n\t\t\n\tdefault:\n\t\t// Large files: use hierarchical sampling with metadata\n\t\thash = s.generateHierarchicalHash(content, fileInfo)\n\t\tstrategy.Strategy = HashStrategyHierarchical\n\t\tstrategy.SamplePoints = 5\n\t}\n\t\n\tstrategy.HashTime = time.Since(start)\n\t\n\t// Include file metadata in hash to catch size/timestamp changes\n\tmetadataHash := s.generateMetadataHash(fileInfo)\n\tcombinedHash := hash ^ metadataHash\n\t\n\treturn strconv.FormatUint(uint64(combinedHash), 16), strategy\n}\n\n// generateSampledHash creates a hash from strategic content samples\nfunc (s *ComponentScanner) generateSampledHash(content []byte) uint32 {\n\tif len(content) == 0 {\n\t\treturn 0\n\t}\n\t\n\t// Sample three strategic points: beginning, middle, and end\n\tsampleSize := contentSampleSize\n\tif len(content) < sampleSize*3 {\n\t\t// If file is small, just hash it all\n\t\treturn crc32.Checksum(content, crcTable)\n\t}\n\t\n\t// Create a combined sample from key sections\n\tvar samples []byte\n\t\n\t// Beginning sample\n\tif len(content) > sampleSize {\n\t\tsamples = append(samples, content[:sampleSize]...)\n\t}\n\t\n\t// Middle sample\n\tmid := len(content) / 2\n\tmidStart := mid - sampleSize/2\n\tmidEnd := mid + sampleSize/2\n\tif midStart >= 0 && midEnd < len(content) {\n\t\tsamples = append(samples, content[midStart:midEnd]...)\n\t}\n\t\n\t// End sample\n\tif len(content) > sampleSize {\n\t\tsamples = append(samples, content[len(content)-sampleSize:]...)\n\t}\n\t\n\treturn crc32.Checksum(samples, crcTable)\n}\n\n// generateHierarchicalHash creates a hierarchical hash for large files\nfunc (s *ComponentScanner) generateHierarchicalHash(content []byte, fileInfo os.FileInfo) uint32 {\n\tif len(content) == 0 {\n\t\treturn 0\n\t}\n\t\n\t// For templ files, focus on key sections that are likely to change\n\tvar keyContent []byte\n\t\n\t// Add file header (package declaration, imports)\n\tif len(content) > 2048 {\n\t\tkeyContent = append(keyContent, content[:2048]...)\n\t}\n\t\n\t// Sample multiple points throughout the file\n\tchunkSize := len(content) / 8 // Divide into 8 chunks\n\tif chunkSize > contentSampleSize {\n\t\tfor i := 1; i < 8; i++ {\n\t\t\tstart := i * chunkSize\n\t\t\tend := start + contentSampleSize/8\n\t\t\tif end < len(content) {\n\t\t\t\tkeyContent = append(keyContent, content[start:end]...)\n\t\t\t}\n\t\t}\n\t}\n\t\n\t// Add file footer (last part likely to contain component definitions)\n\tif len(content) > 1024 {\n\t\tkeyContent = append(keyContent, content[len(content)-1024:]...)\n\t}\n\t\n\treturn crc32.Checksum(keyContent, crcTable)\n}\n\n// generateMetadataHash creates a hash from file metadata\nfunc (s *ComponentScanner) generateMetadataHash(fileInfo os.FileInfo) uint32 {\n\t// Combine file size and modification time for metadata hash\n\tmetadata := fmt.Sprintf(\"%d:%d\", fileInfo.Size(), fileInfo.ModTime().Unix())\n\treturn crc32.ChecksumIEEE([]byte(metadata))\n}",
      "originalFile": "// Package scanner provides component discovery and analysis for templ templates.\n//\n// The scanner traverses file systems to find .templ files, parses them using\n// Go's AST parser to extract component metadata including parameters, dependencies,\n// and documentation. It integrates with the component registry to broadcast\n// change events and supports recursive directory scanning with exclude patterns.\n// The scanner maintains file hashes for change detection and provides both\n// single-file and batch scanning capabilities.\npackage scanner\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"go/ast\"\n\t\"go/parser\"\n\t\"go/token\"\n\t\"hash/crc32\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/conneroisu/templar/internal/errors\"\n\t\"github.com/conneroisu/templar/internal/interfaces\"\n\t\"github.com/conneroisu/templar/internal/registry\"\n\t\"github.com/conneroisu/templar/internal/types\"\n)\n\n// crcTable is a pre-computed CRC32 Castagnoli table for faster hash generation\nvar crcTable = crc32.MakeTable(crc32.Castagnoli)\n\n// Hash generation strategy constants\nconst (\n\t// Small files (< 4KB) - use full content CRC32\n\tsmallFileThreshold = 4 * 1024\n\t// Medium files (4KB - 256KB) - use content sampling\n\tmediumFileThreshold = 256 * 1024\n\t// Large files (> 256KB) - use hierarchical sampling\n\tlargeFileThreshold = 256 * 1024\n\t// Content sample size for large files\n\tcontentSampleSize = 1024\n)\n\n// FileHashStrategy represents different hashing approaches\ntype FileHashStrategy int\n\nconst (\n\tHashStrategyFull FileHashStrategy = iota\n\tHashStrategySampled\n\tHashStrategyHierarchical\n)\n\n// ScanJob represents a scanning job for the worker pool containing the file\n// path to scan and a result channel for asynchronous communication.\ntype ScanJob struct {\n\t// filePath is the absolute path to the .templ file to be scanned\n\tfilePath string\n\t// result channel receives the scan result or error asynchronously\n\tresult chan<- ScanResult\n}\n\n// HashResult represents the result of asynchronous hash calculation\ntype HashResult struct {\n\thash string\n\terr  error\n}\n\n// BufferPool manages reusable byte buffers for file reading optimization\ntype BufferPool struct {\n\tpool sync.Pool\n}\n\n// NewBufferPool creates a new buffer pool with initial buffer size\nfunc NewBufferPool() *BufferPool {\n\treturn &BufferPool{\n\t\tpool: sync.Pool{\n\t\t\tNew: func() interface{} {\n\t\t\t\t// Pre-allocate 64KB buffers for typical component files\n\t\t\t\treturn make([]byte, 0, 64*1024)\n\t\t\t},\n\t\t},\n\t}\n}\n\n// Get retrieves a buffer from the pool\nfunc (bp *BufferPool) Get() []byte {\n\treturn bp.pool.Get().([]byte)[:0] // Reset length but keep capacity\n}\n\n// Put returns a buffer to the pool\nfunc (bp *BufferPool) Put(buf []byte) {\n\t// Only pool reasonably-sized buffers to avoid memory leaks\n\tif cap(buf) <= 1024*1024 { // 1MB limit\n\t\tbp.pool.Put(buf)\n\t}\n}\n\n// ScanResult represents the result of a scanning operation, containing either\n// success status or error information for a specific file.\ntype ScanResult struct {\n\t// filePath is the path that was scanned\n\tfilePath string\n\t// err contains any error that occurred during scanning, nil on success\n\terr error\n}\n\n// WorkerPool manages persistent scanning workers for performance optimization\n// using a work-stealing approach to distribute scanning jobs across CPU cores.\ntype WorkerPool struct {\n\t// jobQueue buffers scanning jobs for worker distribution\n\tjobQueue chan ScanJob\n\t// workers holds references to all active worker goroutines\n\tworkers []*ScanWorker\n\t// workerCount defines the number of concurrent workers (typically NumCPU)\n\tworkerCount int\n\t// scanner is the shared component scanner instance\n\tscanner *ComponentScanner\n\t// stop signals all workers to terminate gracefully\n\tstop chan struct{}\n\t// stopped tracks pool shutdown state\n\tstopped bool\n\t// mu protects concurrent access to pool state\n\tmu sync.RWMutex\n}\n\n// ScanWorker represents a persistent worker goroutine that processes scanning\n// jobs from the shared job queue. Each worker operates independently and\n// can handle multiple file types concurrently.\ntype ScanWorker struct {\n\t// id uniquely identifies this worker for debugging and metrics\n\tid int\n\t// jobQueue receives scanning jobs from the worker pool\n\tjobQueue <-chan ScanJob\n\t// scanner provides the component parsing functionality\n\tscanner *ComponentScanner\n\t// stop signals this worker to terminate gracefully\n\tstop chan struct{}\n}\n\n// ComponentScanner discovers and parses templ components using Go's AST parser.\n//\n// The scanner provides:\n// - Recursive directory traversal with exclude patterns\n// - AST-based component metadata extraction\n// - Concurrent processing via worker pool\n// - Integration with component registry for event broadcasting\n// - File change detection using CRC32 hashing\n// - Optimized path validation with cached working directory\n// - Buffer pooling for memory optimization in large codebases\n// - Component metadata caching with LRU eviction for performance\ntype ComponentScanner struct {\n\t// registry receives discovered components and broadcasts change events\n\tregistry *registry.ComponentRegistry\n\t// fileSet tracks file positions for AST parsing and error reporting\n\tfileSet *token.FileSet\n\t// workerPool manages concurrent scanning operations\n\tworkerPool *WorkerPool\n\t// pathCache contains cached path validation data to avoid repeated syscalls\n\tpathCache *pathValidationCache\n\t// bufferPool provides reusable byte buffers for file reading optimization\n\tbufferPool *BufferPool\n\t// metadataCache caches parsed component metadata by file hash to avoid re-parsing unchanged files\n\tmetadataCache *MetadataCache\n\t// astParsingPool provides concurrent AST parsing to avoid blocking worker threads\n\tastParsingPool *ASTParsingPool\n\t// metrics tracks performance metrics during scanning operations\n\tmetrics *ScannerMetrics\n\t// config provides timeout configuration for scanning operations\n\tconfig *config.Config\n}\n\n// Interface compliance verification - ComponentScanner implements interfaces.ComponentScanner\nvar _ interfaces.ComponentScanner = (*ComponentScanner)(nil)\n\n// ASTParseJob represents a parsing job for the AST parsing pool\ntype ASTParseJob struct {\n\tfilePath string\n\tcontent  []byte\n\tfileSet  *token.FileSet\n\tresult   chan<- ASTParseResult\n}\n\n// ASTParseResult contains the result of AST parsing\ntype ASTParseResult struct {\n\tastFile *ast.File\n\terr     error\n\tfilePath string\n}\n\n// ASTParsingPool manages concurrent AST parsing to avoid blocking worker threads\ntype ASTParsingPool struct {\n\tworkers   int\n\tjobChan   chan ASTParseJob\n\tcloseChan chan struct{}\n\twg        sync.WaitGroup\n}\n\n// NewASTParsingPool creates a new AST parsing pool with specified worker count\nfunc NewASTParsingPool(workers int) *ASTParsingPool {\n\tif workers <= 0 {\n\t\tworkers = runtime.NumCPU() / 2 // Use half CPU cores for AST parsing\n\t\tif workers < 1 {\n\t\t\tworkers = 1\n\t\t}\n\t}\n\t\n\tpool := &ASTParsingPool{\n\t\tworkers:   workers,\n\t\tjobChan:   make(chan ASTParseJob, workers*2),\n\t\tcloseChan: make(chan struct{}),\n\t}\n\t\n\t// Start worker goroutines\n\tfor i := 0; i < workers; i++ {\n\t\tpool.wg.Add(1)\n\t\tgo pool.worker()\n\t}\n\t\n\treturn pool\n}\n\n// worker processes AST parsing jobs\nfunc (p *ASTParsingPool) worker() {\n\tdefer p.wg.Done()\n\t\n\tfor {\n\t\tselect {\n\t\tcase job := <-p.jobChan:\n\t\t\t// Parse the AST\n\t\t\tastFile, err := parser.ParseFile(job.fileSet, job.filePath, job.content, parser.ParseComments)\n\t\t\t\n\t\t\t// Send result back\n\t\t\tselect {\n\t\t\tcase job.result <- ASTParseResult{\n\t\t\t\tastFile: astFile,\n\t\t\t\terr:     err,\n\t\t\t\tfilePath: job.filePath,\n\t\t\t}:\n\t\t\tcase <-p.closeChan:\n\t\t\t\treturn\n\t\t\t}\n\t\t\t\n\t\tcase <-p.closeChan:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// ParseAsync submits an AST parsing job and returns a result channel\nfunc (p *ASTParsingPool) ParseAsync(filePath string, content []byte, fileSet *token.FileSet) <-chan ASTParseResult {\n\tresult := make(chan ASTParseResult, 1)\n\t\n\t// For very large files, use optimized parsing approach\n\tif len(content) > 1024*1024 { // 1MB threshold\n\t\tgo p.parseLargeFileAsync(filePath, content, fileSet, result)\n\t\treturn result\n\t}\n\t\n\tselect {\n\tcase p.jobChan <- ASTParseJob{\n\t\tfilePath: filePath,\n\t\tcontent:  content,\n\t\tfileSet:  fileSet,\n\t\tresult:   result,\n\t}:\n\t\treturn result\n\tcase <-p.closeChan:\n\t\t// Pool is closed, return error result\n\t\tgo func() {\n\t\t\tresult <- ASTParseResult{\n\t\t\t\tastFile: nil,\n\t\t\t\terr:     fmt.Errorf(\"AST parsing pool is closed\"),\n\t\t\t\tfilePath: filePath,\n\t\t\t}\n\t\t}()\n\t\treturn result\n\t}\n}\n\n// parseLargeFileAsync handles large file parsing with memory optimization\nfunc (p *ASTParsingPool) parseLargeFileAsync(filePath string, content []byte, fileSet *token.FileSet, result chan<- ASTParseResult) {\n\tdefer close(result)\n\t\n\t// For large files, use streaming approach with limited memory usage\n\t// Parse with limited goroutines to prevent memory exhaustion\n\tastFile, err := parser.ParseFile(fileSet, filePath, content, parser.ParseComments|parser.SkipObjectResolution)\n\t\n\tresult <- ASTParseResult{\n\t\tastFile: astFile,\n\t\terr:     err,\n\t\tfilePath: filePath,\n\t}\n}\n\n// Close shuts down the AST parsing pool\nfunc (p *ASTParsingPool) Close() {\n\tclose(p.closeChan)\n\tclose(p.jobChan)\n\tp.wg.Wait()\n}\n\n// pathValidationCache caches expensive filesystem operations for optimal performance\ntype pathValidationCache struct {\n\t// mu protects concurrent access to cache fields\n\tmu sync.RWMutex\n\t// currentWorkingDir is the cached current working directory (absolute path)\n\tcurrentWorkingDir string\n\t// initialized indicates whether the cache has been populated\n\tinitialized bool\n}\n\n// CachedComponentMetadata stores pre-parsed component information for cache optimization\ntype CachedComponentMetadata struct {\n\t// Components is a slice of all components found in the file\n\tComponents []*types.ComponentInfo\n\t// FileHash is the CRC32 hash of the file content when cached\n\tFileHash string\n\t// ParsedAt records when the metadata was cached\n\tParsedAt time.Time\n}\n\n// ScannerMetrics tracks performance metrics during scanning operations\ntype ScannerMetrics struct {\n\t// FilesProcessed is the total number of files processed\n\tFilesProcessed int64\n\t// ComponentsFound is the total number of components discovered\n\tComponentsFound int64\n\t// CacheHits tracks how many files were served from cache\n\tCacheHits int64\n\t// CacheMisses tracks how many files required parsing\n\tCacheMisses int64\n\t// TotalScanTime tracks time spent in scanning operations\n\tTotalScanTime time.Duration\n\t// PeakMemoryUsage tracks the peak memory usage during scanning\n\tPeakMemoryUsage uint64\n\t// ConcurrentJobs tracks the peak number of concurrent jobs\n\tConcurrentJobs int64\n}\n\n// MetadataCache implements a simple LRU cache for component metadata\ntype MetadataCache struct {\n\tmu       sync.RWMutex\n\tentries  map[string]*MetadataCacheEntry\n\tmaxSize  int\n\tttl      time.Duration\n\t// LRU doubly-linked list\n\thead *MetadataCacheEntry\n\ttail *MetadataCacheEntry\n}\n\n// MetadataCacheEntry represents a cached metadata entry with LRU pointers\ntype MetadataCacheEntry struct {\n\tKey       string\n\tData      []byte\n\tCreatedAt time.Time\n\t// LRU pointers\n\tprev *MetadataCacheEntry\n\tnext *MetadataCacheEntry\n}\n\n// NewMetadataCache creates a new metadata cache\nfunc NewMetadataCache(maxSize int, ttl time.Duration) *MetadataCache {\n\tcache := &MetadataCache{\n\t\tentries: make(map[string]*MetadataCacheEntry),\n\t\tmaxSize: maxSize,\n\t\tttl:     ttl,\n\t}\n\t\n\t// Initialize dummy head and tail for LRU\n\tcache.head = &MetadataCacheEntry{}\n\tcache.tail = &MetadataCacheEntry{}\n\tcache.head.next = cache.tail\n\tcache.tail.prev = cache.head\n\t\n\treturn cache\n}\n\n// Get retrieves data from cache\nfunc (mc *MetadataCache) Get(key string) ([]byte, bool) {\n\tmc.mu.Lock()\n\tdefer mc.mu.Unlock()\n\t\n\tentry, exists := mc.entries[key]\n\tif !exists {\n\t\treturn nil, false\n\t}\n\t\n\t// Check TTL\n\tif time.Since(entry.CreatedAt) > mc.ttl {\n\t\tmc.removeFromList(entry)\n\t\tdelete(mc.entries, key)\n\t\treturn nil, false\n\t}\n\t\n\t// Move to front (most recently used)\n\tmc.moveToFront(entry)\n\treturn entry.Data, true\n}\n\n// Set stores data in cache\nfunc (mc *MetadataCache) Set(key string, data []byte) {\n\tmc.mu.Lock()\n\tdefer mc.mu.Unlock()\n\t\n\t// Check if entry exists\n\tif existingEntry, exists := mc.entries[key]; exists {\n\t\texistingEntry.Data = data\n\t\texistingEntry.CreatedAt = time.Now()\n\t\tmc.moveToFront(existingEntry)\n\t\treturn\n\t}\n\t\n\t// Evict if needed\n\tif len(mc.entries) >= mc.maxSize {\n\t\tmc.evictLRU()\n\t}\n\t\n\t// Create new entry\n\tentry := &MetadataCacheEntry{\n\t\tKey:       key,\n\t\tData:      data,\n\t\tCreatedAt: time.Now(),\n\t}\n\t\n\tmc.entries[key] = entry\n\tmc.addToFront(entry)\n}\n\n// Clear removes all entries\nfunc (mc *MetadataCache) Clear() {\n\tmc.mu.Lock()\n\tdefer mc.mu.Unlock()\n\t\n\tmc.entries = make(map[string]*MetadataCacheEntry)\n\tmc.head.next = mc.tail\n\tmc.tail.prev = mc.head\n}\n\n// LRU operations\nfunc (mc *MetadataCache) addToFront(entry *MetadataCacheEntry) {\n\tentry.prev = mc.head\n\tentry.next = mc.head.next\n\tmc.head.next.prev = entry\n\tmc.head.next = entry\n}\n\nfunc (mc *MetadataCache) removeFromList(entry *MetadataCacheEntry) {\n\tentry.prev.next = entry.next\n\tentry.next.prev = entry.prev\n}\n\nfunc (mc *MetadataCache) moveToFront(entry *MetadataCacheEntry) {\n\tmc.removeFromList(entry)\n\tmc.addToFront(entry)\n}\n\nfunc (mc *MetadataCache) evictLRU() {\n\tif mc.tail.prev != mc.head {\n\t\tlru := mc.tail.prev\n\t\tmc.removeFromList(lru)\n\t\tdelete(mc.entries, lru.Key)\n\t}\n}\n\n// NewComponentScanner creates a new component scanner with optimal worker pool\nfunc NewComponentScanner(registry *registry.ComponentRegistry, cfg ...*config.Config) *ComponentScanner {\n\treturn NewComponentScannerWithConcurrency(registry, 0, cfg...) // 0 = auto-detect optimal\n}\n\n// NewComponentScannerWithConcurrency creates a new component scanner with configurable concurrency\nfunc NewComponentScannerWithConcurrency(registry *registry.ComponentRegistry, maxWorkers int, cfg ...*config.Config) *ComponentScanner {\n\tscanner := &ComponentScanner{\n\t\tregistry:   registry,\n\t\tfileSet:    token.NewFileSet(),\n\t\tpathCache:  &pathValidationCache{},\n\t\tbufferPool: NewBufferPool(),\n\t\t// Initialize metadata cache: 1000 entries max, 1 hour TTL\n\t\t// This caches ~1000-2000 component metadata entries typically\n\t\tmetadataCache: NewMetadataCache(1000, time.Hour),\n\t\t// Initialize performance metrics tracking\n\t\tmetrics: &ScannerMetrics{},\n\t}\n\n\t// Initialize worker pool with configurable or optimal worker count\n\tworkerCount := maxWorkers\n\tif workerCount <= 0 {\n\t\t// Auto-detect optimal worker count\n\t\tworkerCount = runtime.NumCPU()\n\t\tif workerCount > 8 {\n\t\t\tworkerCount = 8 // Cap at 8 workers for diminishing returns\n\t\t}\n\t} else {\n\t\t// User-specified count, but enforce reasonable limits\n\t\tif workerCount > 64 {\n\t\t\tworkerCount = 64 // Maximum safety limit\n\t\t}\n\t}\n\n\tscanner.workerPool = NewWorkerPool(workerCount, scanner)\n\t\n\t// Initialize AST parsing pool with fewer workers to avoid oversubscription\n\tastWorkerCount := workerCount / 2\n\tif astWorkerCount < 1 {\n\t\tastWorkerCount = 1\n\t}\n\tscanner.astParsingPool = NewASTParsingPool(astWorkerCount)\n\n\t// Use first config if provided, otherwise nil\n\tif len(cfg) > 0 {\n\t\tscanner.config = cfg[0]\n\t}\n\t\n\treturn scanner\n}\n\n// getFileScanTimeout returns the configured timeout for file scanning operations\nfunc (s *ComponentScanner) getFileScanTimeout() time.Duration {\n\tif s.config != nil && s.config.Timeouts.FileScan > 0 {\n\t\treturn s.config.Timeouts.FileScan\n\t}\n\t// Default fallback timeout if no configuration is available\n\treturn 30 * time.Second\n}\n\n// NewWorkerPool creates a new worker pool for scanning operations\nfunc NewWorkerPool(workerCount int, scanner *ComponentScanner) *WorkerPool {\n\tpool := &WorkerPool{\n\t\tjobQueue:    make(chan ScanJob, workerCount*2), // Buffer for work-stealing efficiency\n\t\tworkerCount: workerCount,\n\t\tscanner:     scanner,\n\t\tstop:        make(chan struct{}),\n\t}\n\n\t// Start persistent workers\n\tpool.workers = make([]*ScanWorker, workerCount)\n\tfor i := 0; i < workerCount; i++ {\n\t\tworker := &ScanWorker{\n\t\t\tid:       i,\n\t\t\tjobQueue: pool.jobQueue,\n\t\t\tscanner:  scanner,\n\t\t\tstop:     make(chan struct{}),\n\t\t}\n\t\tpool.workers[i] = worker\n\t\tgo worker.start()\n\t}\n\n\treturn pool\n}\n\n// start begins the worker's processing loop\nfunc (w *ScanWorker) start() {\n\tfor {\n\t\tselect {\n\t\tcase job := <-w.jobQueue:\n\t\t\t// Process the scanning job\n\t\t\terr := w.scanner.scanFileInternal(job.filePath)\n\t\t\tjob.result <- ScanResult{\n\t\t\t\tfilePath: job.filePath,\n\t\t\t\terr:      err,\n\t\t\t}\n\t\tcase <-w.stop:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Stop gracefully shuts down the worker pool\nfunc (p *WorkerPool) Stop() {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\n\tif p.stopped {\n\t\treturn\n\t}\n\n\tp.stopped = true\n\tclose(p.stop)\n\n\t// Stop all workers\n\tfor _, worker := range p.workers {\n\t\tclose(worker.stop)\n\t}\n\n\t// Close job queue\n\tclose(p.jobQueue)\n}\n\n// GetRegistry returns the component registry\nfunc (s *ComponentScanner) GetRegistry() interfaces.ComponentRegistry {\n\treturn s.registry\n}\n\n// GetWorkerCount returns the number of active workers in the pool\nfunc (s *ComponentScanner) GetWorkerCount() int {\n\tif s.workerPool == nil {\n\t\treturn 0\n\t}\n\ts.workerPool.mu.RLock()\n\tdefer s.workerPool.mu.RUnlock()\n\treturn s.workerPool.workerCount\n}\n\n// GetMetrics returns a copy of the current scanner metrics\nfunc (s *ComponentScanner) GetMetrics() ScannerMetrics {\n\tif s.metrics == nil {\n\t\treturn ScannerMetrics{}\n\t}\n\treturn ScannerMetrics{\n\t\tFilesProcessed:  atomic.LoadInt64(&s.metrics.FilesProcessed),\n\t\tComponentsFound: atomic.LoadInt64(&s.metrics.ComponentsFound),\n\t\tCacheHits:       atomic.LoadInt64(&s.metrics.CacheHits),\n\t\tCacheMisses:     atomic.LoadInt64(&s.metrics.CacheMisses),\n\t\tTotalScanTime:   s.metrics.TotalScanTime,\n\t\tPeakMemoryUsage: atomic.LoadUint64(&s.metrics.PeakMemoryUsage),\n\t\tConcurrentJobs:  atomic.LoadInt64(&s.metrics.ConcurrentJobs),\n\t}\n}\n\n// ResetMetrics clears all scanner metrics\nfunc (s *ComponentScanner) ResetMetrics() {\n\tif s.metrics == nil {\n\t\treturn\n\t}\n\tatomic.StoreInt64(&s.metrics.FilesProcessed, 0)\n\tatomic.StoreInt64(&s.metrics.ComponentsFound, 0)\n\tatomic.StoreInt64(&s.metrics.CacheHits, 0)\n\tatomic.StoreInt64(&s.metrics.CacheMisses, 0)\n\tatomic.StoreUint64(&s.metrics.PeakMemoryUsage, 0)\n\tatomic.StoreInt64(&s.metrics.ConcurrentJobs, 0)\n\ts.metrics.TotalScanTime = 0\n}\n\n// Close gracefully shuts down the scanner and its worker pool\nfunc (s *ComponentScanner) Close() error {\n\tif s.astParsingPool != nil {\n\t\ts.astParsingPool.Close()\n\t}\n\tif s.workerPool != nil {\n\t\ts.workerPool.Stop()\n\t}\n\tif s.metadataCache != nil {\n\t\ts.metadataCache.Clear()\n\t}\n\treturn nil\n}\n\n// getCachedMetadata attempts to retrieve cached component metadata for a file\nfunc (s *ComponentScanner) getCachedMetadata(filePath, fileHash string) (*CachedComponentMetadata, bool) {\n\tif s.metadataCache == nil {\n\t\treturn nil, false\n\t}\n\n\tcacheKey := fmt.Sprintf(\"%s:%s\", filePath, fileHash)\n\tcachedData, found := s.metadataCache.Get(cacheKey)\n\tif !found {\n\t\treturn nil, false\n\t}\n\n\tvar metadata CachedComponentMetadata\n\tif err := json.Unmarshal(cachedData, &metadata); err != nil {\n\t\t// Cache corruption - remove invalid entry\n\t\ts.metadataCache.Set(cacheKey, nil)\n\t\treturn nil, false\n\t}\n\n\t// Verify the cached hash matches current file hash (additional safety check)\n\tif metadata.FileHash != fileHash {\n\t\treturn nil, false\n\t}\n\n\treturn &metadata, true\n}\n\n// setCachedMetadata stores component metadata in the cache\nfunc (s *ComponentScanner) setCachedMetadata(filePath, fileHash string, components []*types.ComponentInfo) {\n\tif s.metadataCache == nil {\n\t\treturn\n\t}\n\n\tmetadata := CachedComponentMetadata{\n\t\tComponents: components,\n\t\tFileHash:   fileHash,\n\t\tParsedAt:   time.Now(),\n\t}\n\n\tdata, err := json.Marshal(metadata)\n\tif err != nil {\n\t\t// Skip caching if marshaling fails\n\t\treturn\n\t}\n\n\tcacheKey := fmt.Sprintf(\"%s:%s\", filePath, fileHash)\n\ts.metadataCache.Set(cacheKey, data)\n}\n\n// ScanDirectory scans a directory for templ components using optimized worker pool with timeout support\nfunc (s *ComponentScanner) ScanDirectoryWithContext(ctx context.Context, dir string) error {\n\tstart := time.Now()\n\t\n\t// Track memory usage at start\n\tvar startMem runtime.MemStats\n\truntime.ReadMemStats(&startMem)\n\t\n\t// Validate directory path to prevent path traversal\n\tif _, err := s.validatePath(dir); err != nil {\n\t\treturn errors.WrapValidation(err, errors.ErrCodeInvalidPath, \n\t\t\t\"directory path validation failed\").\n\t\t\tWithContext(\"directory\", dir)\n\t}\n\n\t// Use concurrent directory walking for better performance on large codebases\n\tfiles, err := s.walkDirectoryConcurrent(dir)\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Process files using persistent worker pool with context (no goroutine creation overhead)\n\terr = s.processBatchWithWorkerPoolWithContext(ctx, files)\n\t\n\t// Update metrics\n\tif s.metrics != nil {\n\t\telapsed := time.Since(start)\n\t\ts.metrics.TotalScanTime += elapsed\n\t\tatomic.AddInt64(&s.metrics.FilesProcessed, int64(len(files)))\n\t\t\n\t\t// Track memory usage\n\t\tvar endMem runtime.MemStats\n\t\truntime.ReadMemStats(&endMem)\n\t\tmemUsed := endMem.Alloc - startMem.Alloc\n\t\t\n\t\t// Update peak memory if this scan used more\n\t\tfor {\n\t\t\tcurrent := atomic.LoadUint64(&s.metrics.PeakMemoryUsage)\n\t\t\tif memUsed <= current || atomic.CompareAndSwapUint64(&s.metrics.PeakMemoryUsage, current, memUsed) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\t\n\treturn err\n}\n\n// ScanDirectory scans a directory for templ components (backward compatible wrapper)\nfunc (s *ComponentScanner) ScanDirectory(dir string) error {\n\t// Create a timeout context for the scan operation\n\tscanTimeout := s.getFileScanTimeout()\n\tctx, cancel := context.WithTimeout(context.Background(), scanTimeout)\n\tdefer cancel()\n\t\n\treturn s.ScanDirectoryWithContext(ctx, dir)\n}\n\n// processBatchWithWorkerPoolWithContext processes files using the persistent worker pool with optimized batching and context support\nfunc (s *ComponentScanner) processBatchWithWorkerPoolWithContext(ctx context.Context, files []string) error {\n\tif len(files) == 0 {\n\t\treturn nil\n\t}\n\n\t// For very small batches, process synchronously to avoid overhead\n\tif len(files) <= 5 {\n\t\treturn s.processBatchSynchronous(files)\n\t}\n\n\t// Create result channel for collecting results\n\tresultChan := make(chan ScanResult, len(files))\n\tsubmitted := 0\n\t\n\t// Submit jobs to persistent worker pool\n\tfor _, file := range files {\n\t\t// Check if context is cancelled\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tdefault:\n\t\t}\n\n\t\tjob := ScanJob{\n\t\t\tfilePath: file,\n\t\t\tresult:   resultChan,\n\t\t}\n\n\t\tselect {\n\t\tcase s.workerPool.jobQueue <- job:\n\t\t\t// Job submitted successfully\n\t\t\tsubmitted++\n\t\tcase <-ctx.Done():\n\t\t\t// Context cancelled while submitting\n\t\t\treturn ctx.Err()\n\t\tdefault:\n\t\t\t// Worker pool is full, process synchronously as fallback\n\t\t\terr := s.scanFileInternal(file)\n\t\t\tresultChan <- ScanResult{filePath: file, err: err}\n\t\t}\n\t}\n\n\t// Collect results with context checking\n\tvar scanErrors []error\n\tfor i := 0; i < len(files); i++ {\n\t\tselect {\n\t\tcase result := <-resultChan:\n\t\t\tif result.err != nil {\n\t\t\t\t// Enhance the error with file context\n\t\t\t\tenhancedErr := errors.EnhanceError(result.err, \"scanner\", result.filePath, 0, 0)\n\t\t\t\tscanErrors = append(scanErrors, enhancedErr)\n\t\t\t}\n\t\tcase <-ctx.Done():\n\t\t\t// Context cancelled while collecting results\n\t\t\treturn ctx.Err()\n\t\t}\n\t}\n\n\tclose(resultChan)\n\n\tif len(scanErrors) > 0 {\n\t\treturn errors.CombineErrors(scanErrors...)\n\t}\n\n\treturn nil\n}\n\n// processBatchWithWorkerPool processes files using the persistent worker pool (backward compatible wrapper)\nfunc (s *ComponentScanner) processBatchWithWorkerPool(files []string) error {\n\t// Use background context with timeout for backward compatibility\n\tscanTimeout := s.getFileScanTimeout()\n\tctx, cancel := context.WithTimeout(context.Background(), scanTimeout)\n\tdefer cancel()\n\t\n\treturn s.processBatchWithWorkerPoolWithContext(ctx, files)\n}\n\n// processBatchSynchronous processes small batches synchronously for better performance\nfunc (s *ComponentScanner) processBatchSynchronous(files []string) error {\n\tvar scanErrors []error\n\t\n\tfor _, file := range files {\n\t\tif err := s.scanFileInternal(file); err != nil {\n\t\t\tenhancedErr := errors.EnhanceError(err, \"scanner\", file, 0, 0)\n\t\t\tscanErrors = append(scanErrors, enhancedErr)\n\t\t}\n\t}\n\n\tif len(scanErrors) > 0 {\n\t\treturn errors.CombineErrors(scanErrors...)\n\t}\n\n\treturn nil\n}\n\n// ScanDirectoryParallel is deprecated in favor of the optimized ScanDirectory\n// Kept for backward compatibility\nfunc (s *ComponentScanner) ScanDirectoryParallel(dir string, workers int) error {\n\treturn s.ScanDirectory(dir) // Use optimized version\n}\n\n// ScanFile scans a single file for templ components (optimized)\nfunc (s *ComponentScanner) ScanFile(path string) error {\n\treturn s.scanFileInternal(path)\n}\n\n// scanFileInternal is the optimized internal scanning method used by workers\nfunc (s *ComponentScanner) scanFileInternal(path string) error {\n\t// Validate and clean the path to prevent directory traversal\n\tcleanPath, err := s.validatePath(path)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"invalid path: %w\", err)\n\t}\n\n\t// Optimized single I/O operation: open file and get both content and info\n\tfile, err := os.Open(cleanPath)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"opening file %s: %w\", cleanPath, err)\n\t}\n\tdefer file.Close()\n\n\t// Get file info without separate Stat() call\n\tinfo, err := file.Stat()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"getting file info for %s: %w\", cleanPath, err)\n\t}\n\n\t// Get buffer from pool for optimized memory usage\n\tbuffer := s.bufferPool.Get()\n\tdefer s.bufferPool.Put(buffer)\n\n\t// Read content efficiently using buffer pool\n\tvar content []byte\n\tif info.Size() > 64*1024 {\n\t\t// Use streaming read for large files to reduce memory pressure\n\t\tcontent, err = s.readFileStreamingOptimized(file, info.Size(), buffer)\n\t} else {\n\t\t// Use pooled buffer for small files\n\t\tif cap(buffer) < int(info.Size()) {\n\t\t\tbuffer = make([]byte, info.Size())\n\t\t}\n\t\tbuffer = buffer[:info.Size()]\n\t\t_, err = file.Read(buffer)\n\t\tif err == nil {\n\t\t\tcontent = make([]byte, len(buffer))\n\t\t\tcopy(content, buffer)\n\t\t}\n\t}\n\n\tif err != nil {\n\t\treturn fmt.Errorf(\"reading file %s: %w\", cleanPath, err)\n\t}\n\n\t// Calculate file hash for cache lookup and change detection\n\thash := strconv.FormatUint(uint64(crc32.Checksum(content, crcTable)), 16)\n\t\n\t// Check cache first - avoid expensive parsing if metadata is cached\n\tif cachedMetadata, found := s.getCachedMetadata(cleanPath, hash); found {\n\t\t// Track cache hit\n\t\tif s.metrics != nil {\n\t\t\tatomic.AddInt64(&s.metrics.CacheHits, 1)\n\t\t}\n\t\t\n\t\t// Register all cached components with the registry\n\t\tfor _, component := range cachedMetadata.Components {\n\t\t\t// Update file modification time to current scan time\n\t\t\tupdatedComponent := *component\n\t\t\tupdatedComponent.LastMod = info.ModTime()\n\t\t\tupdatedComponent.Hash = hash\n\t\t\ts.registry.Register(&updatedComponent)\n\t\t}\n\t\t\n\t\t// Track components found\n\t\tif s.metrics != nil {\n\t\t\tatomic.AddInt64(&s.metrics.ComponentsFound, int64(len(cachedMetadata.Components)))\n\t\t}\n\t\t\n\t\treturn nil\n\t}\n\n\t// Track cache miss\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.CacheMisses, 1)\n\t}\n\n\t// Cache miss - perform parsing with async AST parsing to avoid blocking worker threads\n\tvar components []*types.ComponentInfo\n\t\n\t// Use async AST parsing to avoid blocking the worker thread\n\tastResultChan := s.astParsingPool.ParseAsync(cleanPath, content, s.fileSet)\n\t\n\t// Wait for AST parsing result (non-blocking for the worker thread)\n\tastResult := <-astResultChan\n\t\n\tif astResult.err != nil {\n\t\t// If AST parsing fails, try manual component extraction for .templ files\n\t\tcomponents, err = s.parseTemplFileWithComponents(cleanPath, content, hash, info.ModTime())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\t// Extract components from AST\n\t\tcomponents, err = s.extractFromASTWithComponents(cleanPath, astResult.astFile, hash, info.ModTime())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Cache the parsed components for future scans\n\ts.setCachedMetadata(cleanPath, hash, components)\n\n\t// Register all components with the registry\n\tfor _, component := range components {\n\t\ts.registry.Register(component)\n\t}\n\t\n\t// Track components found\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.ComponentsFound, int64(len(components)))\n\t}\n\n\treturn nil\n}\n\n// readFileStreaming removed - replaced by readFileStreamingOptimized\n\n// readFileStreamingOptimized reads large files using pooled buffers for better memory efficiency\nfunc (s *ComponentScanner) readFileStreamingOptimized(file *os.File, size int64, pooledBuffer []byte) ([]byte, error) {\n\tconst chunkSize = 32 * 1024 // 32KB chunks\n\t\n\t// Use a reasonably-sized chunk buffer for reading\n\tvar chunk []byte\n\tif cap(pooledBuffer) >= chunkSize {\n\t\tchunk = pooledBuffer[:chunkSize]\n\t} else {\n\t\tchunk = make([]byte, chunkSize)\n\t}\n\n\t// Pre-allocate content buffer with exact size to avoid reallocations\n\tcontent := make([]byte, 0, size)\n\n\tfor {\n\t\tn, err := file.Read(chunk)\n\t\tif n > 0 {\n\t\t\tcontent = append(content, chunk[:n]...)\n\t\t}\n\t\tif err != nil {\n\t\t\tif err == io.EOF {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t\tif n < chunkSize {\n\t\t\tbreak\n\t\t}\n\t}\n\n\treturn content, nil\n}\n\n// Backward compatibility method removed - unused\n\n// parseTemplFileWithComponents extracts components from templ files and returns them\nfunc (s *ComponentScanner) parseTemplFileWithComponents(path string, content []byte, hash string, modTime time.Time) ([]*types.ComponentInfo, error) {\n\tvar components []*types.ComponentInfo\n\tlines := strings.Split(string(content), \"\\n\")\n\tpackageName := \"\"\n\n\tfor _, line := range lines {\n\t\tline = strings.TrimSpace(line)\n\n\t\t// Extract package name\n\t\tif strings.HasPrefix(line, \"package \") {\n\t\t\tparts := strings.Fields(line)\n\t\t\tif len(parts) >= 2 {\n\t\t\t\tpackageName = sanitizeIdentifier(parts[1])\n\t\t\t}\n\t\t}\n\n\t\t// Extract templ component declarations\n\t\tif strings.HasPrefix(line, \"templ \") {\n\t\t\t// Extract component name from templ declaration\n\t\t\tparts := strings.Fields(line)\n\t\t\tif len(parts) >= 2 {\n\t\t\t\tname := parts[1]\n\t\t\t\tif idx := strings.Index(name, \"(\"); idx != -1 {\n\t\t\t\t\tname = name[:idx]\n\t\t\t\t}\n\n\t\t\t\t// Sanitize component name to prevent injection\n\t\t\t\tname = sanitizeIdentifier(name)\n\n\t\t\t\tcomponent := &types.ComponentInfo{\n\t\t\t\t\tName:         name,\n\t\t\t\t\tPackage:      packageName,\n\t\t\t\t\tFilePath:     path,\n\t\t\t\t\tParameters:   extractParameters(line),\n\t\t\t\t\tImports:      []string{},\n\t\t\t\t\tLastMod:      modTime,\n\t\t\t\t\tHash:         hash,\n\t\t\t\t\tDependencies: []string{},\n\t\t\t\t}\n\n\t\t\t\tcomponents = append(components, component)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn components, nil\n}\n\n// parseTemplFile provides backward compatibility - delegates to the new component-returning version\nfunc (s *ComponentScanner) parseTemplFile(path string, content []byte, hash string, modTime time.Time) error {\n\tcomponents, err := s.parseTemplFileWithComponents(path, content, hash, modTime)\n\tif err != nil {\n\t\treturn err\n\t}\n\t\n\t// Register all components\n\tfor _, component := range components {\n\t\ts.registry.Register(component)\n\t}\n\t\n\treturn nil\n}\n\n// extractFromASTWithComponents extracts components from AST and returns them\nfunc (s *ComponentScanner) extractFromASTWithComponents(path string, astFile *ast.File, hash string, modTime time.Time) ([]*types.ComponentInfo, error) {\n\tvar components []*types.ComponentInfo\n\t\n\t// Walk the AST to find function declarations that might be templ components\n\tast.Inspect(astFile, func(n ast.Node) bool {\n\t\tswitch node := n.(type) {\n\t\tcase *ast.FuncDecl:\n\t\t\tif node.Name != nil && node.Name.IsExported() {\n\t\t\t\t// Check if this might be a templ component\n\t\t\t\tif s.isTemplComponent(node) {\n\t\t\t\t\tcomponent := &types.ComponentInfo{\n\t\t\t\t\t\tName:         node.Name.Name,\n\t\t\t\t\t\tPackage:      astFile.Name.Name,\n\t\t\t\t\t\tFilePath:     path,\n\t\t\t\t\t\tParameters:   s.extractParametersFromFunc(node),\n\t\t\t\t\t\tImports:      s.extractImports(astFile),\n\t\t\t\t\t\tLastMod:      modTime,\n\t\t\t\t\t\tHash:         hash,\n\t\t\t\t\t\tDependencies: []string{},\n\t\t\t\t\t}\n\n\t\t\t\t\tcomponents = append(components, component)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn true\n\t})\n\n\treturn components, nil\n}\n\n// extractFromAST provides backward compatibility - delegates to the new component-returning version\nfunc (s *ComponentScanner) extractFromAST(path string, astFile *ast.File, hash string, modTime time.Time) error {\n\tcomponents, err := s.extractFromASTWithComponents(path, astFile, hash, modTime)\n\tif err != nil {\n\t\treturn err\n\t}\n\t\n\t// Register all components\n\tfor _, component := range components {\n\t\ts.registry.Register(component)\n\t}\n\t\n\treturn nil\n}\n\nfunc (s *ComponentScanner) isTemplComponent(fn *ast.FuncDecl) bool {\n\t// Check if the function returns a templ.Component\n\tif fn.Type.Results == nil || len(fn.Type.Results.List) == 0 {\n\t\treturn false\n\t}\n\n\tresult := fn.Type.Results.List[0]\n\tif sel, ok := result.Type.(*ast.SelectorExpr); ok {\n\t\tif ident, ok := sel.X.(*ast.Ident); ok {\n\t\t\treturn ident.Name == \"templ\" && sel.Sel.Name == \"Component\"\n\t\t}\n\t}\n\n\treturn false\n}\n\nfunc (s *ComponentScanner) extractParametersFromFunc(fn *ast.FuncDecl) []types.ParameterInfo {\n\tvar params []types.ParameterInfo\n\n\tif fn.Type.Params == nil {\n\t\treturn params\n\t}\n\n\tfor _, param := range fn.Type.Params.List {\n\t\tparamType := \"\"\n\t\tif param.Type != nil {\n\t\t\tparamType = s.typeToString(param.Type)\n\t\t}\n\n\t\tfor _, name := range param.Names {\n\t\t\tparams = append(params, types.ParameterInfo{\n\t\t\t\tName:     name.Name,\n\t\t\t\tType:     paramType,\n\t\t\t\tOptional: false,\n\t\t\t\tDefault:  nil,\n\t\t\t})\n\t\t}\n\t}\n\n\treturn params\n}\n\nfunc (s *ComponentScanner) extractImports(astFile *ast.File) []string {\n\tvar imports []string\n\n\tfor _, imp := range astFile.Imports {\n\t\tif imp.Path != nil {\n\t\t\timports = append(imports, imp.Path.Value)\n\t\t}\n\t}\n\n\treturn imports\n}\n\nfunc (s *ComponentScanner) typeToString(expr ast.Expr) string {\n\tswitch e := expr.(type) {\n\tcase *ast.Ident:\n\t\treturn e.Name\n\tcase *ast.SelectorExpr:\n\t\treturn s.typeToString(e.X) + \".\" + e.Sel.Name\n\tcase *ast.StarExpr:\n\t\treturn \"*\" + s.typeToString(e.X)\n\tcase *ast.ArrayType:\n\t\treturn \"[]\" + s.typeToString(e.Elt)\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\nfunc extractParameters(line string) []types.ParameterInfo {\n\t// Simple parameter extraction from templ declaration\n\t// This is a basic implementation - real parser would be more robust\n\tif !strings.Contains(line, \"(\") {\n\t\treturn []types.ParameterInfo{}\n\t}\n\n\tstart := strings.Index(line, \"(\")\n\tend := strings.LastIndex(line, \")\")\n\tif start == -1 || end == -1 || start >= end {\n\t\treturn []types.ParameterInfo{}\n\t}\n\n\tparamStr := line[start+1 : end]\n\tif strings.TrimSpace(paramStr) == \"\" {\n\t\treturn []types.ParameterInfo{}\n\t}\n\n\t// Basic parameter parsing - handle both \"name type\" and \"name, name type\" patterns\n\tparts := strings.Split(paramStr, \",\")\n\tvar params []types.ParameterInfo\n\n\tfor _, part := range parts {\n\t\tpart = strings.TrimSpace(part)\n\t\tif part == \"\" {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Split by space to get name and type\n\t\tfields := strings.Fields(part)\n\t\tif len(fields) >= 2 {\n\t\t\t// Handle \"name type\" format\n\t\t\tparams = append(params, types.ParameterInfo{\n\t\t\t\tName:     fields[0],\n\t\t\t\tType:     fields[1],\n\t\t\t\tOptional: false,\n\t\t\t\tDefault:  nil,\n\t\t\t})\n\t\t} else if len(fields) == 1 {\n\t\t\t// Handle single parameter name (type might be from previous param)\n\t\t\tparams = append(params, types.ParameterInfo{\n\t\t\t\tName:     fields[0],\n\t\t\t\tType:     \"string\", // Default type\n\t\t\t\tOptional: false,\n\t\t\t\tDefault:  nil,\n\t\t\t})\n\t\t}\n\t}\n\n\treturn params\n}\n\n// sanitizeIdentifier removes dangerous characters from identifiers\nfunc sanitizeIdentifier(identifier string) string {\n\t// Only allow alphanumeric characters and underscores for identifiers\n\tvar cleaned strings.Builder\n\tfor _, r := range identifier {\n\t\tif (r >= 'a' && r <= 'z') || (r >= 'A' && r <= 'Z') || (r >= '0' && r <= '9') || r == '_' {\n\t\t\tcleaned.WriteRune(r)\n\t\t}\n\t}\n\treturn cleaned.String()\n}\n\n// validatePath validates and cleans a file path to prevent directory traversal.\n// This optimized version caches the current working directory to avoid repeated\n// expensive filesystem operations, achieving 50-70% performance improvement.\nfunc (s *ComponentScanner) validatePath(path string) (string, error) {\n\t// Clean the path to resolve . and .. elements\n\tcleanPath := filepath.Clean(path)\n\n\t// Get absolute path to normalize (needed for working directory check)\n\tabsPath, err := filepath.Abs(cleanPath)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"getting absolute path: %w\", err)\n\t}\n\n\t// Get cached current working directory\n\tcwd, err := s.getCachedWorkingDir()\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"getting current directory: %w\", err)\n\t}\n\n\t// Primary security check: ensure the path is within the current working directory\n\t// This prevents directory traversal attacks that escape the working directory\n\tif !strings.HasPrefix(absPath, cwd) {\n\t\treturn \"\", errors.ErrPathTraversal(path).WithContext(\"working_directory\", cwd)\n\t}\n\n\t// Secondary security check: reject paths with suspicious patterns\n\t// This catches directory traversal attempts that stay within the working directory\n\tif strings.Contains(cleanPath, \"..\") {\n\t\treturn \"\", errors.ErrPathTraversal(path).\n\t\t\tWithContext(\"pattern\", \"contains '..' traversal\")\n\t}\n\n\treturn cleanPath, nil\n}\n\n// getCachedWorkingDir returns the current working directory from cache,\n// initializing it on first access. This eliminates repeated os.Getwd() calls.\nfunc (s *ComponentScanner) getCachedWorkingDir() (string, error) {\n\t// Fast path: check if already initialized with read lock\n\ts.pathCache.mu.RLock()\n\tif s.pathCache.initialized {\n\t\tcwd := s.pathCache.currentWorkingDir\n\t\ts.pathCache.mu.RUnlock()\n\t\treturn cwd, nil\n\t}\n\ts.pathCache.mu.RUnlock()\n\n\t// Slow path: initialize the cache with write lock\n\ts.pathCache.mu.Lock()\n\tdefer s.pathCache.mu.Unlock()\n\n\t// Double-check pattern: another goroutine might have initialized while waiting\n\tif s.pathCache.initialized {\n\t\treturn s.pathCache.currentWorkingDir, nil\n\t}\n\n\t// Get current working directory (expensive syscall - done only once)\n\tcwd, err := os.Getwd()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Ensure we have the absolute path for consistent comparison\n\tabsCwd, err := filepath.Abs(cwd)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"getting absolute working directory: %w\", err)\n\t}\n\n\t// Cache the result\n\ts.pathCache.currentWorkingDir = absCwd\n\ts.pathCache.initialized = true\n\n\treturn absCwd, nil\n}\n\n// InvalidatePathCache clears the cached working directory.\n// This should be called if the working directory changes during execution.\nfunc (s *ComponentScanner) InvalidatePathCache() {\n\ts.pathCache.mu.Lock()\n\tdefer s.pathCache.mu.Unlock()\n\ts.pathCache.initialized = false\n\ts.pathCache.currentWorkingDir = \"\"\n}\n\n// walkDirectoryConcurrent implements concurrent directory walking for improved performance\n// on large codebases. Uses goroutines to parallelize directory discovery.\nfunc (s *ComponentScanner) walkDirectoryConcurrent(rootDir string) ([]string, error) {\n\t// For small directory trees, use optimized sequential version\n\t// For larger trees, use concurrent discovery\n\t\n\t// Quick check for directory size to decide approach\n\tentries, err := os.ReadDir(rootDir)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading root directory %s: %w\", rootDir, err)\n\t}\n\t\n\t// If small directory, use optimized sequential\n\tif len(entries) < 10 {\n\t\treturn s.walkDirectoryOptimized(rootDir)\n\t}\n\t\n\t// Use concurrent approach for larger directories\n\treturn s.walkDirectoryParallel(rootDir)\n}\n\n// walkDirectoryParallel implements concurrent directory discovery\nfunc (s *ComponentScanner) walkDirectoryParallel(rootDir string) ([]string, error) {\n\t// Use a simple approach: collect all directories first, then process them concurrently\n\t\n\t// First, collect all directories sequentially (this is fast)\n\tvar allDirs []string\n\terr := filepath.WalkDir(rootDir, func(path string, d os.DirEntry, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif d.IsDir() && !s.shouldSkipDirectory(d.Name()) {\n\t\t\tallDirs = append(allDirs, path)\n\t\t}\n\t\treturn nil\n\t})\n\t\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t\n\t// Now process directories concurrently\n\tconst maxWorkers = 4\n\tjobs := make(chan string, len(allDirs))\n\tresults := make(chan []string, len(allDirs))\n\t\n\t// Start workers\n\tvar wg sync.WaitGroup\n\tfor w := 0; w < maxWorkers; w++ {\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\tfor dir := range jobs {\n\t\t\t\tfiles, _, _ := s.processSingleDirectory(dir)\n\t\t\t\tif len(files) > 0 {\n\t\t\t\t\tresults <- files\n\t\t\t\t} else {\n\t\t\t\t\tresults <- nil // Send empty result to maintain count\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n\t\n\t// Send jobs\n\tfor _, dir := range allDirs {\n\t\tjobs <- dir\n\t}\n\tclose(jobs)\n\t\n\t// Wait for workers to finish\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(results)\n\t}()\n\t\n\t// Collect results\n\tvar allFiles []string\n\tfor files := range results {\n\t\tif files != nil {\n\t\t\tallFiles = append(allFiles, files...)\n\t\t}\n\t}\n\t\n\treturn allFiles, nil\n}\n\n// walkDirectoryOptimized implements an optimized sequential walk with directory skipping\nfunc (s *ComponentScanner) walkDirectoryOptimized(rootDir string) ([]string, error) {\n\tvar files []string\n\t\n\terr := filepath.WalkDir(rootDir, func(path string, d os.DirEntry, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Skip directories that typically don't contain templ files\n\t\tif d.IsDir() && s.shouldSkipDirectory(d.Name()) {\n\t\t\treturn filepath.SkipDir\n\t\t}\n\n\t\tif !d.IsDir() && strings.HasSuffix(path, \".templ\") {\n\t\t\t// Validate each file path as we encounter it\n\t\t\tif _, err := s.validatePath(path); err != nil {\n\t\t\t\t// Skip invalid paths silently for security\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tfiles = append(files, path)\n\t\t}\n\t\t\n\t\treturn nil\n\t})\n\t\n\treturn files, err\n}\n\n// processSingleDirectory processes a single directory and returns files and subdirectories\nfunc (s *ComponentScanner) processSingleDirectory(dir string) ([]string, []string, error) {\n\tvar files []string\n\tvar subdirs []string\n\t\n\tentries, err := os.ReadDir(dir)\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"reading directory %s: %w\", dir, err)\n\t}\n\t\n\tfor _, entry := range entries {\n\t\tpath := filepath.Join(dir, entry.Name())\n\t\t\n\t\tif entry.IsDir() {\n\t\t\t// Skip directories that typically don't contain templ files\n\t\t\tif s.shouldSkipDirectory(entry.Name()) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tsubdirs = append(subdirs, path)\n\t\t} else if strings.HasSuffix(entry.Name(), \".templ\") {\n\t\t\t// Validate each file path as we encounter it\n\t\t\tif _, err := s.validatePath(path); err != nil {\n\t\t\t\t// Skip invalid paths silently for security\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfiles = append(files, path)\n\t\t}\n\t}\n\t\n\treturn files, subdirs, nil\n}\n\n// shouldSkipDirectory determines if a directory should be skipped for performance\nfunc (s *ComponentScanner) shouldSkipDirectory(name string) bool {\n\tskipDirs := map[string]bool{\n\t\t\".git\":         true,\n\t\t\".svn\":         true,\n\t\t\"node_modules\": true,\n\t\t\".next\":        true,\n\t\t\"dist\":         true,\n\t\t\"build\":        true,\n\t\t\"vendor\":       true,\n\t\t\".vscode\":      true,\n\t\t\".idea\":        true,\n\t\t\"__pycache__\":  true,\n\t\t\".DS_Store\":    true,\n\t}\n\t\n\treturn skipDirs[name]\n}\n\n\n",
      "structuredPatch": [
        {
          "oldStart": 1515,
          "oldLines": 4,
          "newStart": 1515,
          "newLines": 130,
          "lines": [
            "   return skipDirs[name]",
            " }",
            " ",
            "+// HashingStrategy contains information about the hash generation approach used",
            "+type HashingStrategy struct {",
            "+  Strategy     FileHashStrategy",
            "+  SamplePoints int",
            "+  HashTime     time.Duration",
            "+  FileSize     int64",
            "+}",
            " ",
            "+// generateOptimizedHash creates an optimized hash based on file size and content characteristics",
            "+func (s *ComponentScanner) generateOptimizedHash(content []byte, fileInfo os.FileInfo) (string, *HashingStrategy) {",
            "+  start := time.Now()",
            "+  fileSize := int64(len(content))",
            "+  ",
            "+  strategy := &HashingStrategy{",
            "+    FileSize: fileSize,",
            "+  }",
            "+  ",
            "+  var hash uint32",
            "+  ",
            "+  switch {",
            "+  case fileSize <= smallFileThreshold:",
            "+    // Small files: use full content CRC32 (fast anyway)",
            "+    hash = crc32.Checksum(content, crcTable)",
            "+    strategy.Strategy = HashStrategyFull",
            "+    strategy.SamplePoints = 1",
            "+    ",
            "+  case fileSize <= mediumFileThreshold:",
            "+    // Medium files: use content sampling",
            "+    hash = s.generateSampledHash(content)",
            "+    strategy.Strategy = HashStrategySampled",
            "+    strategy.SamplePoints = 3",
            "+    ",
            "+  default:",
            "+    // Large files: use hierarchical sampling with metadata",
            "+    hash = s.generateHierarchicalHash(content, fileInfo)",
            "+    strategy.Strategy = HashStrategyHierarchical",
            "+    strategy.SamplePoints = 5",
            "+  }",
            "+  ",
            "+  strategy.HashTime = time.Since(start)",
            "+  ",
            "+  // Include file metadata in hash to catch size/timestamp changes",
            "+  metadataHash := s.generateMetadataHash(fileInfo)",
            "+  combinedHash := hash ^ metadataHash",
            "+  ",
            "+  return strconv.FormatUint(uint64(combinedHash), 16), strategy",
            "+}",
            "+",
            "+// generateSampledHash creates a hash from strategic content samples",
            "+func (s *ComponentScanner) generateSampledHash(content []byte) uint32 {",
            "+  if len(content) == 0 {",
            "+    return 0",
            "+  }",
            "+  ",
            "+  // Sample three strategic points: beginning, middle, and end",
            "+  sampleSize := contentSampleSize",
            "+  if len(content) < sampleSize*3 {",
            "+    // If file is small, just hash it all",
            "+    return crc32.Checksum(content, crcTable)",
            "+  }",
            "+  ",
            "+  // Create a combined sample from key sections",
            "+  var samples []byte",
            "+  ",
            "+  // Beginning sample",
            "+  if len(content) > sampleSize {",
            "+    samples = append(samples, content[:sampleSize]...)",
            "+  }",
            "+  ",
            "+  // Middle sample",
            "+  mid := len(content) / 2",
            "+  midStart := mid - sampleSize/2",
            "+  midEnd := mid + sampleSize/2",
            "+  if midStart >= 0 && midEnd < len(content) {",
            "+    samples = append(samples, content[midStart:midEnd]...)",
            "+  }",
            "+  ",
            "+  // End sample",
            "+  if len(content) > sampleSize {",
            "+    samples = append(samples, content[len(content)-sampleSize:]...)",
            "+  }",
            "+  ",
            "+  return crc32.Checksum(samples, crcTable)",
            "+}",
            "+",
            "+// generateHierarchicalHash creates a hierarchical hash for large files",
            "+func (s *ComponentScanner) generateHierarchicalHash(content []byte, fileInfo os.FileInfo) uint32 {",
            "+  if len(content) == 0 {",
            "+    return 0",
            "+  }",
            "+  ",
            "+  // For templ files, focus on key sections that are likely to change",
            "+  var keyContent []byte",
            "+  ",
            "+  // Add file header (package declaration, imports)",
            "+  if len(content) > 2048 {",
            "+    keyContent = append(keyContent, content[:2048]...)",
            "+  }",
            "+  ",
            "+  // Sample multiple points throughout the file",
            "+  chunkSize := len(content) / 8 // Divide into 8 chunks",
            "+  if chunkSize > contentSampleSize {",
            "+    for i := 1; i < 8; i++ {",
            "+      start := i * chunkSize",
            "+      end := start + contentSampleSize/8",
            "+      if end < len(content) {",
            "+        keyContent = append(keyContent, content[start:end]...)",
            "+      }",
            "+    }",
            "+  }",
            "+  ",
            "+  // Add file footer (last part likely to contain component definitions)",
            "+  if len(content) > 1024 {",
            "+    keyContent = append(keyContent, content[len(content)-1024:]...)",
            "+  }",
            "+  ",
            "+  return crc32.Checksum(keyContent, crcTable)",
            "+}",
            "+",
            "+// generateMetadataHash creates a hash from file metadata",
            "+func (s *ComponentScanner) generateMetadataHash(fileInfo os.FileInfo) uint32 {",
            "+  // Combine file size and modification time for metadata hash",
            "+  metadata := fmt.Sprintf(\"%d:%d\", fileInfo.Size(), fileInfo.ModTime().Unix())",
            "+  return crc32.ChecksumIEEE([]byte(metadata))",
            "+}",
            "+",
            "+"
          ]
        }
      ],
      "userModified": false,
      "replaceAll": false
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Read",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
      "offset": 360,
      "limit": 30
    },
    "tool_response": {
      "type": "text",
      "file": {
        "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
        "content": "\t\t\t// Results queue full - this could cause result loss\n\t\t\tfmt.Printf(\"Warning: Results queue full, dropping cache hit result for component %s\\n\", buildResult.Component.Name)\n\t\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full_cache_hit\")\n\t\t}\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\t}\n\n\t// Create timeout context for build operation based on configuration\n\tbuildTimeout := bp.getBuildTimeout()\n\tbuildCtx, cancel := context.WithTimeout(ctx, buildTimeout)\n\tdefer cancel()\n\n\t// Execute build with pooled output buffer and context-based timeout\n\toutput, err := bp.compiler.CompileWithPools(buildCtx, task.Component, bp.objectPools)\n\n\t// Parse errors if build failed\n\tvar parsedErrors []*errors.ParsedError\n\tif err != nil {\n\t\t// Wrap the error with build context for better debugging\n\t\terr = errors.WrapBuild(err, errors.ErrCodeBuildFailed, \n\t\t\t\"component compilation failed\", task.Component.Name).\n\t\t\tWithLocation(task.Component.FilePath, 0, 0)\n\t\tparsedErrors = bp.errorParser.ParseError(string(output))\n\t}\n\n\t// Use object pool for build result\n\tbuildResult := bp.objectPools.GetBuildResult()\n\tbuildResult.Component = task.Component\n\tbuildResult.Output = output",
        "numLines": 30,
        "startLine": 360,
        "totalLines": 866
      }
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Read",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
      "offset": 395,
      "limit": 30
    },
    "tool_response": {
      "type": "text",
      "file": {
        "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
        "content": "\n\t// Cache successful builds\n\tif err == nil {\n\t\tbp.cache.Set(contentHash, output)\n\t}\n\n\t// Non-blocking send to results channel to prevent worker hangs\n\tselect {\n\tcase bp.queue.results <- *buildResult:\n\t\t// Result successfully queued\n\tdefault:\n\t\t// Results queue full - this could cause result loss\n\t\tfmt.Printf(\"Warning: Results queue full, dropping result for component %s\\n\", buildResult.Component.Name)\n\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full\")\n\t}\n\tbp.objectPools.PutBuildResult(buildResult)\n}\n\nfunc (bp *BuildPipeline) processResults(ctx context.Context) {\n\tdefer bp.resultWg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase result := <-bp.queue.results:\n\t\t\tbp.handleBuildResult(result)\n\t\t}\n\t}\n}",
        "numLines": 30,
        "startLine": 395,
        "totalLines": 866
      }
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Edit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/scanner.go",
      "old_string": "\t// Calculate file hash for cache lookup and change detection\n\thash := strconv.FormatUint(uint64(crc32.Checksum(content, crcTable)), 16)",
      "new_string": "\t// Calculate optimized file hash for cache lookup and change detection\n\thash, hashStrategy := s.generateOptimizedHash(content, info)\n\t\n\t// Track hash generation metrics\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.FilesProcessed, 1)\n\t\t// Add hash strategy metrics if needed for monitoring\n\t}"
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/scanner.go",
      "oldString": "\t// Calculate file hash for cache lookup and change detection\n\thash := strconv.FormatUint(uint64(crc32.Checksum(content, crcTable)), 16)",
      "newString": "\t// Calculate optimized file hash for cache lookup and change detection\n\thash, hashStrategy := s.generateOptimizedHash(content, info)\n\t\n\t// Track hash generation metrics\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.FilesProcessed, 1)\n\t\t// Add hash strategy metrics if needed for monitoring\n\t}",
      "originalFile": "// Package scanner provides component discovery and analysis for templ templates.\n//\n// The scanner traverses file systems to find .templ files, parses them using\n// Go's AST parser to extract component metadata including parameters, dependencies,\n// and documentation. It integrates with the component registry to broadcast\n// change events and supports recursive directory scanning with exclude patterns.\n// The scanner maintains file hashes for change detection and provides both\n// single-file and batch scanning capabilities.\npackage scanner\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"go/ast\"\n\t\"go/parser\"\n\t\"go/token\"\n\t\"hash/crc32\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/conneroisu/templar/internal/errors\"\n\t\"github.com/conneroisu/templar/internal/interfaces\"\n\t\"github.com/conneroisu/templar/internal/registry\"\n\t\"github.com/conneroisu/templar/internal/types\"\n)\n\n// crcTable is a pre-computed CRC32 Castagnoli table for faster hash generation\nvar crcTable = crc32.MakeTable(crc32.Castagnoli)\n\n// Hash generation strategy constants\nconst (\n\t// Small files (< 4KB) - use full content CRC32\n\tsmallFileThreshold = 4 * 1024\n\t// Medium files (4KB - 256KB) - use content sampling\n\tmediumFileThreshold = 256 * 1024\n\t// Large files (> 256KB) - use hierarchical sampling\n\tlargeFileThreshold = 256 * 1024\n\t// Content sample size for large files\n\tcontentSampleSize = 1024\n)\n\n// FileHashStrategy represents different hashing approaches\ntype FileHashStrategy int\n\nconst (\n\tHashStrategyFull FileHashStrategy = iota\n\tHashStrategySampled\n\tHashStrategyHierarchical\n)\n\n// ScanJob represents a scanning job for the worker pool containing the file\n// path to scan and a result channel for asynchronous communication.\ntype ScanJob struct {\n\t// filePath is the absolute path to the .templ file to be scanned\n\tfilePath string\n\t// result channel receives the scan result or error asynchronously\n\tresult chan<- ScanResult\n}\n\n// HashResult represents the result of asynchronous hash calculation\ntype HashResult struct {\n\thash string\n\terr  error\n}\n\n// BufferPool manages reusable byte buffers for file reading optimization\ntype BufferPool struct {\n\tpool sync.Pool\n}\n\n// NewBufferPool creates a new buffer pool with initial buffer size\nfunc NewBufferPool() *BufferPool {\n\treturn &BufferPool{\n\t\tpool: sync.Pool{\n\t\t\tNew: func() interface{} {\n\t\t\t\t// Pre-allocate 64KB buffers for typical component files\n\t\t\t\treturn make([]byte, 0, 64*1024)\n\t\t\t},\n\t\t},\n\t}\n}\n\n// Get retrieves a buffer from the pool\nfunc (bp *BufferPool) Get() []byte {\n\treturn bp.pool.Get().([]byte)[:0] // Reset length but keep capacity\n}\n\n// Put returns a buffer to the pool\nfunc (bp *BufferPool) Put(buf []byte) {\n\t// Only pool reasonably-sized buffers to avoid memory leaks\n\tif cap(buf) <= 1024*1024 { // 1MB limit\n\t\tbp.pool.Put(buf)\n\t}\n}\n\n// ScanResult represents the result of a scanning operation, containing either\n// success status or error information for a specific file.\ntype ScanResult struct {\n\t// filePath is the path that was scanned\n\tfilePath string\n\t// err contains any error that occurred during scanning, nil on success\n\terr error\n}\n\n// WorkerPool manages persistent scanning workers for performance optimization\n// using a work-stealing approach to distribute scanning jobs across CPU cores.\ntype WorkerPool struct {\n\t// jobQueue buffers scanning jobs for worker distribution\n\tjobQueue chan ScanJob\n\t// workers holds references to all active worker goroutines\n\tworkers []*ScanWorker\n\t// workerCount defines the number of concurrent workers (typically NumCPU)\n\tworkerCount int\n\t// scanner is the shared component scanner instance\n\tscanner *ComponentScanner\n\t// stop signals all workers to terminate gracefully\n\tstop chan struct{}\n\t// stopped tracks pool shutdown state\n\tstopped bool\n\t// mu protects concurrent access to pool state\n\tmu sync.RWMutex\n}\n\n// ScanWorker represents a persistent worker goroutine that processes scanning\n// jobs from the shared job queue. Each worker operates independently and\n// can handle multiple file types concurrently.\ntype ScanWorker struct {\n\t// id uniquely identifies this worker for debugging and metrics\n\tid int\n\t// jobQueue receives scanning jobs from the worker pool\n\tjobQueue <-chan ScanJob\n\t// scanner provides the component parsing functionality\n\tscanner *ComponentScanner\n\t// stop signals this worker to terminate gracefully\n\tstop chan struct{}\n}\n\n// ComponentScanner discovers and parses templ components using Go's AST parser.\n//\n// The scanner provides:\n// - Recursive directory traversal with exclude patterns\n// - AST-based component metadata extraction\n// - Concurrent processing via worker pool\n// - Integration with component registry for event broadcasting\n// - File change detection using CRC32 hashing\n// - Optimized path validation with cached working directory\n// - Buffer pooling for memory optimization in large codebases\n// - Component metadata caching with LRU eviction for performance\ntype ComponentScanner struct {\n\t// registry receives discovered components and broadcasts change events\n\tregistry *registry.ComponentRegistry\n\t// fileSet tracks file positions for AST parsing and error reporting\n\tfileSet *token.FileSet\n\t// workerPool manages concurrent scanning operations\n\tworkerPool *WorkerPool\n\t// pathCache contains cached path validation data to avoid repeated syscalls\n\tpathCache *pathValidationCache\n\t// bufferPool provides reusable byte buffers for file reading optimization\n\tbufferPool *BufferPool\n\t// metadataCache caches parsed component metadata by file hash to avoid re-parsing unchanged files\n\tmetadataCache *MetadataCache\n\t// astParsingPool provides concurrent AST parsing to avoid blocking worker threads\n\tastParsingPool *ASTParsingPool\n\t// metrics tracks performance metrics during scanning operations\n\tmetrics *ScannerMetrics\n\t// config provides timeout configuration for scanning operations\n\tconfig *config.Config\n}\n\n// Interface compliance verification - ComponentScanner implements interfaces.ComponentScanner\nvar _ interfaces.ComponentScanner = (*ComponentScanner)(nil)\n\n// ASTParseJob represents a parsing job for the AST parsing pool\ntype ASTParseJob struct {\n\tfilePath string\n\tcontent  []byte\n\tfileSet  *token.FileSet\n\tresult   chan<- ASTParseResult\n}\n\n// ASTParseResult contains the result of AST parsing\ntype ASTParseResult struct {\n\tastFile *ast.File\n\terr     error\n\tfilePath string\n}\n\n// ASTParsingPool manages concurrent AST parsing to avoid blocking worker threads\ntype ASTParsingPool struct {\n\tworkers   int\n\tjobChan   chan ASTParseJob\n\tcloseChan chan struct{}\n\twg        sync.WaitGroup\n}\n\n// NewASTParsingPool creates a new AST parsing pool with specified worker count\nfunc NewASTParsingPool(workers int) *ASTParsingPool {\n\tif workers <= 0 {\n\t\tworkers = runtime.NumCPU() / 2 // Use half CPU cores for AST parsing\n\t\tif workers < 1 {\n\t\t\tworkers = 1\n\t\t}\n\t}\n\t\n\tpool := &ASTParsingPool{\n\t\tworkers:   workers,\n\t\tjobChan:   make(chan ASTParseJob, workers*2),\n\t\tcloseChan: make(chan struct{}),\n\t}\n\t\n\t// Start worker goroutines\n\tfor i := 0; i < workers; i++ {\n\t\tpool.wg.Add(1)\n\t\tgo pool.worker()\n\t}\n\t\n\treturn pool\n}\n\n// worker processes AST parsing jobs\nfunc (p *ASTParsingPool) worker() {\n\tdefer p.wg.Done()\n\t\n\tfor {\n\t\tselect {\n\t\tcase job := <-p.jobChan:\n\t\t\t// Parse the AST\n\t\t\tastFile, err := parser.ParseFile(job.fileSet, job.filePath, job.content, parser.ParseComments)\n\t\t\t\n\t\t\t// Send result back\n\t\t\tselect {\n\t\t\tcase job.result <- ASTParseResult{\n\t\t\t\tastFile: astFile,\n\t\t\t\terr:     err,\n\t\t\t\tfilePath: job.filePath,\n\t\t\t}:\n\t\t\tcase <-p.closeChan:\n\t\t\t\treturn\n\t\t\t}\n\t\t\t\n\t\tcase <-p.closeChan:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// ParseAsync submits an AST parsing job and returns a result channel\nfunc (p *ASTParsingPool) ParseAsync(filePath string, content []byte, fileSet *token.FileSet) <-chan ASTParseResult {\n\tresult := make(chan ASTParseResult, 1)\n\t\n\t// For very large files, use optimized parsing approach\n\tif len(content) > 1024*1024 { // 1MB threshold\n\t\tgo p.parseLargeFileAsync(filePath, content, fileSet, result)\n\t\treturn result\n\t}\n\t\n\tselect {\n\tcase p.jobChan <- ASTParseJob{\n\t\tfilePath: filePath,\n\t\tcontent:  content,\n\t\tfileSet:  fileSet,\n\t\tresult:   result,\n\t}:\n\t\treturn result\n\tcase <-p.closeChan:\n\t\t// Pool is closed, return error result\n\t\tgo func() {\n\t\t\tresult <- ASTParseResult{\n\t\t\t\tastFile: nil,\n\t\t\t\terr:     fmt.Errorf(\"AST parsing pool is closed\"),\n\t\t\t\tfilePath: filePath,\n\t\t\t}\n\t\t}()\n\t\treturn result\n\t}\n}\n\n// parseLargeFileAsync handles large file parsing with memory optimization\nfunc (p *ASTParsingPool) parseLargeFileAsync(filePath string, content []byte, fileSet *token.FileSet, result chan<- ASTParseResult) {\n\tdefer close(result)\n\t\n\t// For large files, use streaming approach with limited memory usage\n\t// Parse with limited goroutines to prevent memory exhaustion\n\tastFile, err := parser.ParseFile(fileSet, filePath, content, parser.ParseComments|parser.SkipObjectResolution)\n\t\n\tresult <- ASTParseResult{\n\t\tastFile: astFile,\n\t\terr:     err,\n\t\tfilePath: filePath,\n\t}\n}\n\n// Close shuts down the AST parsing pool\nfunc (p *ASTParsingPool) Close() {\n\tclose(p.closeChan)\n\tclose(p.jobChan)\n\tp.wg.Wait()\n}\n\n// pathValidationCache caches expensive filesystem operations for optimal performance\ntype pathValidationCache struct {\n\t// mu protects concurrent access to cache fields\n\tmu sync.RWMutex\n\t// currentWorkingDir is the cached current working directory (absolute path)\n\tcurrentWorkingDir string\n\t// initialized indicates whether the cache has been populated\n\tinitialized bool\n}\n\n// CachedComponentMetadata stores pre-parsed component information for cache optimization\ntype CachedComponentMetadata struct {\n\t// Components is a slice of all components found in the file\n\tComponents []*types.ComponentInfo\n\t// FileHash is the CRC32 hash of the file content when cached\n\tFileHash string\n\t// ParsedAt records when the metadata was cached\n\tParsedAt time.Time\n}\n\n// ScannerMetrics tracks performance metrics during scanning operations\ntype ScannerMetrics struct {\n\t// FilesProcessed is the total number of files processed\n\tFilesProcessed int64\n\t// ComponentsFound is the total number of components discovered\n\tComponentsFound int64\n\t// CacheHits tracks how many files were served from cache\n\tCacheHits int64\n\t// CacheMisses tracks how many files required parsing\n\tCacheMisses int64\n\t// TotalScanTime tracks time spent in scanning operations\n\tTotalScanTime time.Duration\n\t// PeakMemoryUsage tracks the peak memory usage during scanning\n\tPeakMemoryUsage uint64\n\t// ConcurrentJobs tracks the peak number of concurrent jobs\n\tConcurrentJobs int64\n}\n\n// MetadataCache implements a simple LRU cache for component metadata\ntype MetadataCache struct {\n\tmu       sync.RWMutex\n\tentries  map[string]*MetadataCacheEntry\n\tmaxSize  int\n\tttl      time.Duration\n\t// LRU doubly-linked list\n\thead *MetadataCacheEntry\n\ttail *MetadataCacheEntry\n}\n\n// MetadataCacheEntry represents a cached metadata entry with LRU pointers\ntype MetadataCacheEntry struct {\n\tKey       string\n\tData      []byte\n\tCreatedAt time.Time\n\t// LRU pointers\n\tprev *MetadataCacheEntry\n\tnext *MetadataCacheEntry\n}\n\n// NewMetadataCache creates a new metadata cache\nfunc NewMetadataCache(maxSize int, ttl time.Duration) *MetadataCache {\n\tcache := &MetadataCache{\n\t\tentries: make(map[string]*MetadataCacheEntry),\n\t\tmaxSize: maxSize,\n\t\tttl:     ttl,\n\t}\n\t\n\t// Initialize dummy head and tail for LRU\n\tcache.head = &MetadataCacheEntry{}\n\tcache.tail = &MetadataCacheEntry{}\n\tcache.head.next = cache.tail\n\tcache.tail.prev = cache.head\n\t\n\treturn cache\n}\n\n// Get retrieves data from cache\nfunc (mc *MetadataCache) Get(key string) ([]byte, bool) {\n\tmc.mu.Lock()\n\tdefer mc.mu.Unlock()\n\t\n\tentry, exists := mc.entries[key]\n\tif !exists {\n\t\treturn nil, false\n\t}\n\t\n\t// Check TTL\n\tif time.Since(entry.CreatedAt) > mc.ttl {\n\t\tmc.removeFromList(entry)\n\t\tdelete(mc.entries, key)\n\t\treturn nil, false\n\t}\n\t\n\t// Move to front (most recently used)\n\tmc.moveToFront(entry)\n\treturn entry.Data, true\n}\n\n// Set stores data in cache\nfunc (mc *MetadataCache) Set(key string, data []byte) {\n\tmc.mu.Lock()\n\tdefer mc.mu.Unlock()\n\t\n\t// Check if entry exists\n\tif existingEntry, exists := mc.entries[key]; exists {\n\t\texistingEntry.Data = data\n\t\texistingEntry.CreatedAt = time.Now()\n\t\tmc.moveToFront(existingEntry)\n\t\treturn\n\t}\n\t\n\t// Evict if needed\n\tif len(mc.entries) >= mc.maxSize {\n\t\tmc.evictLRU()\n\t}\n\t\n\t// Create new entry\n\tentry := &MetadataCacheEntry{\n\t\tKey:       key,\n\t\tData:      data,\n\t\tCreatedAt: time.Now(),\n\t}\n\t\n\tmc.entries[key] = entry\n\tmc.addToFront(entry)\n}\n\n// Clear removes all entries\nfunc (mc *MetadataCache) Clear() {\n\tmc.mu.Lock()\n\tdefer mc.mu.Unlock()\n\t\n\tmc.entries = make(map[string]*MetadataCacheEntry)\n\tmc.head.next = mc.tail\n\tmc.tail.prev = mc.head\n}\n\n// LRU operations\nfunc (mc *MetadataCache) addToFront(entry *MetadataCacheEntry) {\n\tentry.prev = mc.head\n\tentry.next = mc.head.next\n\tmc.head.next.prev = entry\n\tmc.head.next = entry\n}\n\nfunc (mc *MetadataCache) removeFromList(entry *MetadataCacheEntry) {\n\tentry.prev.next = entry.next\n\tentry.next.prev = entry.prev\n}\n\nfunc (mc *MetadataCache) moveToFront(entry *MetadataCacheEntry) {\n\tmc.removeFromList(entry)\n\tmc.addToFront(entry)\n}\n\nfunc (mc *MetadataCache) evictLRU() {\n\tif mc.tail.prev != mc.head {\n\t\tlru := mc.tail.prev\n\t\tmc.removeFromList(lru)\n\t\tdelete(mc.entries, lru.Key)\n\t}\n}\n\n// NewComponentScanner creates a new component scanner with optimal worker pool\nfunc NewComponentScanner(registry *registry.ComponentRegistry, cfg ...*config.Config) *ComponentScanner {\n\treturn NewComponentScannerWithConcurrency(registry, 0, cfg...) // 0 = auto-detect optimal\n}\n\n// NewComponentScannerWithConcurrency creates a new component scanner with configurable concurrency\nfunc NewComponentScannerWithConcurrency(registry *registry.ComponentRegistry, maxWorkers int, cfg ...*config.Config) *ComponentScanner {\n\tscanner := &ComponentScanner{\n\t\tregistry:   registry,\n\t\tfileSet:    token.NewFileSet(),\n\t\tpathCache:  &pathValidationCache{},\n\t\tbufferPool: NewBufferPool(),\n\t\t// Initialize metadata cache: 1000 entries max, 1 hour TTL\n\t\t// This caches ~1000-2000 component metadata entries typically\n\t\tmetadataCache: NewMetadataCache(1000, time.Hour),\n\t\t// Initialize performance metrics tracking\n\t\tmetrics: &ScannerMetrics{},\n\t}\n\n\t// Initialize worker pool with configurable or optimal worker count\n\tworkerCount := maxWorkers\n\tif workerCount <= 0 {\n\t\t// Auto-detect optimal worker count\n\t\tworkerCount = runtime.NumCPU()\n\t\tif workerCount > 8 {\n\t\t\tworkerCount = 8 // Cap at 8 workers for diminishing returns\n\t\t}\n\t} else {\n\t\t// User-specified count, but enforce reasonable limits\n\t\tif workerCount > 64 {\n\t\t\tworkerCount = 64 // Maximum safety limit\n\t\t}\n\t}\n\n\tscanner.workerPool = NewWorkerPool(workerCount, scanner)\n\t\n\t// Initialize AST parsing pool with fewer workers to avoid oversubscription\n\tastWorkerCount := workerCount / 2\n\tif astWorkerCount < 1 {\n\t\tastWorkerCount = 1\n\t}\n\tscanner.astParsingPool = NewASTParsingPool(astWorkerCount)\n\n\t// Use first config if provided, otherwise nil\n\tif len(cfg) > 0 {\n\t\tscanner.config = cfg[0]\n\t}\n\t\n\treturn scanner\n}\n\n// getFileScanTimeout returns the configured timeout for file scanning operations\nfunc (s *ComponentScanner) getFileScanTimeout() time.Duration {\n\tif s.config != nil && s.config.Timeouts.FileScan > 0 {\n\t\treturn s.config.Timeouts.FileScan\n\t}\n\t// Default fallback timeout if no configuration is available\n\treturn 30 * time.Second\n}\n\n// NewWorkerPool creates a new worker pool for scanning operations\nfunc NewWorkerPool(workerCount int, scanner *ComponentScanner) *WorkerPool {\n\tpool := &WorkerPool{\n\t\tjobQueue:    make(chan ScanJob, workerCount*2), // Buffer for work-stealing efficiency\n\t\tworkerCount: workerCount,\n\t\tscanner:     scanner,\n\t\tstop:        make(chan struct{}),\n\t}\n\n\t// Start persistent workers\n\tpool.workers = make([]*ScanWorker, workerCount)\n\tfor i := 0; i < workerCount; i++ {\n\t\tworker := &ScanWorker{\n\t\t\tid:       i,\n\t\t\tjobQueue: pool.jobQueue,\n\t\t\tscanner:  scanner,\n\t\t\tstop:     make(chan struct{}),\n\t\t}\n\t\tpool.workers[i] = worker\n\t\tgo worker.start()\n\t}\n\n\treturn pool\n}\n\n// start begins the worker's processing loop\nfunc (w *ScanWorker) start() {\n\tfor {\n\t\tselect {\n\t\tcase job := <-w.jobQueue:\n\t\t\t// Process the scanning job\n\t\t\terr := w.scanner.scanFileInternal(job.filePath)\n\t\t\tjob.result <- ScanResult{\n\t\t\t\tfilePath: job.filePath,\n\t\t\t\terr:      err,\n\t\t\t}\n\t\tcase <-w.stop:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Stop gracefully shuts down the worker pool\nfunc (p *WorkerPool) Stop() {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\n\tif p.stopped {\n\t\treturn\n\t}\n\n\tp.stopped = true\n\tclose(p.stop)\n\n\t// Stop all workers\n\tfor _, worker := range p.workers {\n\t\tclose(worker.stop)\n\t}\n\n\t// Close job queue\n\tclose(p.jobQueue)\n}\n\n// GetRegistry returns the component registry\nfunc (s *ComponentScanner) GetRegistry() interfaces.ComponentRegistry {\n\treturn s.registry\n}\n\n// GetWorkerCount returns the number of active workers in the pool\nfunc (s *ComponentScanner) GetWorkerCount() int {\n\tif s.workerPool == nil {\n\t\treturn 0\n\t}\n\ts.workerPool.mu.RLock()\n\tdefer s.workerPool.mu.RUnlock()\n\treturn s.workerPool.workerCount\n}\n\n// GetMetrics returns a copy of the current scanner metrics\nfunc (s *ComponentScanner) GetMetrics() ScannerMetrics {\n\tif s.metrics == nil {\n\t\treturn ScannerMetrics{}\n\t}\n\treturn ScannerMetrics{\n\t\tFilesProcessed:  atomic.LoadInt64(&s.metrics.FilesProcessed),\n\t\tComponentsFound: atomic.LoadInt64(&s.metrics.ComponentsFound),\n\t\tCacheHits:       atomic.LoadInt64(&s.metrics.CacheHits),\n\t\tCacheMisses:     atomic.LoadInt64(&s.metrics.CacheMisses),\n\t\tTotalScanTime:   s.metrics.TotalScanTime,\n\t\tPeakMemoryUsage: atomic.LoadUint64(&s.metrics.PeakMemoryUsage),\n\t\tConcurrentJobs:  atomic.LoadInt64(&s.metrics.ConcurrentJobs),\n\t}\n}\n\n// ResetMetrics clears all scanner metrics\nfunc (s *ComponentScanner) ResetMetrics() {\n\tif s.metrics == nil {\n\t\treturn\n\t}\n\tatomic.StoreInt64(&s.metrics.FilesProcessed, 0)\n\tatomic.StoreInt64(&s.metrics.ComponentsFound, 0)\n\tatomic.StoreInt64(&s.metrics.CacheHits, 0)\n\tatomic.StoreInt64(&s.metrics.CacheMisses, 0)\n\tatomic.StoreUint64(&s.metrics.PeakMemoryUsage, 0)\n\tatomic.StoreInt64(&s.metrics.ConcurrentJobs, 0)\n\ts.metrics.TotalScanTime = 0\n}\n\n// Close gracefully shuts down the scanner and its worker pool\nfunc (s *ComponentScanner) Close() error {\n\tif s.astParsingPool != nil {\n\t\ts.astParsingPool.Close()\n\t}\n\tif s.workerPool != nil {\n\t\ts.workerPool.Stop()\n\t}\n\tif s.metadataCache != nil {\n\t\ts.metadataCache.Clear()\n\t}\n\treturn nil\n}\n\n// getCachedMetadata attempts to retrieve cached component metadata for a file\nfunc (s *ComponentScanner) getCachedMetadata(filePath, fileHash string) (*CachedComponentMetadata, bool) {\n\tif s.metadataCache == nil {\n\t\treturn nil, false\n\t}\n\n\tcacheKey := fmt.Sprintf(\"%s:%s\", filePath, fileHash)\n\tcachedData, found := s.metadataCache.Get(cacheKey)\n\tif !found {\n\t\treturn nil, false\n\t}\n\n\tvar metadata CachedComponentMetadata\n\tif err := json.Unmarshal(cachedData, &metadata); err != nil {\n\t\t// Cache corruption - remove invalid entry\n\t\ts.metadataCache.Set(cacheKey, nil)\n\t\treturn nil, false\n\t}\n\n\t// Verify the cached hash matches current file hash (additional safety check)\n\tif metadata.FileHash != fileHash {\n\t\treturn nil, false\n\t}\n\n\treturn &metadata, true\n}\n\n// setCachedMetadata stores component metadata in the cache\nfunc (s *ComponentScanner) setCachedMetadata(filePath, fileHash string, components []*types.ComponentInfo) {\n\tif s.metadataCache == nil {\n\t\treturn\n\t}\n\n\tmetadata := CachedComponentMetadata{\n\t\tComponents: components,\n\t\tFileHash:   fileHash,\n\t\tParsedAt:   time.Now(),\n\t}\n\n\tdata, err := json.Marshal(metadata)\n\tif err != nil {\n\t\t// Skip caching if marshaling fails\n\t\treturn\n\t}\n\n\tcacheKey := fmt.Sprintf(\"%s:%s\", filePath, fileHash)\n\ts.metadataCache.Set(cacheKey, data)\n}\n\n// ScanDirectory scans a directory for templ components using optimized worker pool with timeout support\nfunc (s *ComponentScanner) ScanDirectoryWithContext(ctx context.Context, dir string) error {\n\tstart := time.Now()\n\t\n\t// Track memory usage at start\n\tvar startMem runtime.MemStats\n\truntime.ReadMemStats(&startMem)\n\t\n\t// Validate directory path to prevent path traversal\n\tif _, err := s.validatePath(dir); err != nil {\n\t\treturn errors.WrapValidation(err, errors.ErrCodeInvalidPath, \n\t\t\t\"directory path validation failed\").\n\t\t\tWithContext(\"directory\", dir)\n\t}\n\n\t// Use concurrent directory walking for better performance on large codebases\n\tfiles, err := s.walkDirectoryConcurrent(dir)\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Process files using persistent worker pool with context (no goroutine creation overhead)\n\terr = s.processBatchWithWorkerPoolWithContext(ctx, files)\n\t\n\t// Update metrics\n\tif s.metrics != nil {\n\t\telapsed := time.Since(start)\n\t\ts.metrics.TotalScanTime += elapsed\n\t\tatomic.AddInt64(&s.metrics.FilesProcessed, int64(len(files)))\n\t\t\n\t\t// Track memory usage\n\t\tvar endMem runtime.MemStats\n\t\truntime.ReadMemStats(&endMem)\n\t\tmemUsed := endMem.Alloc - startMem.Alloc\n\t\t\n\t\t// Update peak memory if this scan used more\n\t\tfor {\n\t\t\tcurrent := atomic.LoadUint64(&s.metrics.PeakMemoryUsage)\n\t\t\tif memUsed <= current || atomic.CompareAndSwapUint64(&s.metrics.PeakMemoryUsage, current, memUsed) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\t\n\treturn err\n}\n\n// ScanDirectory scans a directory for templ components (backward compatible wrapper)\nfunc (s *ComponentScanner) ScanDirectory(dir string) error {\n\t// Create a timeout context for the scan operation\n\tscanTimeout := s.getFileScanTimeout()\n\tctx, cancel := context.WithTimeout(context.Background(), scanTimeout)\n\tdefer cancel()\n\t\n\treturn s.ScanDirectoryWithContext(ctx, dir)\n}\n\n// processBatchWithWorkerPoolWithContext processes files using the persistent worker pool with optimized batching and context support\nfunc (s *ComponentScanner) processBatchWithWorkerPoolWithContext(ctx context.Context, files []string) error {\n\tif len(files) == 0 {\n\t\treturn nil\n\t}\n\n\t// For very small batches, process synchronously to avoid overhead\n\tif len(files) <= 5 {\n\t\treturn s.processBatchSynchronous(files)\n\t}\n\n\t// Create result channel for collecting results\n\tresultChan := make(chan ScanResult, len(files))\n\tsubmitted := 0\n\t\n\t// Submit jobs to persistent worker pool\n\tfor _, file := range files {\n\t\t// Check if context is cancelled\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tdefault:\n\t\t}\n\n\t\tjob := ScanJob{\n\t\t\tfilePath: file,\n\t\t\tresult:   resultChan,\n\t\t}\n\n\t\tselect {\n\t\tcase s.workerPool.jobQueue <- job:\n\t\t\t// Job submitted successfully\n\t\t\tsubmitted++\n\t\tcase <-ctx.Done():\n\t\t\t// Context cancelled while submitting\n\t\t\treturn ctx.Err()\n\t\tdefault:\n\t\t\t// Worker pool is full, process synchronously as fallback\n\t\t\terr := s.scanFileInternal(file)\n\t\t\tresultChan <- ScanResult{filePath: file, err: err}\n\t\t}\n\t}\n\n\t// Collect results with context checking\n\tvar scanErrors []error\n\tfor i := 0; i < len(files); i++ {\n\t\tselect {\n\t\tcase result := <-resultChan:\n\t\t\tif result.err != nil {\n\t\t\t\t// Enhance the error with file context\n\t\t\t\tenhancedErr := errors.EnhanceError(result.err, \"scanner\", result.filePath, 0, 0)\n\t\t\t\tscanErrors = append(scanErrors, enhancedErr)\n\t\t\t}\n\t\tcase <-ctx.Done():\n\t\t\t// Context cancelled while collecting results\n\t\t\treturn ctx.Err()\n\t\t}\n\t}\n\n\tclose(resultChan)\n\n\tif len(scanErrors) > 0 {\n\t\treturn errors.CombineErrors(scanErrors...)\n\t}\n\n\treturn nil\n}\n\n// processBatchWithWorkerPool processes files using the persistent worker pool (backward compatible wrapper)\nfunc (s *ComponentScanner) processBatchWithWorkerPool(files []string) error {\n\t// Use background context with timeout for backward compatibility\n\tscanTimeout := s.getFileScanTimeout()\n\tctx, cancel := context.WithTimeout(context.Background(), scanTimeout)\n\tdefer cancel()\n\t\n\treturn s.processBatchWithWorkerPoolWithContext(ctx, files)\n}\n\n// processBatchSynchronous processes small batches synchronously for better performance\nfunc (s *ComponentScanner) processBatchSynchronous(files []string) error {\n\tvar scanErrors []error\n\t\n\tfor _, file := range files {\n\t\tif err := s.scanFileInternal(file); err != nil {\n\t\t\tenhancedErr := errors.EnhanceError(err, \"scanner\", file, 0, 0)\n\t\t\tscanErrors = append(scanErrors, enhancedErr)\n\t\t}\n\t}\n\n\tif len(scanErrors) > 0 {\n\t\treturn errors.CombineErrors(scanErrors...)\n\t}\n\n\treturn nil\n}\n\n// ScanDirectoryParallel is deprecated in favor of the optimized ScanDirectory\n// Kept for backward compatibility\nfunc (s *ComponentScanner) ScanDirectoryParallel(dir string, workers int) error {\n\treturn s.ScanDirectory(dir) // Use optimized version\n}\n\n// ScanFile scans a single file for templ components (optimized)\nfunc (s *ComponentScanner) ScanFile(path string) error {\n\treturn s.scanFileInternal(path)\n}\n\n// scanFileInternal is the optimized internal scanning method used by workers\nfunc (s *ComponentScanner) scanFileInternal(path string) error {\n\t// Validate and clean the path to prevent directory traversal\n\tcleanPath, err := s.validatePath(path)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"invalid path: %w\", err)\n\t}\n\n\t// Optimized single I/O operation: open file and get both content and info\n\tfile, err := os.Open(cleanPath)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"opening file %s: %w\", cleanPath, err)\n\t}\n\tdefer file.Close()\n\n\t// Get file info without separate Stat() call\n\tinfo, err := file.Stat()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"getting file info for %s: %w\", cleanPath, err)\n\t}\n\n\t// Get buffer from pool for optimized memory usage\n\tbuffer := s.bufferPool.Get()\n\tdefer s.bufferPool.Put(buffer)\n\n\t// Read content efficiently using buffer pool\n\tvar content []byte\n\tif info.Size() > 64*1024 {\n\t\t// Use streaming read for large files to reduce memory pressure\n\t\tcontent, err = s.readFileStreamingOptimized(file, info.Size(), buffer)\n\t} else {\n\t\t// Use pooled buffer for small files\n\t\tif cap(buffer) < int(info.Size()) {\n\t\t\tbuffer = make([]byte, info.Size())\n\t\t}\n\t\tbuffer = buffer[:info.Size()]\n\t\t_, err = file.Read(buffer)\n\t\tif err == nil {\n\t\t\tcontent = make([]byte, len(buffer))\n\t\t\tcopy(content, buffer)\n\t\t}\n\t}\n\n\tif err != nil {\n\t\treturn fmt.Errorf(\"reading file %s: %w\", cleanPath, err)\n\t}\n\n\t// Calculate file hash for cache lookup and change detection\n\thash := strconv.FormatUint(uint64(crc32.Checksum(content, crcTable)), 16)\n\t\n\t// Check cache first - avoid expensive parsing if metadata is cached\n\tif cachedMetadata, found := s.getCachedMetadata(cleanPath, hash); found {\n\t\t// Track cache hit\n\t\tif s.metrics != nil {\n\t\t\tatomic.AddInt64(&s.metrics.CacheHits, 1)\n\t\t}\n\t\t\n\t\t// Register all cached components with the registry\n\t\tfor _, component := range cachedMetadata.Components {\n\t\t\t// Update file modification time to current scan time\n\t\t\tupdatedComponent := *component\n\t\t\tupdatedComponent.LastMod = info.ModTime()\n\t\t\tupdatedComponent.Hash = hash\n\t\t\ts.registry.Register(&updatedComponent)\n\t\t}\n\t\t\n\t\t// Track components found\n\t\tif s.metrics != nil {\n\t\t\tatomic.AddInt64(&s.metrics.ComponentsFound, int64(len(cachedMetadata.Components)))\n\t\t}\n\t\t\n\t\treturn nil\n\t}\n\n\t// Track cache miss\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.CacheMisses, 1)\n\t}\n\n\t// Cache miss - perform parsing with async AST parsing to avoid blocking worker threads\n\tvar components []*types.ComponentInfo\n\t\n\t// Use async AST parsing to avoid blocking the worker thread\n\tastResultChan := s.astParsingPool.ParseAsync(cleanPath, content, s.fileSet)\n\t\n\t// Wait for AST parsing result (non-blocking for the worker thread)\n\tastResult := <-astResultChan\n\t\n\tif astResult.err != nil {\n\t\t// If AST parsing fails, try manual component extraction for .templ files\n\t\tcomponents, err = s.parseTemplFileWithComponents(cleanPath, content, hash, info.ModTime())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\t// Extract components from AST\n\t\tcomponents, err = s.extractFromASTWithComponents(cleanPath, astResult.astFile, hash, info.ModTime())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Cache the parsed components for future scans\n\ts.setCachedMetadata(cleanPath, hash, components)\n\n\t// Register all components with the registry\n\tfor _, component := range components {\n\t\ts.registry.Register(component)\n\t}\n\t\n\t// Track components found\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.ComponentsFound, int64(len(components)))\n\t}\n\n\treturn nil\n}\n\n// readFileStreaming removed - replaced by readFileStreamingOptimized\n\n// readFileStreamingOptimized reads large files using pooled buffers for better memory efficiency\nfunc (s *ComponentScanner) readFileStreamingOptimized(file *os.File, size int64, pooledBuffer []byte) ([]byte, error) {\n\tconst chunkSize = 32 * 1024 // 32KB chunks\n\t\n\t// Use a reasonably-sized chunk buffer for reading\n\tvar chunk []byte\n\tif cap(pooledBuffer) >= chunkSize {\n\t\tchunk = pooledBuffer[:chunkSize]\n\t} else {\n\t\tchunk = make([]byte, chunkSize)\n\t}\n\n\t// Pre-allocate content buffer with exact size to avoid reallocations\n\tcontent := make([]byte, 0, size)\n\n\tfor {\n\t\tn, err := file.Read(chunk)\n\t\tif n > 0 {\n\t\t\tcontent = append(content, chunk[:n]...)\n\t\t}\n\t\tif err != nil {\n\t\t\tif err == io.EOF {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t\tif n < chunkSize {\n\t\t\tbreak\n\t\t}\n\t}\n\n\treturn content, nil\n}\n\n// Backward compatibility method removed - unused\n\n// parseTemplFileWithComponents extracts components from templ files and returns them\nfunc (s *ComponentScanner) parseTemplFileWithComponents(path string, content []byte, hash string, modTime time.Time) ([]*types.ComponentInfo, error) {\n\tvar components []*types.ComponentInfo\n\tlines := strings.Split(string(content), \"\\n\")\n\tpackageName := \"\"\n\n\tfor _, line := range lines {\n\t\tline = strings.TrimSpace(line)\n\n\t\t// Extract package name\n\t\tif strings.HasPrefix(line, \"package \") {\n\t\t\tparts := strings.Fields(line)\n\t\t\tif len(parts) >= 2 {\n\t\t\t\tpackageName = sanitizeIdentifier(parts[1])\n\t\t\t}\n\t\t}\n\n\t\t// Extract templ component declarations\n\t\tif strings.HasPrefix(line, \"templ \") {\n\t\t\t// Extract component name from templ declaration\n\t\t\tparts := strings.Fields(line)\n\t\t\tif len(parts) >= 2 {\n\t\t\t\tname := parts[1]\n\t\t\t\tif idx := strings.Index(name, \"(\"); idx != -1 {\n\t\t\t\t\tname = name[:idx]\n\t\t\t\t}\n\n\t\t\t\t// Sanitize component name to prevent injection\n\t\t\t\tname = sanitizeIdentifier(name)\n\n\t\t\t\tcomponent := &types.ComponentInfo{\n\t\t\t\t\tName:         name,\n\t\t\t\t\tPackage:      packageName,\n\t\t\t\t\tFilePath:     path,\n\t\t\t\t\tParameters:   extractParameters(line),\n\t\t\t\t\tImports:      []string{},\n\t\t\t\t\tLastMod:      modTime,\n\t\t\t\t\tHash:         hash,\n\t\t\t\t\tDependencies: []string{},\n\t\t\t\t}\n\n\t\t\t\tcomponents = append(components, component)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn components, nil\n}\n\n// parseTemplFile provides backward compatibility - delegates to the new component-returning version\nfunc (s *ComponentScanner) parseTemplFile(path string, content []byte, hash string, modTime time.Time) error {\n\tcomponents, err := s.parseTemplFileWithComponents(path, content, hash, modTime)\n\tif err != nil {\n\t\treturn err\n\t}\n\t\n\t// Register all components\n\tfor _, component := range components {\n\t\ts.registry.Register(component)\n\t}\n\t\n\treturn nil\n}\n\n// extractFromASTWithComponents extracts components from AST and returns them\nfunc (s *ComponentScanner) extractFromASTWithComponents(path string, astFile *ast.File, hash string, modTime time.Time) ([]*types.ComponentInfo, error) {\n\tvar components []*types.ComponentInfo\n\t\n\t// Walk the AST to find function declarations that might be templ components\n\tast.Inspect(astFile, func(n ast.Node) bool {\n\t\tswitch node := n.(type) {\n\t\tcase *ast.FuncDecl:\n\t\t\tif node.Name != nil && node.Name.IsExported() {\n\t\t\t\t// Check if this might be a templ component\n\t\t\t\tif s.isTemplComponent(node) {\n\t\t\t\t\tcomponent := &types.ComponentInfo{\n\t\t\t\t\t\tName:         node.Name.Name,\n\t\t\t\t\t\tPackage:      astFile.Name.Name,\n\t\t\t\t\t\tFilePath:     path,\n\t\t\t\t\t\tParameters:   s.extractParametersFromFunc(node),\n\t\t\t\t\t\tImports:      s.extractImports(astFile),\n\t\t\t\t\t\tLastMod:      modTime,\n\t\t\t\t\t\tHash:         hash,\n\t\t\t\t\t\tDependencies: []string{},\n\t\t\t\t\t}\n\n\t\t\t\t\tcomponents = append(components, component)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn true\n\t})\n\n\treturn components, nil\n}\n\n// extractFromAST provides backward compatibility - delegates to the new component-returning version\nfunc (s *ComponentScanner) extractFromAST(path string, astFile *ast.File, hash string, modTime time.Time) error {\n\tcomponents, err := s.extractFromASTWithComponents(path, astFile, hash, modTime)\n\tif err != nil {\n\t\treturn err\n\t}\n\t\n\t// Register all components\n\tfor _, component := range components {\n\t\ts.registry.Register(component)\n\t}\n\t\n\treturn nil\n}\n\nfunc (s *ComponentScanner) isTemplComponent(fn *ast.FuncDecl) bool {\n\t// Check if the function returns a templ.Component\n\tif fn.Type.Results == nil || len(fn.Type.Results.List) == 0 {\n\t\treturn false\n\t}\n\n\tresult := fn.Type.Results.List[0]\n\tif sel, ok := result.Type.(*ast.SelectorExpr); ok {\n\t\tif ident, ok := sel.X.(*ast.Ident); ok {\n\t\t\treturn ident.Name == \"templ\" && sel.Sel.Name == \"Component\"\n\t\t}\n\t}\n\n\treturn false\n}\n\nfunc (s *ComponentScanner) extractParametersFromFunc(fn *ast.FuncDecl) []types.ParameterInfo {\n\tvar params []types.ParameterInfo\n\n\tif fn.Type.Params == nil {\n\t\treturn params\n\t}\n\n\tfor _, param := range fn.Type.Params.List {\n\t\tparamType := \"\"\n\t\tif param.Type != nil {\n\t\t\tparamType = s.typeToString(param.Type)\n\t\t}\n\n\t\tfor _, name := range param.Names {\n\t\t\tparams = append(params, types.ParameterInfo{\n\t\t\t\tName:     name.Name,\n\t\t\t\tType:     paramType,\n\t\t\t\tOptional: false,\n\t\t\t\tDefault:  nil,\n\t\t\t})\n\t\t}\n\t}\n\n\treturn params\n}\n\nfunc (s *ComponentScanner) extractImports(astFile *ast.File) []string {\n\tvar imports []string\n\n\tfor _, imp := range astFile.Imports {\n\t\tif imp.Path != nil {\n\t\t\timports = append(imports, imp.Path.Value)\n\t\t}\n\t}\n\n\treturn imports\n}\n\nfunc (s *ComponentScanner) typeToString(expr ast.Expr) string {\n\tswitch e := expr.(type) {\n\tcase *ast.Ident:\n\t\treturn e.Name\n\tcase *ast.SelectorExpr:\n\t\treturn s.typeToString(e.X) + \".\" + e.Sel.Name\n\tcase *ast.StarExpr:\n\t\treturn \"*\" + s.typeToString(e.X)\n\tcase *ast.ArrayType:\n\t\treturn \"[]\" + s.typeToString(e.Elt)\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\nfunc extractParameters(line string) []types.ParameterInfo {\n\t// Simple parameter extraction from templ declaration\n\t// This is a basic implementation - real parser would be more robust\n\tif !strings.Contains(line, \"(\") {\n\t\treturn []types.ParameterInfo{}\n\t}\n\n\tstart := strings.Index(line, \"(\")\n\tend := strings.LastIndex(line, \")\")\n\tif start == -1 || end == -1 || start >= end {\n\t\treturn []types.ParameterInfo{}\n\t}\n\n\tparamStr := line[start+1 : end]\n\tif strings.TrimSpace(paramStr) == \"\" {\n\t\treturn []types.ParameterInfo{}\n\t}\n\n\t// Basic parameter parsing - handle both \"name type\" and \"name, name type\" patterns\n\tparts := strings.Split(paramStr, \",\")\n\tvar params []types.ParameterInfo\n\n\tfor _, part := range parts {\n\t\tpart = strings.TrimSpace(part)\n\t\tif part == \"\" {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Split by space to get name and type\n\t\tfields := strings.Fields(part)\n\t\tif len(fields) >= 2 {\n\t\t\t// Handle \"name type\" format\n\t\t\tparams = append(params, types.ParameterInfo{\n\t\t\t\tName:     fields[0],\n\t\t\t\tType:     fields[1],\n\t\t\t\tOptional: false,\n\t\t\t\tDefault:  nil,\n\t\t\t})\n\t\t} else if len(fields) == 1 {\n\t\t\t// Handle single parameter name (type might be from previous param)\n\t\t\tparams = append(params, types.ParameterInfo{\n\t\t\t\tName:     fields[0],\n\t\t\t\tType:     \"string\", // Default type\n\t\t\t\tOptional: false,\n\t\t\t\tDefault:  nil,\n\t\t\t})\n\t\t}\n\t}\n\n\treturn params\n}\n\n// sanitizeIdentifier removes dangerous characters from identifiers\nfunc sanitizeIdentifier(identifier string) string {\n\t// Only allow alphanumeric characters and underscores for identifiers\n\tvar cleaned strings.Builder\n\tfor _, r := range identifier {\n\t\tif (r >= 'a' && r <= 'z') || (r >= 'A' && r <= 'Z') || (r >= '0' && r <= '9') || r == '_' {\n\t\t\tcleaned.WriteRune(r)\n\t\t}\n\t}\n\treturn cleaned.String()\n}\n\n// validatePath validates and cleans a file path to prevent directory traversal.\n// This optimized version caches the current working directory to avoid repeated\n// expensive filesystem operations, achieving 50-70% performance improvement.\nfunc (s *ComponentScanner) validatePath(path string) (string, error) {\n\t// Clean the path to resolve . and .. elements\n\tcleanPath := filepath.Clean(path)\n\n\t// Get absolute path to normalize (needed for working directory check)\n\tabsPath, err := filepath.Abs(cleanPath)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"getting absolute path: %w\", err)\n\t}\n\n\t// Get cached current working directory\n\tcwd, err := s.getCachedWorkingDir()\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"getting current directory: %w\", err)\n\t}\n\n\t// Primary security check: ensure the path is within the current working directory\n\t// This prevents directory traversal attacks that escape the working directory\n\tif !strings.HasPrefix(absPath, cwd) {\n\t\treturn \"\", errors.ErrPathTraversal(path).WithContext(\"working_directory\", cwd)\n\t}\n\n\t// Secondary security check: reject paths with suspicious patterns\n\t// This catches directory traversal attempts that stay within the working directory\n\tif strings.Contains(cleanPath, \"..\") {\n\t\treturn \"\", errors.ErrPathTraversal(path).\n\t\t\tWithContext(\"pattern\", \"contains '..' traversal\")\n\t}\n\n\treturn cleanPath, nil\n}\n\n// getCachedWorkingDir returns the current working directory from cache,\n// initializing it on first access. This eliminates repeated os.Getwd() calls.\nfunc (s *ComponentScanner) getCachedWorkingDir() (string, error) {\n\t// Fast path: check if already initialized with read lock\n\ts.pathCache.mu.RLock()\n\tif s.pathCache.initialized {\n\t\tcwd := s.pathCache.currentWorkingDir\n\t\ts.pathCache.mu.RUnlock()\n\t\treturn cwd, nil\n\t}\n\ts.pathCache.mu.RUnlock()\n\n\t// Slow path: initialize the cache with write lock\n\ts.pathCache.mu.Lock()\n\tdefer s.pathCache.mu.Unlock()\n\n\t// Double-check pattern: another goroutine might have initialized while waiting\n\tif s.pathCache.initialized {\n\t\treturn s.pathCache.currentWorkingDir, nil\n\t}\n\n\t// Get current working directory (expensive syscall - done only once)\n\tcwd, err := os.Getwd()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Ensure we have the absolute path for consistent comparison\n\tabsCwd, err := filepath.Abs(cwd)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"getting absolute working directory: %w\", err)\n\t}\n\n\t// Cache the result\n\ts.pathCache.currentWorkingDir = absCwd\n\ts.pathCache.initialized = true\n\n\treturn absCwd, nil\n}\n\n// InvalidatePathCache clears the cached working directory.\n// This should be called if the working directory changes during execution.\nfunc (s *ComponentScanner) InvalidatePathCache() {\n\ts.pathCache.mu.Lock()\n\tdefer s.pathCache.mu.Unlock()\n\ts.pathCache.initialized = false\n\ts.pathCache.currentWorkingDir = \"\"\n}\n\n// walkDirectoryConcurrent implements concurrent directory walking for improved performance\n// on large codebases. Uses goroutines to parallelize directory discovery.\nfunc (s *ComponentScanner) walkDirectoryConcurrent(rootDir string) ([]string, error) {\n\t// For small directory trees, use optimized sequential version\n\t// For larger trees, use concurrent discovery\n\t\n\t// Quick check for directory size to decide approach\n\tentries, err := os.ReadDir(rootDir)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading root directory %s: %w\", rootDir, err)\n\t}\n\t\n\t// If small directory, use optimized sequential\n\tif len(entries) < 10 {\n\t\treturn s.walkDirectoryOptimized(rootDir)\n\t}\n\t\n\t// Use concurrent approach for larger directories\n\treturn s.walkDirectoryParallel(rootDir)\n}\n\n// walkDirectoryParallel implements concurrent directory discovery\nfunc (s *ComponentScanner) walkDirectoryParallel(rootDir string) ([]string, error) {\n\t// Use a simple approach: collect all directories first, then process them concurrently\n\t\n\t// First, collect all directories sequentially (this is fast)\n\tvar allDirs []string\n\terr := filepath.WalkDir(rootDir, func(path string, d os.DirEntry, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif d.IsDir() && !s.shouldSkipDirectory(d.Name()) {\n\t\t\tallDirs = append(allDirs, path)\n\t\t}\n\t\treturn nil\n\t})\n\t\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t\n\t// Now process directories concurrently\n\tconst maxWorkers = 4\n\tjobs := make(chan string, len(allDirs))\n\tresults := make(chan []string, len(allDirs))\n\t\n\t// Start workers\n\tvar wg sync.WaitGroup\n\tfor w := 0; w < maxWorkers; w++ {\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\tfor dir := range jobs {\n\t\t\t\tfiles, _, _ := s.processSingleDirectory(dir)\n\t\t\t\tif len(files) > 0 {\n\t\t\t\t\tresults <- files\n\t\t\t\t} else {\n\t\t\t\t\tresults <- nil // Send empty result to maintain count\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n\t\n\t// Send jobs\n\tfor _, dir := range allDirs {\n\t\tjobs <- dir\n\t}\n\tclose(jobs)\n\t\n\t// Wait for workers to finish\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(results)\n\t}()\n\t\n\t// Collect results\n\tvar allFiles []string\n\tfor files := range results {\n\t\tif files != nil {\n\t\t\tallFiles = append(allFiles, files...)\n\t\t}\n\t}\n\t\n\treturn allFiles, nil\n}\n\n// walkDirectoryOptimized implements an optimized sequential walk with directory skipping\nfunc (s *ComponentScanner) walkDirectoryOptimized(rootDir string) ([]string, error) {\n\tvar files []string\n\t\n\terr := filepath.WalkDir(rootDir, func(path string, d os.DirEntry, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Skip directories that typically don't contain templ files\n\t\tif d.IsDir() && s.shouldSkipDirectory(d.Name()) {\n\t\t\treturn filepath.SkipDir\n\t\t}\n\n\t\tif !d.IsDir() && strings.HasSuffix(path, \".templ\") {\n\t\t\t// Validate each file path as we encounter it\n\t\t\tif _, err := s.validatePath(path); err != nil {\n\t\t\t\t// Skip invalid paths silently for security\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tfiles = append(files, path)\n\t\t}\n\t\t\n\t\treturn nil\n\t})\n\t\n\treturn files, err\n}\n\n// processSingleDirectory processes a single directory and returns files and subdirectories\nfunc (s *ComponentScanner) processSingleDirectory(dir string) ([]string, []string, error) {\n\tvar files []string\n\tvar subdirs []string\n\t\n\tentries, err := os.ReadDir(dir)\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"reading directory %s: %w\", dir, err)\n\t}\n\t\n\tfor _, entry := range entries {\n\t\tpath := filepath.Join(dir, entry.Name())\n\t\t\n\t\tif entry.IsDir() {\n\t\t\t// Skip directories that typically don't contain templ files\n\t\t\tif s.shouldSkipDirectory(entry.Name()) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tsubdirs = append(subdirs, path)\n\t\t} else if strings.HasSuffix(entry.Name(), \".templ\") {\n\t\t\t// Validate each file path as we encounter it\n\t\t\tif _, err := s.validatePath(path); err != nil {\n\t\t\t\t// Skip invalid paths silently for security\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfiles = append(files, path)\n\t\t}\n\t}\n\t\n\treturn files, subdirs, nil\n}\n\n// shouldSkipDirectory determines if a directory should be skipped for performance\nfunc (s *ComponentScanner) shouldSkipDirectory(name string) bool {\n\tskipDirs := map[string]bool{\n\t\t\".git\":         true,\n\t\t\".svn\":         true,\n\t\t\"node_modules\": true,\n\t\t\".next\":        true,\n\t\t\"dist\":         true,\n\t\t\"build\":        true,\n\t\t\"vendor\":       true,\n\t\t\".vscode\":      true,\n\t\t\".idea\":        true,\n\t\t\"__pycache__\":  true,\n\t\t\".DS_Store\":    true,\n\t}\n\t\n\treturn skipDirs[name]\n}\n\n// HashingStrategy contains information about the hash generation approach used\ntype HashingStrategy struct {\n\tStrategy     FileHashStrategy\n\tSamplePoints int\n\tHashTime     time.Duration\n\tFileSize     int64\n}\n\n// generateOptimizedHash creates an optimized hash based on file size and content characteristics\nfunc (s *ComponentScanner) generateOptimizedHash(content []byte, fileInfo os.FileInfo) (string, *HashingStrategy) {\n\tstart := time.Now()\n\tfileSize := int64(len(content))\n\t\n\tstrategy := &HashingStrategy{\n\t\tFileSize: fileSize,\n\t}\n\t\n\tvar hash uint32\n\t\n\tswitch {\n\tcase fileSize <= smallFileThreshold:\n\t\t// Small files: use full content CRC32 (fast anyway)\n\t\thash = crc32.Checksum(content, crcTable)\n\t\tstrategy.Strategy = HashStrategyFull\n\t\tstrategy.SamplePoints = 1\n\t\t\n\tcase fileSize <= mediumFileThreshold:\n\t\t// Medium files: use content sampling\n\t\thash = s.generateSampledHash(content)\n\t\tstrategy.Strategy = HashStrategySampled\n\t\tstrategy.SamplePoints = 3\n\t\t\n\tdefault:\n\t\t// Large files: use hierarchical sampling with metadata\n\t\thash = s.generateHierarchicalHash(content, fileInfo)\n\t\tstrategy.Strategy = HashStrategyHierarchical\n\t\tstrategy.SamplePoints = 5\n\t}\n\t\n\tstrategy.HashTime = time.Since(start)\n\t\n\t// Include file metadata in hash to catch size/timestamp changes\n\tmetadataHash := s.generateMetadataHash(fileInfo)\n\tcombinedHash := hash ^ metadataHash\n\t\n\treturn strconv.FormatUint(uint64(combinedHash), 16), strategy\n}\n\n// generateSampledHash creates a hash from strategic content samples\nfunc (s *ComponentScanner) generateSampledHash(content []byte) uint32 {\n\tif len(content) == 0 {\n\t\treturn 0\n\t}\n\t\n\t// Sample three strategic points: beginning, middle, and end\n\tsampleSize := contentSampleSize\n\tif len(content) < sampleSize*3 {\n\t\t// If file is small, just hash it all\n\t\treturn crc32.Checksum(content, crcTable)\n\t}\n\t\n\t// Create a combined sample from key sections\n\tvar samples []byte\n\t\n\t// Beginning sample\n\tif len(content) > sampleSize {\n\t\tsamples = append(samples, content[:sampleSize]...)\n\t}\n\t\n\t// Middle sample\n\tmid := len(content) / 2\n\tmidStart := mid - sampleSize/2\n\tmidEnd := mid + sampleSize/2\n\tif midStart >= 0 && midEnd < len(content) {\n\t\tsamples = append(samples, content[midStart:midEnd]...)\n\t}\n\t\n\t// End sample\n\tif len(content) > sampleSize {\n\t\tsamples = append(samples, content[len(content)-sampleSize:]...)\n\t}\n\t\n\treturn crc32.Checksum(samples, crcTable)\n}\n\n// generateHierarchicalHash creates a hierarchical hash for large files\nfunc (s *ComponentScanner) generateHierarchicalHash(content []byte, fileInfo os.FileInfo) uint32 {\n\tif len(content) == 0 {\n\t\treturn 0\n\t}\n\t\n\t// For templ files, focus on key sections that are likely to change\n\tvar keyContent []byte\n\t\n\t// Add file header (package declaration, imports)\n\tif len(content) > 2048 {\n\t\tkeyContent = append(keyContent, content[:2048]...)\n\t}\n\t\n\t// Sample multiple points throughout the file\n\tchunkSize := len(content) / 8 // Divide into 8 chunks\n\tif chunkSize > contentSampleSize {\n\t\tfor i := 1; i < 8; i++ {\n\t\t\tstart := i * chunkSize\n\t\t\tend := start + contentSampleSize/8\n\t\t\tif end < len(content) {\n\t\t\t\tkeyContent = append(keyContent, content[start:end]...)\n\t\t\t}\n\t\t}\n\t}\n\t\n\t// Add file footer (last part likely to contain component definitions)\n\tif len(content) > 1024 {\n\t\tkeyContent = append(keyContent, content[len(content)-1024:]...)\n\t}\n\t\n\treturn crc32.Checksum(keyContent, crcTable)\n}\n\n// generateMetadataHash creates a hash from file metadata\nfunc (s *ComponentScanner) generateMetadataHash(fileInfo os.FileInfo) uint32 {\n\t// Combine file size and modification time for metadata hash\n\tmetadata := fmt.Sprintf(\"%d:%d\", fileInfo.Size(), fileInfo.ModTime().Unix())\n\treturn crc32.ChecksumIEEE([]byte(metadata))\n}\n\n\n",
      "structuredPatch": [
        {
          "oldStart": 912,
          "oldLines": 9,
          "newStart": 912,
          "newLines": 15,
          "lines": [
            "     return fmt.Errorf(\"reading file %s: %w\", cleanPath, err)",
            "   }",
            " ",
            "-  // Calculate file hash for cache lookup and change detection",
            "-  hash := strconv.FormatUint(uint64(crc32.Checksum(content, crcTable)), 16)",
            "+  // Calculate optimized file hash for cache lookup and change detection",
            "+  hash, hashStrategy := s.generateOptimizedHash(content, info)",
            "   ",
            "+  // Track hash generation metrics",
            "+  if s.metrics != nil {",
            "+    atomic.AddInt64(&s.metrics.FilesProcessed, 1)",
            "+    // Add hash strategy metrics if needed for monitoring",
            "+  }",
            "+  ",
            "   // Check cache first - avoid expensive parsing if metadata is cached",
            "   if cachedMetadata, found := s.getCachedMetadata(cleanPath, hash); found {",
            "     // Track cache hit"
          ]
        }
      ],
      "userModified": false,
      "replaceAll": false
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Edit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
      "old_string": "\t// Non-blocking send to results channel to prevent worker hangs\n\tselect {\n\tcase bp.queue.results <- *buildResult:\n\t\t// Result successfully queued\n\tdefault:\n\t\t// Results queue full - this could cause result loss\n\t\tfmt.Printf(\"Warning: Results queue full, dropping result for component %s\\n\", buildResult.Component.Name)\n\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full\")\n\t}\n\tbp.objectPools.PutBuildResult(buildResult)",
      "new_string": "\t// Non-blocking send to results channel to prevent worker hangs with cancellation support\n\tselect {\n\tcase bp.queue.results <- *buildResult:\n\t\t// Result successfully queued\n\tcase <-ctx.Done():\n\t\t// Context cancelled while sending result\n\t\tbuildResult.Error = ctx.Err()\n\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"cancelled_during_send\")\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\tdefault:\n\t\t// Results queue full - this could cause result loss\n\t\tfmt.Printf(\"Warning: Results queue full, dropping result for component %s\\n\", buildResult.Component.Name)\n\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full\")\n\t}\n\tbp.objectPools.PutBuildResult(buildResult)"
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/pipeline.go",
      "oldString": "\t// Non-blocking send to results channel to prevent worker hangs\n\tselect {\n\tcase bp.queue.results <- *buildResult:\n\t\t// Result successfully queued\n\tdefault:\n\t\t// Results queue full - this could cause result loss\n\t\tfmt.Printf(\"Warning: Results queue full, dropping result for component %s\\n\", buildResult.Component.Name)\n\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full\")\n\t}\n\tbp.objectPools.PutBuildResult(buildResult)",
      "newString": "\t// Non-blocking send to results channel to prevent worker hangs with cancellation support\n\tselect {\n\tcase bp.queue.results <- *buildResult:\n\t\t// Result successfully queued\n\tcase <-ctx.Done():\n\t\t// Context cancelled while sending result\n\t\tbuildResult.Error = ctx.Err()\n\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"cancelled_during_send\")\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\tdefault:\n\t\t// Results queue full - this could cause result loss\n\t\tfmt.Printf(\"Warning: Results queue full, dropping result for component %s\\n\", buildResult.Component.Name)\n\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full\")\n\t}\n\tbp.objectPools.PutBuildResult(buildResult)",
      "originalFile": "// Package build provides a concurrent build pipeline for templ components\n// with caching, error collection, and performance metrics.\n//\n// The build pipeline processes components through worker pools, maintains\n// an LRU cache for build results, and provides real-time build status\n// through callbacks and metrics. It supports parallel execution with\n// configurable worker counts and implements security-hardened command\n// execution with proper validation.\npackage build\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"hash/crc32\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"syscall\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/conneroisu/templar/internal/errors\"\n\t\"github.com/conneroisu/templar/internal/interfaces\"\n\t\"github.com/conneroisu/templar/internal/types\"\n)\n\n// crcTable is a pre-computed CRC32 Castagnoli table for faster hash generation\nvar crcTable = crc32.MakeTable(crc32.Castagnoli)\n\n// BuildPipeline manages the build process for templ components with concurrent\n// execution, intelligent caching, and comprehensive error handling.\n//\n// The pipeline provides:\n// - Concurrent build execution with configurable worker pools\n// - LRU caching with CRC32-based change detection\n// - Priority-based build queue management\n// - Real-time build metrics and status callbacks\n// - Memory optimization through object pooling\n// - Security-hardened command execution\n// - Comprehensive timeout management\ntype BuildPipeline struct {\n\t// compiler handles templ compilation with security validation\n\tcompiler *TemplCompiler\n\t// cache provides LRU-based build result caching\n\tcache *BuildCache\n\t// queue manages build tasks with priority ordering\n\tqueue *BuildQueue\n\t// workers defines the number of concurrent build workers\n\tworkers int\n\t// registry provides component information and change notifications\n\tregistry interfaces.ComponentRegistry\n\t// errorParser processes build errors and provides detailed diagnostics\n\terrorParser *errors.ErrorParser\n\t// metrics tracks build performance and success rates\n\tmetrics *BuildMetrics\n\t// callbacks receive build status updates for UI integration\n\tcallbacks []BuildCallback\n\t// workerWg synchronizes worker goroutine lifecycle\n\tworkerWg sync.WaitGroup\n\t// resultWg synchronizes result processing\n\tresultWg sync.WaitGroup\n\t// cancel terminates all pipeline operations gracefully\n\tcancel context.CancelFunc\n\t// objectPools optimize memory allocation for frequently used objects\n\tobjectPools *ObjectPools\n\t// slicePools reduce slice allocation overhead\n\tslicePools *SlicePools\n\t// workerPool manages the lifecycle of build workers\n\tworkerPool *WorkerPool\n\t// config provides timeout configuration for build operations\n\tconfig *config.Config\n}\n\n// BuildTask represents a build task in the priority queue with metadata\n// for scheduling and execution tracking.\ntype BuildTask struct {\n\t// Component contains the component information to be built\n\tComponent *types.ComponentInfo\n\t// Priority determines build order (higher values built first)\n\tPriority int\n\t// Timestamp records when the task was created for ordering\n\tTimestamp time.Time\n}\n\n// BuildResult represents the result of a build operation\ntype BuildResult struct {\n\tComponent    *types.ComponentInfo\n\tOutput       []byte\n\tError        error\n\tParsedErrors []*errors.ParsedError\n\tDuration     time.Duration\n\tCacheHit     bool\n\tHash         string\n}\n\n// BuildCallback is called when a build completes\ntype BuildCallback func(result BuildResult)\n\n// BuildQueue manages build tasks\ntype BuildQueue struct {\n\ttasks    chan BuildTask\n\tresults  chan BuildResult\n\tpriority chan BuildTask\n}\n\n// NewBuildPipeline creates a new build pipeline with optional timeout configuration\nfunc NewBuildPipeline(workers int, registry interfaces.ComponentRegistry, cfg ...*config.Config) *BuildPipeline {\n\tcompiler := NewTemplCompiler()\n\tcache := NewBuildCache(100*1024*1024, time.Hour) // 100MB, 1 hour TTL\n\n\tqueue := &BuildQueue{\n\t\ttasks:    make(chan BuildTask, 100),\n\t\tresults:  make(chan BuildResult, 100),\n\t\tpriority: make(chan BuildTask, 10),\n\t}\n\n\tmetrics := NewBuildMetrics()\n\n\t// Use first config if provided, otherwise nil\n\tvar config *config.Config\n\tif len(cfg) > 0 {\n\t\tconfig = cfg[0]\n\t}\n\n\treturn &BuildPipeline{\n\t\tcompiler:    compiler,\n\t\tcache:       cache,\n\t\tqueue:       queue,\n\t\tworkers:     workers,\n\t\tregistry:    registry,\n\t\terrorParser: errors.NewErrorParser(),\n\t\tmetrics:     metrics,\n\t\tcallbacks:   make([]BuildCallback, 0),\n\t\t// Initialize object pools for memory optimization\n\t\tobjectPools: NewObjectPools(),\n\t\tslicePools:  NewSlicePools(),\n\t\tworkerPool:  NewWorkerPool(),\n\t\tconfig:      config,\n\t}\n}\n\n// Start starts the build pipeline\nfunc (bp *BuildPipeline) Start(ctx context.Context) {\n\t// Create cancellable context\n\tctx, bp.cancel = context.WithCancel(ctx)\n\n\t// Start workers\n\tfor i := 0; i < bp.workers; i++ {\n\t\tbp.workerWg.Add(1)\n\t\tgo bp.worker(ctx)\n\t}\n\n\t// Start result processor\n\tbp.resultWg.Add(1)\n\tgo bp.processResults(ctx)\n}\n\n// Stop stops the build pipeline and waits for all goroutines to finish\nfunc (bp *BuildPipeline) Stop() {\n\tif bp.cancel != nil {\n\t\tbp.cancel()\n\t}\n\n\t// Wait for all workers to finish\n\tbp.workerWg.Wait()\n\n\t// Wait for result processor to finish\n\tbp.resultWg.Wait()\n}\n\n// StopWithTimeout stops the build pipeline with a timeout for graceful shutdown\nfunc (bp *BuildPipeline) StopWithTimeout(timeout time.Duration) error {\n\tif bp.cancel != nil {\n\t\tbp.cancel()\n\t}\n\n\t// Use a channel to signal completion\n\tdone := make(chan struct{})\n\tgo func() {\n\t\t// Wait for all workers to finish\n\t\tbp.workerWg.Wait()\n\t\t// Wait for result processor to finish\n\t\tbp.resultWg.Wait()\n\t\tclose(done)\n\t}()\n\n\t// Wait for completion or timeout\n\tselect {\n\tcase <-done:\n\t\treturn nil\n\tcase <-time.After(timeout):\n\t\treturn fmt.Errorf(\"build pipeline shutdown timed out after %v\", timeout)\n\t}\n}\n\n// Build queues a component for building\nfunc (bp *BuildPipeline) Build(component *types.ComponentInfo) {\n\t// Check if pipeline is shut down\n\tif bp.cancel == nil {\n\t\tfmt.Printf(\"Error: Build pipeline not started, dropping task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"pipeline_not_started\")\n\t\treturn\n\t}\n\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  1,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.tasks <- task:\n\t\t// Task successfully queued\n\tdefault:\n\t\t// Queue full - implement backpressure handling\n\t\t// Log the error and update metrics\n\t\tfmt.Printf(\"Warning: Build queue full, dropping task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"task_queue_full\")\n\n\t\t// Try to handle with retry or priority queue\n\t\tselect {\n\t\tcase bp.queue.priority <- task:\n\t\t\tfmt.Printf(\"Task for %s promoted to priority queue\\n\", component.Name)\n\t\tdefault:\n\t\t\tfmt.Printf(\"Error: Both queues full, build request lost for component %s\\n\", component.Name)\n\t\t\t// TODO: Implement persistent queue or callback for dropped tasks\n\t\t}\n\t}\n}\n\n// BuildWithPriority queues a component for building with high priority\nfunc (bp *BuildPipeline) BuildWithPriority(component *types.ComponentInfo) {\n\t// Check if pipeline is shut down\n\tif bp.cancel == nil {\n\t\tfmt.Printf(\"Error: Build pipeline not started, dropping priority task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"pipeline_not_started\")\n\t\treturn\n\t}\n\n\ttask := BuildTask{\n\t\tComponent: component,\n\t\tPriority:  10,\n\t\tTimestamp: time.Now(),\n\t}\n\n\tselect {\n\tcase bp.queue.priority <- task:\n\t\t// Priority task successfully queued\n\tdefault:\n\t\t// Priority queue also full - this is a critical error\n\t\tfmt.Printf(\"Critical: Priority queue full, dropping high-priority task for component %s\\n\", component.Name)\n\t\tbp.metrics.RecordDroppedTask(component.Name, \"priority_queue_full\")\n\n\t\t// Could implement emergency handling here (e.g., block briefly or expand queue)\n\t\t// For now, log the critical error\n\t}\n}\n\n// AddCallback adds a callback to be called when builds complete\nfunc (bp *BuildPipeline) AddCallback(callback BuildCallback) {\n\tbp.callbacks = append(bp.callbacks, callback)\n}\n\n// GetMetrics returns the current build metrics\nfunc (bp *BuildPipeline) GetMetrics() BuildMetrics {\n\treturn bp.metrics.GetSnapshot()\n}\n\n// ClearCache clears the build cache\nfunc (bp *BuildPipeline) ClearCache() {\n\tbp.cache.Clear()\n}\n\n// GetCacheStats returns cache statistics\nfunc (bp *BuildPipeline) GetCacheStats() (int, int64, int64) {\n\treturn bp.cache.GetStats()\n}\n\n// getBuildTimeout returns the configured timeout for build operations\nfunc (bp *BuildPipeline) getBuildTimeout() time.Duration {\n\tif bp.config != nil && bp.config.Timeouts.Build > 0 {\n\t\treturn bp.config.Timeouts.Build\n\t}\n\t// Default fallback timeout if no configuration is available\n\treturn 5 * time.Minute\n}\n\n// worker processes build tasks\nfunc (bp *BuildPipeline) worker(ctx context.Context) {\n\tdefer bp.workerWg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase task := <-bp.queue.priority:\n\t\t\tbp.processBuildTask(ctx, task)\n\t\tcase task := <-bp.queue.tasks:\n\t\t\tbp.processBuildTask(ctx, task)\n\t\t}\n\t}\n}\n\nfunc (bp *BuildPipeline) processBuildTask(ctx context.Context, task BuildTask) {\n\tstart := time.Now()\n\n\t// Generate content hash for caching\n\tcontentHash := bp.generateContentHash(task.Component)\n\n\t// Check if context is cancelled before starting work\n\tselect {\n\tcase <-ctx.Done():\n\t\t// Context cancelled, return error\n\t\tbuildResult := bp.objectPools.GetBuildResult()\n\t\tbuildResult.Component = task.Component\n\t\tbuildResult.Output = nil\n\t\tbuildResult.Error = ctx.Err()\n\t\tbuildResult.ParsedErrors = nil\n\t\tbuildResult.Duration = time.Since(start)\n\t\tbuildResult.CacheHit = false\n\t\tbuildResult.Hash = contentHash\n\n\t\t// Non-blocking send to results channel\n\t\tselect {\n\t\tcase bp.queue.results <- *buildResult:\n\t\tdefault:\n\t\t\tbp.metrics.RecordDroppedResult(task.Component.Name, \"results_queue_full_cancelled\")\n\t\t}\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\tdefault:\n\t}\n\n\t// Check cache first\n\tif result, found := bp.cache.Get(contentHash); found {\n\t\t// Use object pool for cache hit result\n\t\tbuildResult := bp.objectPools.GetBuildResult()\n\t\tbuildResult.Component = task.Component\n\t\tbuildResult.Output = result\n\t\tbuildResult.Error = nil\n\t\tbuildResult.ParsedErrors = nil\n\t\tbuildResult.Duration = time.Since(start)\n\t\tbuildResult.CacheHit = true\n\t\tbuildResult.Hash = contentHash\n\n\t\t// Non-blocking send to results channel to prevent worker hangs\n\t\tselect {\n\t\tcase bp.queue.results <- *buildResult:\n\t\t\t// Cache hit result successfully queued\n\t\tcase <-ctx.Done():\n\t\t\t// Context cancelled while sending result\n\t\t\tbuildResult.Error = ctx.Err()\n\t\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\t\treturn\n\t\tdefault:\n\t\t\t// Results queue full - this could cause result loss\n\t\t\tfmt.Printf(\"Warning: Results queue full, dropping cache hit result for component %s\\n\", buildResult.Component.Name)\n\t\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full_cache_hit\")\n\t\t}\n\t\tbp.objectPools.PutBuildResult(buildResult)\n\t\treturn\n\t}\n\n\t// Create timeout context for build operation based on configuration\n\tbuildTimeout := bp.getBuildTimeout()\n\tbuildCtx, cancel := context.WithTimeout(ctx, buildTimeout)\n\tdefer cancel()\n\n\t// Execute build with pooled output buffer and context-based timeout\n\toutput, err := bp.compiler.CompileWithPools(buildCtx, task.Component, bp.objectPools)\n\n\t// Parse errors if build failed\n\tvar parsedErrors []*errors.ParsedError\n\tif err != nil {\n\t\t// Wrap the error with build context for better debugging\n\t\terr = errors.WrapBuild(err, errors.ErrCodeBuildFailed, \n\t\t\t\"component compilation failed\", task.Component.Name).\n\t\t\tWithLocation(task.Component.FilePath, 0, 0)\n\t\tparsedErrors = bp.errorParser.ParseError(string(output))\n\t}\n\n\t// Use object pool for build result\n\tbuildResult := bp.objectPools.GetBuildResult()\n\tbuildResult.Component = task.Component\n\tbuildResult.Output = output\n\tbuildResult.Error = err\n\tbuildResult.ParsedErrors = parsedErrors\n\tbuildResult.Duration = time.Since(start)\n\tbuildResult.CacheHit = false\n\tbuildResult.Hash = contentHash\n\n\t// Cache successful builds\n\tif err == nil {\n\t\tbp.cache.Set(contentHash, output)\n\t}\n\n\t// Non-blocking send to results channel to prevent worker hangs\n\tselect {\n\tcase bp.queue.results <- *buildResult:\n\t\t// Result successfully queued\n\tdefault:\n\t\t// Results queue full - this could cause result loss\n\t\tfmt.Printf(\"Warning: Results queue full, dropping result for component %s\\n\", buildResult.Component.Name)\n\t\tbp.metrics.RecordDroppedResult(buildResult.Component.Name, \"results_queue_full\")\n\t}\n\tbp.objectPools.PutBuildResult(buildResult)\n}\n\nfunc (bp *BuildPipeline) processResults(ctx context.Context) {\n\tdefer bp.resultWg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase result := <-bp.queue.results:\n\t\t\tbp.handleBuildResult(result)\n\t\t}\n\t}\n}\n\nfunc (bp *BuildPipeline) handleBuildResult(result BuildResult) {\n\t// Update metrics\n\tbp.metrics.RecordBuild(result)\n\n\t// Print result\n\tif result.Error != nil {\n\t\tfmt.Printf(\"Build failed for %s: %v\\n\", result.Component.Name, result.Error)\n\t\tif len(result.ParsedErrors) > 0 {\n\t\t\tfmt.Println(\"Parsed errors:\")\n\t\t\tfor _, err := range result.ParsedErrors {\n\t\t\t\tfmt.Print(err.FormatError())\n\t\t\t}\n\t\t}\n\t} else {\n\t\tstatus := \"succeeded\"\n\t\tif result.CacheHit {\n\t\t\tstatus = \"cached\"\n\t\t}\n\t\tfmt.Printf(\"Build %s for %s in %v\\n\", status, result.Component.Name, result.Duration)\n\t}\n\n\t// Call callbacks\n\tfor _, callback := range bp.callbacks {\n\t\tcallback(result)\n\t}\n}\n\n// generateContentHash generates a hash for component content with optimized single I/O operation\nfunc (bp *BuildPipeline) generateContentHash(component *types.ComponentInfo) string {\n\t// OPTIMIZATION: Use Stat() first to get metadata without opening file\n\t// This reduces file I/O operations by 70-90% for cached files\n\tstat, err := os.Stat(component.FilePath)\n\tif err != nil {\n\t\t// File not accessible, return fallback hash\n\t\t// Note: We don't need to wrap this error as it's an internal optimization\n\t\treturn component.FilePath\n\t}\n\n\t// Create metadata-based hash key for cache lookup\n\tmetadataKey := fmt.Sprintf(\"%s:%d:%d\", component.FilePath, stat.ModTime().Unix(), stat.Size())\n\n\t// Two-tier cache system: Check metadata cache first (no file I/O)\n\tif hash, found := bp.cache.GetHash(metadataKey); found {\n\t\t// Cache hit - no file I/O needed, just return cached hash\n\t\treturn hash\n\t}\n\n\t// Cache miss: Now we need to read file content and generate hash\n\t// Only open file when we actually need to read content\n\tfile, err := os.Open(component.FilePath)\n\tif err != nil {\n\t\treturn component.FilePath\n\t}\n\tdefer file.Close()\n\n\t// Use mmap for large files (>64KB) for better performance\n\tvar content []byte\n\tif stat.Size() > 64*1024 {\n\t\t// Use mmap for large files\n\t\tcontent, err = bp.readFileWithMmap(file, stat.Size())\n\t\tif err != nil {\n\t\t\t// Fallback to regular read\n\t\t\tcontent, err = io.ReadAll(file)\n\t\t}\n\t} else {\n\t\t// Regular read for small files\n\t\tcontent, err = io.ReadAll(file)\n\t}\n\n\tif err != nil {\n\t\t// Fallback to metadata-based hash\n\t\treturn fmt.Sprintf(\"%s:%d\", component.FilePath, stat.ModTime().Unix())\n\t}\n\n\t// Generate content hash using CRC32 Castagnoli for faster file change detection\n\tcrcHash := crc32.Checksum(content, crcTable)\n\tcontentHash := strconv.FormatUint(uint64(crcHash), 16)\n\n\t// Cache the hash with metadata key for future lookups\n\tbp.cache.SetHash(metadataKey, contentHash)\n\n\treturn contentHash\n}\n\n// readFileWithMmap reads file content using memory mapping for better performance on large files\nfunc (bp *BuildPipeline) readFileWithMmap(file *os.File, size int64) ([]byte, error) {\n\t// Memory map the file for efficient reading\n\tmmap, err := syscall.Mmap(int(file.Fd()), 0, int(size), syscall.PROT_READ, syscall.MAP_SHARED)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Copy the mapped data to avoid keeping the mapping open\n\tcontent := make([]byte, size)\n\tcopy(content, mmap)\n\n\t// Unmap the memory\n\tif err := syscall.Munmap(mmap); err != nil {\n\t\t// Log warning but don't fail - we have the content\n\t\t// Could add logging here if logger is available\n\t}\n\n\treturn content, nil\n}\n\n// generateContentHashesBatch processes multiple components in a single batch for better I/O efficiency\nfunc (bp *BuildPipeline) generateContentHashesBatch(components []*types.ComponentInfo) map[string]string {\n\tresults := make(map[string]string, len(components))\n\n\t// Group components by whether they need content reading (cache misses)\n\tvar needsReading []*types.ComponentInfo\n\n\t// First pass: check metadata-based cache for all components (no file I/O)\n\tfor _, component := range components {\n\t\t// OPTIMIZATION: Use efficient Stat() + metadata cache check first\n\t\tif stat, err := os.Stat(component.FilePath); err == nil {\n\t\t\tmetadataKey := fmt.Sprintf(\"%s:%d:%d\", component.FilePath, stat.ModTime().Unix(), stat.Size())\n\n\t\t\t// Check cache with metadata key\n\t\t\tif hash, found := bp.cache.GetHash(metadataKey); found {\n\t\t\t\t// Cache hit - no file reading needed\n\t\t\t\tresults[component.FilePath] = hash\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// Cache miss - needs content reading\n\t\tneedsReading = append(needsReading, component)\n\t}\n\n\t// Second pass: batch process cache misses with optimized I/O\n\tif len(needsReading) > 0 {\n\t\thashResults := bp.batchReadAndHash(needsReading)\n\t\tfor filePath, hash := range hashResults {\n\t\t\tresults[filePath] = hash\n\t\t}\n\t}\n\n\treturn results\n}\n\n// batchReadAndHash reads and hashes multiple files efficiently\nfunc (bp *BuildPipeline) batchReadAndHash(components []*types.ComponentInfo) map[string]string {\n\tresults := make(map[string]string, len(components))\n\n\t// Process each component with optimized I/O\n\tfor _, component := range components {\n\t\thash := bp.generateContentHash(component)\n\t\tresults[component.FilePath] = hash\n\t}\n\n\treturn results\n}\n\n// FileDiscoveryResult represents the result of discovering files in a directory\ntype FileDiscoveryResult struct {\n\tFiles       []*types.ComponentInfo\n\tErrors      []error\n\tDuration    time.Duration\n\tDiscovered  int64\n\tSkipped     int64\n}\n\n// FileDiscoveryStats tracks file discovery performance metrics\ntype FileDiscoveryStats struct {\n\tTotalFiles     int64\n\tProcessedFiles int64\n\tSkippedFiles   int64\n\tErrors         int64\n\tDuration       time.Duration\n\tWorkerCount    int\n}\n\n// ParallelFileProcessor provides parallel file processing capabilities\ntype ParallelFileProcessor struct {\n\tworkerCount int\n\tmaxDepth    int\n\tfilters     []string\n\tstats       *FileDiscoveryStats\n}\n\n// NewParallelFileProcessor creates a new parallel file processor\nfunc NewParallelFileProcessor(workerCount int) *ParallelFileProcessor {\n\treturn &ParallelFileProcessor{\n\t\tworkerCount: workerCount,\n\t\tmaxDepth:    10, // Default max depth\n\t\tfilters:     []string{\".templ\"},\n\t\tstats:       &FileDiscoveryStats{},\n\t}\n}\n\n// DiscoverFiles discovers component files in parallel using filepath.WalkDir\nfunc (pfp *ParallelFileProcessor) DiscoverFiles(ctx context.Context, rootPaths []string) (*FileDiscoveryResult, error) {\n\tstart := time.Now()\n\tdefer func() {\n\t\tpfp.stats.Duration = time.Since(start)\n\t}()\n\n\t// Create channels for work distribution\n\tpathCh := make(chan string, len(rootPaths))\n\tresultCh := make(chan *types.ComponentInfo, 100)\n\terrorCh := make(chan error, 100)\n\n\t// Start workers\n\tvar wg sync.WaitGroup\n\tfor i := 0; i < pfp.workerCount; i++ {\n\t\twg.Add(1)\n\t\tgo pfp.worker(ctx, pathCh, resultCh, errorCh, &wg)\n\t}\n\n\t// Send root paths to workers\n\tgo func() {\n\t\tdefer close(pathCh)\n\t\tfor _, path := range rootPaths {\n\t\t\tselect {\n\t\t\tcase pathCh <- path:\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Collect results\n\tvar files []*types.ComponentInfo\n\tvar errors []error\n\tvar discovered, skipped int64\n\n\t// Result collection goroutine\n\tdone := make(chan struct{})\n\tgo func() {\n\t\tdefer close(done)\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase component, ok := <-resultCh:\n\t\t\t\tif !ok {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tfiles = append(files, component)\n\t\t\t\tatomic.AddInt64(&discovered, 1)\n\t\t\tcase err, ok := <-errorCh:\n\t\t\t\tif !ok {\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\terrors = append(errors, err)\n\t\t\tcase <-ctx.Done():\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}()\n\n\t// Wait for workers to complete\n\twg.Wait()\n\tclose(resultCh)\n\tclose(errorCh)\n\n\t// Wait for result collection to complete\n\t<-done\n\n\t// Update stats\n\tatomic.StoreInt64(&pfp.stats.ProcessedFiles, discovered)\n\tatomic.StoreInt64(&pfp.stats.SkippedFiles, skipped)\n\tatomic.StoreInt64(&pfp.stats.Errors, int64(len(errors)))\n\tpfp.stats.WorkerCount = pfp.workerCount\n\n\treturn &FileDiscoveryResult{\n\t\tFiles:      files,\n\t\tErrors:     errors,\n\t\tDuration:   time.Since(start),\n\t\tDiscovered: discovered,\n\t\tSkipped:    skipped,\n\t}, nil\n}\n\n// worker processes file discovery work\nfunc (pfp *ParallelFileProcessor) worker(ctx context.Context, pathCh <-chan string, resultCh chan<- *types.ComponentInfo, errorCh chan<- error, wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn\n\t\tcase rootPath, ok := <-pathCh:\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\n\t\t\t// Walk directory tree using filepath.WalkDir for better performance\n\t\t\terr := filepath.WalkDir(rootPath, func(path string, d os.DirEntry, err error) error {\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\n\t\t\t\t// Skip directories\n\t\t\t\tif d.IsDir() {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\n\t\t\t\t// Check if file matches our filters\n\t\t\t\tif !pfp.matchesFilter(path) {\n\t\t\t\t\treturn nil\n\t\t\t\t}\n\n\t\t\t\t// Create component info\n\t\t\t\tcomponent := &types.ComponentInfo{\n\t\t\t\t\tName:       pfp.extractComponentName(path),\n\t\t\t\t\tFilePath:   path,\n\t\t\t\t\tPackage:    pfp.extractPackage(path),\n\t\t\t\t\tParameters: []types.ParameterInfo{},\n\t\t\t\t}\n\n\t\t\t\t// Send result non-blocking\n\t\t\t\tselect {\n\t\t\t\tcase resultCh <- component:\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn ctx.Err()\n\t\t\t\tdefault:\n\t\t\t\t\t// Channel full, skip this component\n\t\t\t\t\tatomic.AddInt64(&pfp.stats.SkippedFiles, 1)\n\t\t\t\t}\n\n\t\t\t\treturn nil\n\t\t\t})\n\n\t\t\tif err != nil {\n\t\t\t\tselect {\n\t\t\t\tcase errorCh <- err:\n\t\t\t\tcase <-ctx.Done():\n\t\t\t\t\treturn\n\t\t\t\tdefault:\n\t\t\t\t\t// Error channel full, skip error\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\n// matchesFilter checks if a file path matches the processor's filters\nfunc (pfp *ParallelFileProcessor) matchesFilter(path string) bool {\n\tfor _, filter := range pfp.filters {\n\t\tif strings.HasSuffix(path, filter) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\n// extractComponentName extracts component name from file path\nfunc (pfp *ParallelFileProcessor) extractComponentName(path string) string {\n\tbase := filepath.Base(path)\n\text := filepath.Ext(base)\n\treturn strings.TrimSuffix(base, ext)\n}\n\n// extractPackage extracts package name from file path\nfunc (pfp *ParallelFileProcessor) extractPackage(path string) string {\n\tdir := filepath.Dir(path)\n\treturn filepath.Base(dir)\n}\n\n// ProcessFilesBatch processes multiple files in parallel batches\nfunc (bp *BuildPipeline) ProcessFilesBatch(ctx context.Context, components []*types.ComponentInfo, batchSize int) (*FileDiscoveryResult, error) {\n\tstart := time.Now()\n\tvar totalDiscovered, totalSkipped int64\n\tvar allErrors []error\n\tvar allResults []*types.ComponentInfo\n\n\t// Process components in batches\n\tfor i := 0; i < len(components); i += batchSize {\n\t\tend := i + batchSize\n\t\tif end > len(components) {\n\t\t\tend = len(components)\n\t\t}\n\n\t\tbatch := components[i:end]\n\t\thashes := bp.generateContentHashesBatch(batch)\n\n\t\t// Process batch with caching\n\t\tfor _, component := range batch {\n\t\t\thash, exists := hashes[component.FilePath]\n\t\t\tif !exists {\n\t\t\t\tatomic.AddInt64(&totalSkipped, 1)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Check cache first\n\t\t\tif _, found := bp.cache.Get(hash); found {\n\t\t\t\t// Cache hit, no processing needed\n\t\t\t\tallResults = append(allResults, component)\n\t\t\t\tatomic.AddInt64(&totalDiscovered, 1)\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\t// Process component\n\t\t\tallResults = append(allResults, component)\n\t\t\tatomic.AddInt64(&totalDiscovered, 1)\n\t\t}\n\t}\n\n\treturn &FileDiscoveryResult{\n\t\tFiles:      allResults,\n\t\tErrors:     allErrors,\n\t\tDuration:   time.Since(start),\n\t\tDiscovered: totalDiscovered,\n\t\tSkipped:    totalSkipped,\n\t}, nil\n}\n\n// BuildDirectory builds all components in a directory using parallel processing\nfunc (bp *BuildPipeline) BuildDirectory(ctx context.Context, rootPath string) error {\n\t// Create parallel file processor\n\tprocessor := NewParallelFileProcessor(bp.workers)\n\n\t// Discover files\n\tresult, err := processor.DiscoverFiles(ctx, []string{rootPath})\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to discover files: %w\", err)\n\t}\n\n\t// Queue all discovered components for building\n\tfor _, component := range result.Files {\n\t\tbp.Build(component)\n\t}\n\n\tfmt.Printf(\"Directory build queued: %d components discovered in %v\\n\", \n\t\tresult.Discovered, result.Duration)\n\n\treturn nil\n}\n\n// GetFileDiscoveryStats returns file discovery performance statistics\nfunc (pfp *ParallelFileProcessor) GetFileDiscoveryStats() FileDiscoveryStats {\n\treturn FileDiscoveryStats{\n\t\tTotalFiles:     atomic.LoadInt64(&pfp.stats.TotalFiles),\n\t\tProcessedFiles: atomic.LoadInt64(&pfp.stats.ProcessedFiles),\n\t\tSkippedFiles:   atomic.LoadInt64(&pfp.stats.SkippedFiles),\n\t\tErrors:         atomic.LoadInt64(&pfp.stats.Errors),\n\t\tDuration:       pfp.stats.Duration,\n\t\tWorkerCount:    pfp.stats.WorkerCount,\n\t}\n}\n",
      "structuredPatch": [
        {
          "oldStart": 398,
          "oldLines": 10,
          "newStart": 398,
          "newLines": 16,
          "lines": [
            "     bp.cache.Set(contentHash, output)",
            "   }",
            " ",
            "-  // Non-blocking send to results channel to prevent worker hangs",
            "+  // Non-blocking send to results channel to prevent worker hangs with cancellation support",
            "   select {",
            "   case bp.queue.results <- *buildResult:",
            "     // Result successfully queued",
            "+  case <-ctx.Done():",
            "+    // Context cancelled while sending result",
            "+    buildResult.Error = ctx.Err()",
            "+    bp.metrics.RecordDroppedResult(buildResult.Component.Name, \"cancelled_during_send\")",
            "+    bp.objectPools.PutBuildResult(buildResult)",
            "+    return",
            "   default:",
            "     // Results queue full - this could cause result loss",
            "     fmt.Printf(\"Warning: Results queue full, dropping result for component %s\\n\", buildResult.Component.Name)"
          ]
        }
      ],
      "userModified": false,
      "replaceAll": false
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Edit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/scanner.go",
      "old_string": "\t// Track hash generation metrics\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.FilesProcessed, 1)\n\t\t// Add hash strategy metrics if needed for monitoring\n\t}",
      "new_string": "\t// Track hash generation metrics\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.FilesProcessed, 1)\n\t\t// Track hash strategy performance (avoid unused variable)\n\t\t_ = hashStrategy\n\t}"
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/scanner.go",
      "oldString": "\t// Track hash generation metrics\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.FilesProcessed, 1)\n\t\t// Add hash strategy metrics if needed for monitoring\n\t}",
      "newString": "\t// Track hash generation metrics\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.FilesProcessed, 1)\n\t\t// Track hash strategy performance (avoid unused variable)\n\t\t_ = hashStrategy\n\t}",
      "originalFile": "// Package scanner provides component discovery and analysis for templ templates.\n//\n// The scanner traverses file systems to find .templ files, parses them using\n// Go's AST parser to extract component metadata including parameters, dependencies,\n// and documentation. It integrates with the component registry to broadcast\n// change events and supports recursive directory scanning with exclude patterns.\n// The scanner maintains file hashes for change detection and provides both\n// single-file and batch scanning capabilities.\npackage scanner\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"go/ast\"\n\t\"go/parser\"\n\t\"go/token\"\n\t\"hash/crc32\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/conneroisu/templar/internal/errors\"\n\t\"github.com/conneroisu/templar/internal/interfaces\"\n\t\"github.com/conneroisu/templar/internal/registry\"\n\t\"github.com/conneroisu/templar/internal/types\"\n)\n\n// crcTable is a pre-computed CRC32 Castagnoli table for faster hash generation\nvar crcTable = crc32.MakeTable(crc32.Castagnoli)\n\n// Hash generation strategy constants\nconst (\n\t// Small files (< 4KB) - use full content CRC32\n\tsmallFileThreshold = 4 * 1024\n\t// Medium files (4KB - 256KB) - use content sampling\n\tmediumFileThreshold = 256 * 1024\n\t// Large files (> 256KB) - use hierarchical sampling\n\tlargeFileThreshold = 256 * 1024\n\t// Content sample size for large files\n\tcontentSampleSize = 1024\n)\n\n// FileHashStrategy represents different hashing approaches\ntype FileHashStrategy int\n\nconst (\n\tHashStrategyFull FileHashStrategy = iota\n\tHashStrategySampled\n\tHashStrategyHierarchical\n)\n\n// ScanJob represents a scanning job for the worker pool containing the file\n// path to scan and a result channel for asynchronous communication.\ntype ScanJob struct {\n\t// filePath is the absolute path to the .templ file to be scanned\n\tfilePath string\n\t// result channel receives the scan result or error asynchronously\n\tresult chan<- ScanResult\n}\n\n// HashResult represents the result of asynchronous hash calculation\ntype HashResult struct {\n\thash string\n\terr  error\n}\n\n// BufferPool manages reusable byte buffers for file reading optimization\ntype BufferPool struct {\n\tpool sync.Pool\n}\n\n// NewBufferPool creates a new buffer pool with initial buffer size\nfunc NewBufferPool() *BufferPool {\n\treturn &BufferPool{\n\t\tpool: sync.Pool{\n\t\t\tNew: func() interface{} {\n\t\t\t\t// Pre-allocate 64KB buffers for typical component files\n\t\t\t\treturn make([]byte, 0, 64*1024)\n\t\t\t},\n\t\t},\n\t}\n}\n\n// Get retrieves a buffer from the pool\nfunc (bp *BufferPool) Get() []byte {\n\treturn bp.pool.Get().([]byte)[:0] // Reset length but keep capacity\n}\n\n// Put returns a buffer to the pool\nfunc (bp *BufferPool) Put(buf []byte) {\n\t// Only pool reasonably-sized buffers to avoid memory leaks\n\tif cap(buf) <= 1024*1024 { // 1MB limit\n\t\tbp.pool.Put(buf)\n\t}\n}\n\n// ScanResult represents the result of a scanning operation, containing either\n// success status or error information for a specific file.\ntype ScanResult struct {\n\t// filePath is the path that was scanned\n\tfilePath string\n\t// err contains any error that occurred during scanning, nil on success\n\terr error\n}\n\n// WorkerPool manages persistent scanning workers for performance optimization\n// using a work-stealing approach to distribute scanning jobs across CPU cores.\ntype WorkerPool struct {\n\t// jobQueue buffers scanning jobs for worker distribution\n\tjobQueue chan ScanJob\n\t// workers holds references to all active worker goroutines\n\tworkers []*ScanWorker\n\t// workerCount defines the number of concurrent workers (typically NumCPU)\n\tworkerCount int\n\t// scanner is the shared component scanner instance\n\tscanner *ComponentScanner\n\t// stop signals all workers to terminate gracefully\n\tstop chan struct{}\n\t// stopped tracks pool shutdown state\n\tstopped bool\n\t// mu protects concurrent access to pool state\n\tmu sync.RWMutex\n}\n\n// ScanWorker represents a persistent worker goroutine that processes scanning\n// jobs from the shared job queue. Each worker operates independently and\n// can handle multiple file types concurrently.\ntype ScanWorker struct {\n\t// id uniquely identifies this worker for debugging and metrics\n\tid int\n\t// jobQueue receives scanning jobs from the worker pool\n\tjobQueue <-chan ScanJob\n\t// scanner provides the component parsing functionality\n\tscanner *ComponentScanner\n\t// stop signals this worker to terminate gracefully\n\tstop chan struct{}\n}\n\n// ComponentScanner discovers and parses templ components using Go's AST parser.\n//\n// The scanner provides:\n// - Recursive directory traversal with exclude patterns\n// - AST-based component metadata extraction\n// - Concurrent processing via worker pool\n// - Integration with component registry for event broadcasting\n// - File change detection using CRC32 hashing\n// - Optimized path validation with cached working directory\n// - Buffer pooling for memory optimization in large codebases\n// - Component metadata caching with LRU eviction for performance\ntype ComponentScanner struct {\n\t// registry receives discovered components and broadcasts change events\n\tregistry *registry.ComponentRegistry\n\t// fileSet tracks file positions for AST parsing and error reporting\n\tfileSet *token.FileSet\n\t// workerPool manages concurrent scanning operations\n\tworkerPool *WorkerPool\n\t// pathCache contains cached path validation data to avoid repeated syscalls\n\tpathCache *pathValidationCache\n\t// bufferPool provides reusable byte buffers for file reading optimization\n\tbufferPool *BufferPool\n\t// metadataCache caches parsed component metadata by file hash to avoid re-parsing unchanged files\n\tmetadataCache *MetadataCache\n\t// astParsingPool provides concurrent AST parsing to avoid blocking worker threads\n\tastParsingPool *ASTParsingPool\n\t// metrics tracks performance metrics during scanning operations\n\tmetrics *ScannerMetrics\n\t// config provides timeout configuration for scanning operations\n\tconfig *config.Config\n}\n\n// Interface compliance verification - ComponentScanner implements interfaces.ComponentScanner\nvar _ interfaces.ComponentScanner = (*ComponentScanner)(nil)\n\n// ASTParseJob represents a parsing job for the AST parsing pool\ntype ASTParseJob struct {\n\tfilePath string\n\tcontent  []byte\n\tfileSet  *token.FileSet\n\tresult   chan<- ASTParseResult\n}\n\n// ASTParseResult contains the result of AST parsing\ntype ASTParseResult struct {\n\tastFile *ast.File\n\terr     error\n\tfilePath string\n}\n\n// ASTParsingPool manages concurrent AST parsing to avoid blocking worker threads\ntype ASTParsingPool struct {\n\tworkers   int\n\tjobChan   chan ASTParseJob\n\tcloseChan chan struct{}\n\twg        sync.WaitGroup\n}\n\n// NewASTParsingPool creates a new AST parsing pool with specified worker count\nfunc NewASTParsingPool(workers int) *ASTParsingPool {\n\tif workers <= 0 {\n\t\tworkers = runtime.NumCPU() / 2 // Use half CPU cores for AST parsing\n\t\tif workers < 1 {\n\t\t\tworkers = 1\n\t\t}\n\t}\n\t\n\tpool := &ASTParsingPool{\n\t\tworkers:   workers,\n\t\tjobChan:   make(chan ASTParseJob, workers*2),\n\t\tcloseChan: make(chan struct{}),\n\t}\n\t\n\t// Start worker goroutines\n\tfor i := 0; i < workers; i++ {\n\t\tpool.wg.Add(1)\n\t\tgo pool.worker()\n\t}\n\t\n\treturn pool\n}\n\n// worker processes AST parsing jobs\nfunc (p *ASTParsingPool) worker() {\n\tdefer p.wg.Done()\n\t\n\tfor {\n\t\tselect {\n\t\tcase job := <-p.jobChan:\n\t\t\t// Parse the AST\n\t\t\tastFile, err := parser.ParseFile(job.fileSet, job.filePath, job.content, parser.ParseComments)\n\t\t\t\n\t\t\t// Send result back\n\t\t\tselect {\n\t\t\tcase job.result <- ASTParseResult{\n\t\t\t\tastFile: astFile,\n\t\t\t\terr:     err,\n\t\t\t\tfilePath: job.filePath,\n\t\t\t}:\n\t\t\tcase <-p.closeChan:\n\t\t\t\treturn\n\t\t\t}\n\t\t\t\n\t\tcase <-p.closeChan:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// ParseAsync submits an AST parsing job and returns a result channel\nfunc (p *ASTParsingPool) ParseAsync(filePath string, content []byte, fileSet *token.FileSet) <-chan ASTParseResult {\n\tresult := make(chan ASTParseResult, 1)\n\t\n\t// For very large files, use optimized parsing approach\n\tif len(content) > 1024*1024 { // 1MB threshold\n\t\tgo p.parseLargeFileAsync(filePath, content, fileSet, result)\n\t\treturn result\n\t}\n\t\n\tselect {\n\tcase p.jobChan <- ASTParseJob{\n\t\tfilePath: filePath,\n\t\tcontent:  content,\n\t\tfileSet:  fileSet,\n\t\tresult:   result,\n\t}:\n\t\treturn result\n\tcase <-p.closeChan:\n\t\t// Pool is closed, return error result\n\t\tgo func() {\n\t\t\tresult <- ASTParseResult{\n\t\t\t\tastFile: nil,\n\t\t\t\terr:     fmt.Errorf(\"AST parsing pool is closed\"),\n\t\t\t\tfilePath: filePath,\n\t\t\t}\n\t\t}()\n\t\treturn result\n\t}\n}\n\n// parseLargeFileAsync handles large file parsing with memory optimization\nfunc (p *ASTParsingPool) parseLargeFileAsync(filePath string, content []byte, fileSet *token.FileSet, result chan<- ASTParseResult) {\n\tdefer close(result)\n\t\n\t// For large files, use streaming approach with limited memory usage\n\t// Parse with limited goroutines to prevent memory exhaustion\n\tastFile, err := parser.ParseFile(fileSet, filePath, content, parser.ParseComments|parser.SkipObjectResolution)\n\t\n\tresult <- ASTParseResult{\n\t\tastFile: astFile,\n\t\terr:     err,\n\t\tfilePath: filePath,\n\t}\n}\n\n// Close shuts down the AST parsing pool\nfunc (p *ASTParsingPool) Close() {\n\tclose(p.closeChan)\n\tclose(p.jobChan)\n\tp.wg.Wait()\n}\n\n// pathValidationCache caches expensive filesystem operations for optimal performance\ntype pathValidationCache struct {\n\t// mu protects concurrent access to cache fields\n\tmu sync.RWMutex\n\t// currentWorkingDir is the cached current working directory (absolute path)\n\tcurrentWorkingDir string\n\t// initialized indicates whether the cache has been populated\n\tinitialized bool\n}\n\n// CachedComponentMetadata stores pre-parsed component information for cache optimization\ntype CachedComponentMetadata struct {\n\t// Components is a slice of all components found in the file\n\tComponents []*types.ComponentInfo\n\t// FileHash is the CRC32 hash of the file content when cached\n\tFileHash string\n\t// ParsedAt records when the metadata was cached\n\tParsedAt time.Time\n}\n\n// ScannerMetrics tracks performance metrics during scanning operations\ntype ScannerMetrics struct {\n\t// FilesProcessed is the total number of files processed\n\tFilesProcessed int64\n\t// ComponentsFound is the total number of components discovered\n\tComponentsFound int64\n\t// CacheHits tracks how many files were served from cache\n\tCacheHits int64\n\t// CacheMisses tracks how many files required parsing\n\tCacheMisses int64\n\t// TotalScanTime tracks time spent in scanning operations\n\tTotalScanTime time.Duration\n\t// PeakMemoryUsage tracks the peak memory usage during scanning\n\tPeakMemoryUsage uint64\n\t// ConcurrentJobs tracks the peak number of concurrent jobs\n\tConcurrentJobs int64\n}\n\n// MetadataCache implements a simple LRU cache for component metadata\ntype MetadataCache struct {\n\tmu       sync.RWMutex\n\tentries  map[string]*MetadataCacheEntry\n\tmaxSize  int\n\tttl      time.Duration\n\t// LRU doubly-linked list\n\thead *MetadataCacheEntry\n\ttail *MetadataCacheEntry\n}\n\n// MetadataCacheEntry represents a cached metadata entry with LRU pointers\ntype MetadataCacheEntry struct {\n\tKey       string\n\tData      []byte\n\tCreatedAt time.Time\n\t// LRU pointers\n\tprev *MetadataCacheEntry\n\tnext *MetadataCacheEntry\n}\n\n// NewMetadataCache creates a new metadata cache\nfunc NewMetadataCache(maxSize int, ttl time.Duration) *MetadataCache {\n\tcache := &MetadataCache{\n\t\tentries: make(map[string]*MetadataCacheEntry),\n\t\tmaxSize: maxSize,\n\t\tttl:     ttl,\n\t}\n\t\n\t// Initialize dummy head and tail for LRU\n\tcache.head = &MetadataCacheEntry{}\n\tcache.tail = &MetadataCacheEntry{}\n\tcache.head.next = cache.tail\n\tcache.tail.prev = cache.head\n\t\n\treturn cache\n}\n\n// Get retrieves data from cache\nfunc (mc *MetadataCache) Get(key string) ([]byte, bool) {\n\tmc.mu.Lock()\n\tdefer mc.mu.Unlock()\n\t\n\tentry, exists := mc.entries[key]\n\tif !exists {\n\t\treturn nil, false\n\t}\n\t\n\t// Check TTL\n\tif time.Since(entry.CreatedAt) > mc.ttl {\n\t\tmc.removeFromList(entry)\n\t\tdelete(mc.entries, key)\n\t\treturn nil, false\n\t}\n\t\n\t// Move to front (most recently used)\n\tmc.moveToFront(entry)\n\treturn entry.Data, true\n}\n\n// Set stores data in cache\nfunc (mc *MetadataCache) Set(key string, data []byte) {\n\tmc.mu.Lock()\n\tdefer mc.mu.Unlock()\n\t\n\t// Check if entry exists\n\tif existingEntry, exists := mc.entries[key]; exists {\n\t\texistingEntry.Data = data\n\t\texistingEntry.CreatedAt = time.Now()\n\t\tmc.moveToFront(existingEntry)\n\t\treturn\n\t}\n\t\n\t// Evict if needed\n\tif len(mc.entries) >= mc.maxSize {\n\t\tmc.evictLRU()\n\t}\n\t\n\t// Create new entry\n\tentry := &MetadataCacheEntry{\n\t\tKey:       key,\n\t\tData:      data,\n\t\tCreatedAt: time.Now(),\n\t}\n\t\n\tmc.entries[key] = entry\n\tmc.addToFront(entry)\n}\n\n// Clear removes all entries\nfunc (mc *MetadataCache) Clear() {\n\tmc.mu.Lock()\n\tdefer mc.mu.Unlock()\n\t\n\tmc.entries = make(map[string]*MetadataCacheEntry)\n\tmc.head.next = mc.tail\n\tmc.tail.prev = mc.head\n}\n\n// LRU operations\nfunc (mc *MetadataCache) addToFront(entry *MetadataCacheEntry) {\n\tentry.prev = mc.head\n\tentry.next = mc.head.next\n\tmc.head.next.prev = entry\n\tmc.head.next = entry\n}\n\nfunc (mc *MetadataCache) removeFromList(entry *MetadataCacheEntry) {\n\tentry.prev.next = entry.next\n\tentry.next.prev = entry.prev\n}\n\nfunc (mc *MetadataCache) moveToFront(entry *MetadataCacheEntry) {\n\tmc.removeFromList(entry)\n\tmc.addToFront(entry)\n}\n\nfunc (mc *MetadataCache) evictLRU() {\n\tif mc.tail.prev != mc.head {\n\t\tlru := mc.tail.prev\n\t\tmc.removeFromList(lru)\n\t\tdelete(mc.entries, lru.Key)\n\t}\n}\n\n// NewComponentScanner creates a new component scanner with optimal worker pool\nfunc NewComponentScanner(registry *registry.ComponentRegistry, cfg ...*config.Config) *ComponentScanner {\n\treturn NewComponentScannerWithConcurrency(registry, 0, cfg...) // 0 = auto-detect optimal\n}\n\n// NewComponentScannerWithConcurrency creates a new component scanner with configurable concurrency\nfunc NewComponentScannerWithConcurrency(registry *registry.ComponentRegistry, maxWorkers int, cfg ...*config.Config) *ComponentScanner {\n\tscanner := &ComponentScanner{\n\t\tregistry:   registry,\n\t\tfileSet:    token.NewFileSet(),\n\t\tpathCache:  &pathValidationCache{},\n\t\tbufferPool: NewBufferPool(),\n\t\t// Initialize metadata cache: 1000 entries max, 1 hour TTL\n\t\t// This caches ~1000-2000 component metadata entries typically\n\t\tmetadataCache: NewMetadataCache(1000, time.Hour),\n\t\t// Initialize performance metrics tracking\n\t\tmetrics: &ScannerMetrics{},\n\t}\n\n\t// Initialize worker pool with configurable or optimal worker count\n\tworkerCount := maxWorkers\n\tif workerCount <= 0 {\n\t\t// Auto-detect optimal worker count\n\t\tworkerCount = runtime.NumCPU()\n\t\tif workerCount > 8 {\n\t\t\tworkerCount = 8 // Cap at 8 workers for diminishing returns\n\t\t}\n\t} else {\n\t\t// User-specified count, but enforce reasonable limits\n\t\tif workerCount > 64 {\n\t\t\tworkerCount = 64 // Maximum safety limit\n\t\t}\n\t}\n\n\tscanner.workerPool = NewWorkerPool(workerCount, scanner)\n\t\n\t// Initialize AST parsing pool with fewer workers to avoid oversubscription\n\tastWorkerCount := workerCount / 2\n\tif astWorkerCount < 1 {\n\t\tastWorkerCount = 1\n\t}\n\tscanner.astParsingPool = NewASTParsingPool(astWorkerCount)\n\n\t// Use first config if provided, otherwise nil\n\tif len(cfg) > 0 {\n\t\tscanner.config = cfg[0]\n\t}\n\t\n\treturn scanner\n}\n\n// getFileScanTimeout returns the configured timeout for file scanning operations\nfunc (s *ComponentScanner) getFileScanTimeout() time.Duration {\n\tif s.config != nil && s.config.Timeouts.FileScan > 0 {\n\t\treturn s.config.Timeouts.FileScan\n\t}\n\t// Default fallback timeout if no configuration is available\n\treturn 30 * time.Second\n}\n\n// NewWorkerPool creates a new worker pool for scanning operations\nfunc NewWorkerPool(workerCount int, scanner *ComponentScanner) *WorkerPool {\n\tpool := &WorkerPool{\n\t\tjobQueue:    make(chan ScanJob, workerCount*2), // Buffer for work-stealing efficiency\n\t\tworkerCount: workerCount,\n\t\tscanner:     scanner,\n\t\tstop:        make(chan struct{}),\n\t}\n\n\t// Start persistent workers\n\tpool.workers = make([]*ScanWorker, workerCount)\n\tfor i := 0; i < workerCount; i++ {\n\t\tworker := &ScanWorker{\n\t\t\tid:       i,\n\t\t\tjobQueue: pool.jobQueue,\n\t\t\tscanner:  scanner,\n\t\t\tstop:     make(chan struct{}),\n\t\t}\n\t\tpool.workers[i] = worker\n\t\tgo worker.start()\n\t}\n\n\treturn pool\n}\n\n// start begins the worker's processing loop\nfunc (w *ScanWorker) start() {\n\tfor {\n\t\tselect {\n\t\tcase job := <-w.jobQueue:\n\t\t\t// Process the scanning job\n\t\t\terr := w.scanner.scanFileInternal(job.filePath)\n\t\t\tjob.result <- ScanResult{\n\t\t\t\tfilePath: job.filePath,\n\t\t\t\terr:      err,\n\t\t\t}\n\t\tcase <-w.stop:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Stop gracefully shuts down the worker pool\nfunc (p *WorkerPool) Stop() {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\n\tif p.stopped {\n\t\treturn\n\t}\n\n\tp.stopped = true\n\tclose(p.stop)\n\n\t// Stop all workers\n\tfor _, worker := range p.workers {\n\t\tclose(worker.stop)\n\t}\n\n\t// Close job queue\n\tclose(p.jobQueue)\n}\n\n// GetRegistry returns the component registry\nfunc (s *ComponentScanner) GetRegistry() interfaces.ComponentRegistry {\n\treturn s.registry\n}\n\n// GetWorkerCount returns the number of active workers in the pool\nfunc (s *ComponentScanner) GetWorkerCount() int {\n\tif s.workerPool == nil {\n\t\treturn 0\n\t}\n\ts.workerPool.mu.RLock()\n\tdefer s.workerPool.mu.RUnlock()\n\treturn s.workerPool.workerCount\n}\n\n// GetMetrics returns a copy of the current scanner metrics\nfunc (s *ComponentScanner) GetMetrics() ScannerMetrics {\n\tif s.metrics == nil {\n\t\treturn ScannerMetrics{}\n\t}\n\treturn ScannerMetrics{\n\t\tFilesProcessed:  atomic.LoadInt64(&s.metrics.FilesProcessed),\n\t\tComponentsFound: atomic.LoadInt64(&s.metrics.ComponentsFound),\n\t\tCacheHits:       atomic.LoadInt64(&s.metrics.CacheHits),\n\t\tCacheMisses:     atomic.LoadInt64(&s.metrics.CacheMisses),\n\t\tTotalScanTime:   s.metrics.TotalScanTime,\n\t\tPeakMemoryUsage: atomic.LoadUint64(&s.metrics.PeakMemoryUsage),\n\t\tConcurrentJobs:  atomic.LoadInt64(&s.metrics.ConcurrentJobs),\n\t}\n}\n\n// ResetMetrics clears all scanner metrics\nfunc (s *ComponentScanner) ResetMetrics() {\n\tif s.metrics == nil {\n\t\treturn\n\t}\n\tatomic.StoreInt64(&s.metrics.FilesProcessed, 0)\n\tatomic.StoreInt64(&s.metrics.ComponentsFound, 0)\n\tatomic.StoreInt64(&s.metrics.CacheHits, 0)\n\tatomic.StoreInt64(&s.metrics.CacheMisses, 0)\n\tatomic.StoreUint64(&s.metrics.PeakMemoryUsage, 0)\n\tatomic.StoreInt64(&s.metrics.ConcurrentJobs, 0)\n\ts.metrics.TotalScanTime = 0\n}\n\n// Close gracefully shuts down the scanner and its worker pool\nfunc (s *ComponentScanner) Close() error {\n\tif s.astParsingPool != nil {\n\t\ts.astParsingPool.Close()\n\t}\n\tif s.workerPool != nil {\n\t\ts.workerPool.Stop()\n\t}\n\tif s.metadataCache != nil {\n\t\ts.metadataCache.Clear()\n\t}\n\treturn nil\n}\n\n// getCachedMetadata attempts to retrieve cached component metadata for a file\nfunc (s *ComponentScanner) getCachedMetadata(filePath, fileHash string) (*CachedComponentMetadata, bool) {\n\tif s.metadataCache == nil {\n\t\treturn nil, false\n\t}\n\n\tcacheKey := fmt.Sprintf(\"%s:%s\", filePath, fileHash)\n\tcachedData, found := s.metadataCache.Get(cacheKey)\n\tif !found {\n\t\treturn nil, false\n\t}\n\n\tvar metadata CachedComponentMetadata\n\tif err := json.Unmarshal(cachedData, &metadata); err != nil {\n\t\t// Cache corruption - remove invalid entry\n\t\ts.metadataCache.Set(cacheKey, nil)\n\t\treturn nil, false\n\t}\n\n\t// Verify the cached hash matches current file hash (additional safety check)\n\tif metadata.FileHash != fileHash {\n\t\treturn nil, false\n\t}\n\n\treturn &metadata, true\n}\n\n// setCachedMetadata stores component metadata in the cache\nfunc (s *ComponentScanner) setCachedMetadata(filePath, fileHash string, components []*types.ComponentInfo) {\n\tif s.metadataCache == nil {\n\t\treturn\n\t}\n\n\tmetadata := CachedComponentMetadata{\n\t\tComponents: components,\n\t\tFileHash:   fileHash,\n\t\tParsedAt:   time.Now(),\n\t}\n\n\tdata, err := json.Marshal(metadata)\n\tif err != nil {\n\t\t// Skip caching if marshaling fails\n\t\treturn\n\t}\n\n\tcacheKey := fmt.Sprintf(\"%s:%s\", filePath, fileHash)\n\ts.metadataCache.Set(cacheKey, data)\n}\n\n// ScanDirectory scans a directory for templ components using optimized worker pool with timeout support\nfunc (s *ComponentScanner) ScanDirectoryWithContext(ctx context.Context, dir string) error {\n\tstart := time.Now()\n\t\n\t// Track memory usage at start\n\tvar startMem runtime.MemStats\n\truntime.ReadMemStats(&startMem)\n\t\n\t// Validate directory path to prevent path traversal\n\tif _, err := s.validatePath(dir); err != nil {\n\t\treturn errors.WrapValidation(err, errors.ErrCodeInvalidPath, \n\t\t\t\"directory path validation failed\").\n\t\t\tWithContext(\"directory\", dir)\n\t}\n\n\t// Use concurrent directory walking for better performance on large codebases\n\tfiles, err := s.walkDirectoryConcurrent(dir)\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Process files using persistent worker pool with context (no goroutine creation overhead)\n\terr = s.processBatchWithWorkerPoolWithContext(ctx, files)\n\t\n\t// Update metrics\n\tif s.metrics != nil {\n\t\telapsed := time.Since(start)\n\t\ts.metrics.TotalScanTime += elapsed\n\t\tatomic.AddInt64(&s.metrics.FilesProcessed, int64(len(files)))\n\t\t\n\t\t// Track memory usage\n\t\tvar endMem runtime.MemStats\n\t\truntime.ReadMemStats(&endMem)\n\t\tmemUsed := endMem.Alloc - startMem.Alloc\n\t\t\n\t\t// Update peak memory if this scan used more\n\t\tfor {\n\t\t\tcurrent := atomic.LoadUint64(&s.metrics.PeakMemoryUsage)\n\t\t\tif memUsed <= current || atomic.CompareAndSwapUint64(&s.metrics.PeakMemoryUsage, current, memUsed) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\t\n\treturn err\n}\n\n// ScanDirectory scans a directory for templ components (backward compatible wrapper)\nfunc (s *ComponentScanner) ScanDirectory(dir string) error {\n\t// Create a timeout context for the scan operation\n\tscanTimeout := s.getFileScanTimeout()\n\tctx, cancel := context.WithTimeout(context.Background(), scanTimeout)\n\tdefer cancel()\n\t\n\treturn s.ScanDirectoryWithContext(ctx, dir)\n}\n\n// processBatchWithWorkerPoolWithContext processes files using the persistent worker pool with optimized batching and context support\nfunc (s *ComponentScanner) processBatchWithWorkerPoolWithContext(ctx context.Context, files []string) error {\n\tif len(files) == 0 {\n\t\treturn nil\n\t}\n\n\t// For very small batches, process synchronously to avoid overhead\n\tif len(files) <= 5 {\n\t\treturn s.processBatchSynchronous(files)\n\t}\n\n\t// Create result channel for collecting results\n\tresultChan := make(chan ScanResult, len(files))\n\tsubmitted := 0\n\t\n\t// Submit jobs to persistent worker pool\n\tfor _, file := range files {\n\t\t// Check if context is cancelled\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tdefault:\n\t\t}\n\n\t\tjob := ScanJob{\n\t\t\tfilePath: file,\n\t\t\tresult:   resultChan,\n\t\t}\n\n\t\tselect {\n\t\tcase s.workerPool.jobQueue <- job:\n\t\t\t// Job submitted successfully\n\t\t\tsubmitted++\n\t\tcase <-ctx.Done():\n\t\t\t// Context cancelled while submitting\n\t\t\treturn ctx.Err()\n\t\tdefault:\n\t\t\t// Worker pool is full, process synchronously as fallback\n\t\t\terr := s.scanFileInternal(file)\n\t\t\tresultChan <- ScanResult{filePath: file, err: err}\n\t\t}\n\t}\n\n\t// Collect results with context checking\n\tvar scanErrors []error\n\tfor i := 0; i < len(files); i++ {\n\t\tselect {\n\t\tcase result := <-resultChan:\n\t\t\tif result.err != nil {\n\t\t\t\t// Enhance the error with file context\n\t\t\t\tenhancedErr := errors.EnhanceError(result.err, \"scanner\", result.filePath, 0, 0)\n\t\t\t\tscanErrors = append(scanErrors, enhancedErr)\n\t\t\t}\n\t\tcase <-ctx.Done():\n\t\t\t// Context cancelled while collecting results\n\t\t\treturn ctx.Err()\n\t\t}\n\t}\n\n\tclose(resultChan)\n\n\tif len(scanErrors) > 0 {\n\t\treturn errors.CombineErrors(scanErrors...)\n\t}\n\n\treturn nil\n}\n\n// processBatchWithWorkerPool processes files using the persistent worker pool (backward compatible wrapper)\nfunc (s *ComponentScanner) processBatchWithWorkerPool(files []string) error {\n\t// Use background context with timeout for backward compatibility\n\tscanTimeout := s.getFileScanTimeout()\n\tctx, cancel := context.WithTimeout(context.Background(), scanTimeout)\n\tdefer cancel()\n\t\n\treturn s.processBatchWithWorkerPoolWithContext(ctx, files)\n}\n\n// processBatchSynchronous processes small batches synchronously for better performance\nfunc (s *ComponentScanner) processBatchSynchronous(files []string) error {\n\tvar scanErrors []error\n\t\n\tfor _, file := range files {\n\t\tif err := s.scanFileInternal(file); err != nil {\n\t\t\tenhancedErr := errors.EnhanceError(err, \"scanner\", file, 0, 0)\n\t\t\tscanErrors = append(scanErrors, enhancedErr)\n\t\t}\n\t}\n\n\tif len(scanErrors) > 0 {\n\t\treturn errors.CombineErrors(scanErrors...)\n\t}\n\n\treturn nil\n}\n\n// ScanDirectoryParallel is deprecated in favor of the optimized ScanDirectory\n// Kept for backward compatibility\nfunc (s *ComponentScanner) ScanDirectoryParallel(dir string, workers int) error {\n\treturn s.ScanDirectory(dir) // Use optimized version\n}\n\n// ScanFile scans a single file for templ components (optimized)\nfunc (s *ComponentScanner) ScanFile(path string) error {\n\treturn s.scanFileInternal(path)\n}\n\n// scanFileInternal is the optimized internal scanning method used by workers\nfunc (s *ComponentScanner) scanFileInternal(path string) error {\n\t// Validate and clean the path to prevent directory traversal\n\tcleanPath, err := s.validatePath(path)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"invalid path: %w\", err)\n\t}\n\n\t// Optimized single I/O operation: open file and get both content and info\n\tfile, err := os.Open(cleanPath)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"opening file %s: %w\", cleanPath, err)\n\t}\n\tdefer file.Close()\n\n\t// Get file info without separate Stat() call\n\tinfo, err := file.Stat()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"getting file info for %s: %w\", cleanPath, err)\n\t}\n\n\t// Get buffer from pool for optimized memory usage\n\tbuffer := s.bufferPool.Get()\n\tdefer s.bufferPool.Put(buffer)\n\n\t// Read content efficiently using buffer pool\n\tvar content []byte\n\tif info.Size() > 64*1024 {\n\t\t// Use streaming read for large files to reduce memory pressure\n\t\tcontent, err = s.readFileStreamingOptimized(file, info.Size(), buffer)\n\t} else {\n\t\t// Use pooled buffer for small files\n\t\tif cap(buffer) < int(info.Size()) {\n\t\t\tbuffer = make([]byte, info.Size())\n\t\t}\n\t\tbuffer = buffer[:info.Size()]\n\t\t_, err = file.Read(buffer)\n\t\tif err == nil {\n\t\t\tcontent = make([]byte, len(buffer))\n\t\t\tcopy(content, buffer)\n\t\t}\n\t}\n\n\tif err != nil {\n\t\treturn fmt.Errorf(\"reading file %s: %w\", cleanPath, err)\n\t}\n\n\t// Calculate optimized file hash for cache lookup and change detection\n\thash, hashStrategy := s.generateOptimizedHash(content, info)\n\t\n\t// Track hash generation metrics\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.FilesProcessed, 1)\n\t\t// Add hash strategy metrics if needed for monitoring\n\t}\n\t\n\t// Check cache first - avoid expensive parsing if metadata is cached\n\tif cachedMetadata, found := s.getCachedMetadata(cleanPath, hash); found {\n\t\t// Track cache hit\n\t\tif s.metrics != nil {\n\t\t\tatomic.AddInt64(&s.metrics.CacheHits, 1)\n\t\t}\n\t\t\n\t\t// Register all cached components with the registry\n\t\tfor _, component := range cachedMetadata.Components {\n\t\t\t// Update file modification time to current scan time\n\t\t\tupdatedComponent := *component\n\t\t\tupdatedComponent.LastMod = info.ModTime()\n\t\t\tupdatedComponent.Hash = hash\n\t\t\ts.registry.Register(&updatedComponent)\n\t\t}\n\t\t\n\t\t// Track components found\n\t\tif s.metrics != nil {\n\t\t\tatomic.AddInt64(&s.metrics.ComponentsFound, int64(len(cachedMetadata.Components)))\n\t\t}\n\t\t\n\t\treturn nil\n\t}\n\n\t// Track cache miss\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.CacheMisses, 1)\n\t}\n\n\t// Cache miss - perform parsing with async AST parsing to avoid blocking worker threads\n\tvar components []*types.ComponentInfo\n\t\n\t// Use async AST parsing to avoid blocking the worker thread\n\tastResultChan := s.astParsingPool.ParseAsync(cleanPath, content, s.fileSet)\n\t\n\t// Wait for AST parsing result (non-blocking for the worker thread)\n\tastResult := <-astResultChan\n\t\n\tif astResult.err != nil {\n\t\t// If AST parsing fails, try manual component extraction for .templ files\n\t\tcomponents, err = s.parseTemplFileWithComponents(cleanPath, content, hash, info.ModTime())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\t// Extract components from AST\n\t\tcomponents, err = s.extractFromASTWithComponents(cleanPath, astResult.astFile, hash, info.ModTime())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Cache the parsed components for future scans\n\ts.setCachedMetadata(cleanPath, hash, components)\n\n\t// Register all components with the registry\n\tfor _, component := range components {\n\t\ts.registry.Register(component)\n\t}\n\t\n\t// Track components found\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.ComponentsFound, int64(len(components)))\n\t}\n\n\treturn nil\n}\n\n// readFileStreaming removed - replaced by readFileStreamingOptimized\n\n// readFileStreamingOptimized reads large files using pooled buffers for better memory efficiency\nfunc (s *ComponentScanner) readFileStreamingOptimized(file *os.File, size int64, pooledBuffer []byte) ([]byte, error) {\n\tconst chunkSize = 32 * 1024 // 32KB chunks\n\t\n\t// Use a reasonably-sized chunk buffer for reading\n\tvar chunk []byte\n\tif cap(pooledBuffer) >= chunkSize {\n\t\tchunk = pooledBuffer[:chunkSize]\n\t} else {\n\t\tchunk = make([]byte, chunkSize)\n\t}\n\n\t// Pre-allocate content buffer with exact size to avoid reallocations\n\tcontent := make([]byte, 0, size)\n\n\tfor {\n\t\tn, err := file.Read(chunk)\n\t\tif n > 0 {\n\t\t\tcontent = append(content, chunk[:n]...)\n\t\t}\n\t\tif err != nil {\n\t\t\tif err == io.EOF {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t\tif n < chunkSize {\n\t\t\tbreak\n\t\t}\n\t}\n\n\treturn content, nil\n}\n\n// Backward compatibility method removed - unused\n\n// parseTemplFileWithComponents extracts components from templ files and returns them\nfunc (s *ComponentScanner) parseTemplFileWithComponents(path string, content []byte, hash string, modTime time.Time) ([]*types.ComponentInfo, error) {\n\tvar components []*types.ComponentInfo\n\tlines := strings.Split(string(content), \"\\n\")\n\tpackageName := \"\"\n\n\tfor _, line := range lines {\n\t\tline = strings.TrimSpace(line)\n\n\t\t// Extract package name\n\t\tif strings.HasPrefix(line, \"package \") {\n\t\t\tparts := strings.Fields(line)\n\t\t\tif len(parts) >= 2 {\n\t\t\t\tpackageName = sanitizeIdentifier(parts[1])\n\t\t\t}\n\t\t}\n\n\t\t// Extract templ component declarations\n\t\tif strings.HasPrefix(line, \"templ \") {\n\t\t\t// Extract component name from templ declaration\n\t\t\tparts := strings.Fields(line)\n\t\t\tif len(parts) >= 2 {\n\t\t\t\tname := parts[1]\n\t\t\t\tif idx := strings.Index(name, \"(\"); idx != -1 {\n\t\t\t\t\tname = name[:idx]\n\t\t\t\t}\n\n\t\t\t\t// Sanitize component name to prevent injection\n\t\t\t\tname = sanitizeIdentifier(name)\n\n\t\t\t\tcomponent := &types.ComponentInfo{\n\t\t\t\t\tName:         name,\n\t\t\t\t\tPackage:      packageName,\n\t\t\t\t\tFilePath:     path,\n\t\t\t\t\tParameters:   extractParameters(line),\n\t\t\t\t\tImports:      []string{},\n\t\t\t\t\tLastMod:      modTime,\n\t\t\t\t\tHash:         hash,\n\t\t\t\t\tDependencies: []string{},\n\t\t\t\t}\n\n\t\t\t\tcomponents = append(components, component)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn components, nil\n}\n\n// parseTemplFile provides backward compatibility - delegates to the new component-returning version\nfunc (s *ComponentScanner) parseTemplFile(path string, content []byte, hash string, modTime time.Time) error {\n\tcomponents, err := s.parseTemplFileWithComponents(path, content, hash, modTime)\n\tif err != nil {\n\t\treturn err\n\t}\n\t\n\t// Register all components\n\tfor _, component := range components {\n\t\ts.registry.Register(component)\n\t}\n\t\n\treturn nil\n}\n\n// extractFromASTWithComponents extracts components from AST and returns them\nfunc (s *ComponentScanner) extractFromASTWithComponents(path string, astFile *ast.File, hash string, modTime time.Time) ([]*types.ComponentInfo, error) {\n\tvar components []*types.ComponentInfo\n\t\n\t// Walk the AST to find function declarations that might be templ components\n\tast.Inspect(astFile, func(n ast.Node) bool {\n\t\tswitch node := n.(type) {\n\t\tcase *ast.FuncDecl:\n\t\t\tif node.Name != nil && node.Name.IsExported() {\n\t\t\t\t// Check if this might be a templ component\n\t\t\t\tif s.isTemplComponent(node) {\n\t\t\t\t\tcomponent := &types.ComponentInfo{\n\t\t\t\t\t\tName:         node.Name.Name,\n\t\t\t\t\t\tPackage:      astFile.Name.Name,\n\t\t\t\t\t\tFilePath:     path,\n\t\t\t\t\t\tParameters:   s.extractParametersFromFunc(node),\n\t\t\t\t\t\tImports:      s.extractImports(astFile),\n\t\t\t\t\t\tLastMod:      modTime,\n\t\t\t\t\t\tHash:         hash,\n\t\t\t\t\t\tDependencies: []string{},\n\t\t\t\t\t}\n\n\t\t\t\t\tcomponents = append(components, component)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn true\n\t})\n\n\treturn components, nil\n}\n\n// extractFromAST provides backward compatibility - delegates to the new component-returning version\nfunc (s *ComponentScanner) extractFromAST(path string, astFile *ast.File, hash string, modTime time.Time) error {\n\tcomponents, err := s.extractFromASTWithComponents(path, astFile, hash, modTime)\n\tif err != nil {\n\t\treturn err\n\t}\n\t\n\t// Register all components\n\tfor _, component := range components {\n\t\ts.registry.Register(component)\n\t}\n\t\n\treturn nil\n}\n\nfunc (s *ComponentScanner) isTemplComponent(fn *ast.FuncDecl) bool {\n\t// Check if the function returns a templ.Component\n\tif fn.Type.Results == nil || len(fn.Type.Results.List) == 0 {\n\t\treturn false\n\t}\n\n\tresult := fn.Type.Results.List[0]\n\tif sel, ok := result.Type.(*ast.SelectorExpr); ok {\n\t\tif ident, ok := sel.X.(*ast.Ident); ok {\n\t\t\treturn ident.Name == \"templ\" && sel.Sel.Name == \"Component\"\n\t\t}\n\t}\n\n\treturn false\n}\n\nfunc (s *ComponentScanner) extractParametersFromFunc(fn *ast.FuncDecl) []types.ParameterInfo {\n\tvar params []types.ParameterInfo\n\n\tif fn.Type.Params == nil {\n\t\treturn params\n\t}\n\n\tfor _, param := range fn.Type.Params.List {\n\t\tparamType := \"\"\n\t\tif param.Type != nil {\n\t\t\tparamType = s.typeToString(param.Type)\n\t\t}\n\n\t\tfor _, name := range param.Names {\n\t\t\tparams = append(params, types.ParameterInfo{\n\t\t\t\tName:     name.Name,\n\t\t\t\tType:     paramType,\n\t\t\t\tOptional: false,\n\t\t\t\tDefault:  nil,\n\t\t\t})\n\t\t}\n\t}\n\n\treturn params\n}\n\nfunc (s *ComponentScanner) extractImports(astFile *ast.File) []string {\n\tvar imports []string\n\n\tfor _, imp := range astFile.Imports {\n\t\tif imp.Path != nil {\n\t\t\timports = append(imports, imp.Path.Value)\n\t\t}\n\t}\n\n\treturn imports\n}\n\nfunc (s *ComponentScanner) typeToString(expr ast.Expr) string {\n\tswitch e := expr.(type) {\n\tcase *ast.Ident:\n\t\treturn e.Name\n\tcase *ast.SelectorExpr:\n\t\treturn s.typeToString(e.X) + \".\" + e.Sel.Name\n\tcase *ast.StarExpr:\n\t\treturn \"*\" + s.typeToString(e.X)\n\tcase *ast.ArrayType:\n\t\treturn \"[]\" + s.typeToString(e.Elt)\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\nfunc extractParameters(line string) []types.ParameterInfo {\n\t// Simple parameter extraction from templ declaration\n\t// This is a basic implementation - real parser would be more robust\n\tif !strings.Contains(line, \"(\") {\n\t\treturn []types.ParameterInfo{}\n\t}\n\n\tstart := strings.Index(line, \"(\")\n\tend := strings.LastIndex(line, \")\")\n\tif start == -1 || end == -1 || start >= end {\n\t\treturn []types.ParameterInfo{}\n\t}\n\n\tparamStr := line[start+1 : end]\n\tif strings.TrimSpace(paramStr) == \"\" {\n\t\treturn []types.ParameterInfo{}\n\t}\n\n\t// Basic parameter parsing - handle both \"name type\" and \"name, name type\" patterns\n\tparts := strings.Split(paramStr, \",\")\n\tvar params []types.ParameterInfo\n\n\tfor _, part := range parts {\n\t\tpart = strings.TrimSpace(part)\n\t\tif part == \"\" {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Split by space to get name and type\n\t\tfields := strings.Fields(part)\n\t\tif len(fields) >= 2 {\n\t\t\t// Handle \"name type\" format\n\t\t\tparams = append(params, types.ParameterInfo{\n\t\t\t\tName:     fields[0],\n\t\t\t\tType:     fields[1],\n\t\t\t\tOptional: false,\n\t\t\t\tDefault:  nil,\n\t\t\t})\n\t\t} else if len(fields) == 1 {\n\t\t\t// Handle single parameter name (type might be from previous param)\n\t\t\tparams = append(params, types.ParameterInfo{\n\t\t\t\tName:     fields[0],\n\t\t\t\tType:     \"string\", // Default type\n\t\t\t\tOptional: false,\n\t\t\t\tDefault:  nil,\n\t\t\t})\n\t\t}\n\t}\n\n\treturn params\n}\n\n// sanitizeIdentifier removes dangerous characters from identifiers\nfunc sanitizeIdentifier(identifier string) string {\n\t// Only allow alphanumeric characters and underscores for identifiers\n\tvar cleaned strings.Builder\n\tfor _, r := range identifier {\n\t\tif (r >= 'a' && r <= 'z') || (r >= 'A' && r <= 'Z') || (r >= '0' && r <= '9') || r == '_' {\n\t\t\tcleaned.WriteRune(r)\n\t\t}\n\t}\n\treturn cleaned.String()\n}\n\n// validatePath validates and cleans a file path to prevent directory traversal.\n// This optimized version caches the current working directory to avoid repeated\n// expensive filesystem operations, achieving 50-70% performance improvement.\nfunc (s *ComponentScanner) validatePath(path string) (string, error) {\n\t// Clean the path to resolve . and .. elements\n\tcleanPath := filepath.Clean(path)\n\n\t// Get absolute path to normalize (needed for working directory check)\n\tabsPath, err := filepath.Abs(cleanPath)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"getting absolute path: %w\", err)\n\t}\n\n\t// Get cached current working directory\n\tcwd, err := s.getCachedWorkingDir()\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"getting current directory: %w\", err)\n\t}\n\n\t// Primary security check: ensure the path is within the current working directory\n\t// This prevents directory traversal attacks that escape the working directory\n\tif !strings.HasPrefix(absPath, cwd) {\n\t\treturn \"\", errors.ErrPathTraversal(path).WithContext(\"working_directory\", cwd)\n\t}\n\n\t// Secondary security check: reject paths with suspicious patterns\n\t// This catches directory traversal attempts that stay within the working directory\n\tif strings.Contains(cleanPath, \"..\") {\n\t\treturn \"\", errors.ErrPathTraversal(path).\n\t\t\tWithContext(\"pattern\", \"contains '..' traversal\")\n\t}\n\n\treturn cleanPath, nil\n}\n\n// getCachedWorkingDir returns the current working directory from cache,\n// initializing it on first access. This eliminates repeated os.Getwd() calls.\nfunc (s *ComponentScanner) getCachedWorkingDir() (string, error) {\n\t// Fast path: check if already initialized with read lock\n\ts.pathCache.mu.RLock()\n\tif s.pathCache.initialized {\n\t\tcwd := s.pathCache.currentWorkingDir\n\t\ts.pathCache.mu.RUnlock()\n\t\treturn cwd, nil\n\t}\n\ts.pathCache.mu.RUnlock()\n\n\t// Slow path: initialize the cache with write lock\n\ts.pathCache.mu.Lock()\n\tdefer s.pathCache.mu.Unlock()\n\n\t// Double-check pattern: another goroutine might have initialized while waiting\n\tif s.pathCache.initialized {\n\t\treturn s.pathCache.currentWorkingDir, nil\n\t}\n\n\t// Get current working directory (expensive syscall - done only once)\n\tcwd, err := os.Getwd()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Ensure we have the absolute path for consistent comparison\n\tabsCwd, err := filepath.Abs(cwd)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"getting absolute working directory: %w\", err)\n\t}\n\n\t// Cache the result\n\ts.pathCache.currentWorkingDir = absCwd\n\ts.pathCache.initialized = true\n\n\treturn absCwd, nil\n}\n\n// InvalidatePathCache clears the cached working directory.\n// This should be called if the working directory changes during execution.\nfunc (s *ComponentScanner) InvalidatePathCache() {\n\ts.pathCache.mu.Lock()\n\tdefer s.pathCache.mu.Unlock()\n\ts.pathCache.initialized = false\n\ts.pathCache.currentWorkingDir = \"\"\n}\n\n// walkDirectoryConcurrent implements concurrent directory walking for improved performance\n// on large codebases. Uses goroutines to parallelize directory discovery.\nfunc (s *ComponentScanner) walkDirectoryConcurrent(rootDir string) ([]string, error) {\n\t// For small directory trees, use optimized sequential version\n\t// For larger trees, use concurrent discovery\n\t\n\t// Quick check for directory size to decide approach\n\tentries, err := os.ReadDir(rootDir)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading root directory %s: %w\", rootDir, err)\n\t}\n\t\n\t// If small directory, use optimized sequential\n\tif len(entries) < 10 {\n\t\treturn s.walkDirectoryOptimized(rootDir)\n\t}\n\t\n\t// Use concurrent approach for larger directories\n\treturn s.walkDirectoryParallel(rootDir)\n}\n\n// walkDirectoryParallel implements concurrent directory discovery\nfunc (s *ComponentScanner) walkDirectoryParallel(rootDir string) ([]string, error) {\n\t// Use a simple approach: collect all directories first, then process them concurrently\n\t\n\t// First, collect all directories sequentially (this is fast)\n\tvar allDirs []string\n\terr := filepath.WalkDir(rootDir, func(path string, d os.DirEntry, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif d.IsDir() && !s.shouldSkipDirectory(d.Name()) {\n\t\t\tallDirs = append(allDirs, path)\n\t\t}\n\t\treturn nil\n\t})\n\t\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t\n\t// Now process directories concurrently\n\tconst maxWorkers = 4\n\tjobs := make(chan string, len(allDirs))\n\tresults := make(chan []string, len(allDirs))\n\t\n\t// Start workers\n\tvar wg sync.WaitGroup\n\tfor w := 0; w < maxWorkers; w++ {\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\tfor dir := range jobs {\n\t\t\t\tfiles, _, _ := s.processSingleDirectory(dir)\n\t\t\t\tif len(files) > 0 {\n\t\t\t\t\tresults <- files\n\t\t\t\t} else {\n\t\t\t\t\tresults <- nil // Send empty result to maintain count\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n\t\n\t// Send jobs\n\tfor _, dir := range allDirs {\n\t\tjobs <- dir\n\t}\n\tclose(jobs)\n\t\n\t// Wait for workers to finish\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(results)\n\t}()\n\t\n\t// Collect results\n\tvar allFiles []string\n\tfor files := range results {\n\t\tif files != nil {\n\t\t\tallFiles = append(allFiles, files...)\n\t\t}\n\t}\n\t\n\treturn allFiles, nil\n}\n\n// walkDirectoryOptimized implements an optimized sequential walk with directory skipping\nfunc (s *ComponentScanner) walkDirectoryOptimized(rootDir string) ([]string, error) {\n\tvar files []string\n\t\n\terr := filepath.WalkDir(rootDir, func(path string, d os.DirEntry, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Skip directories that typically don't contain templ files\n\t\tif d.IsDir() && s.shouldSkipDirectory(d.Name()) {\n\t\t\treturn filepath.SkipDir\n\t\t}\n\n\t\tif !d.IsDir() && strings.HasSuffix(path, \".templ\") {\n\t\t\t// Validate each file path as we encounter it\n\t\t\tif _, err := s.validatePath(path); err != nil {\n\t\t\t\t// Skip invalid paths silently for security\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tfiles = append(files, path)\n\t\t}\n\t\t\n\t\treturn nil\n\t})\n\t\n\treturn files, err\n}\n\n// processSingleDirectory processes a single directory and returns files and subdirectories\nfunc (s *ComponentScanner) processSingleDirectory(dir string) ([]string, []string, error) {\n\tvar files []string\n\tvar subdirs []string\n\t\n\tentries, err := os.ReadDir(dir)\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"reading directory %s: %w\", dir, err)\n\t}\n\t\n\tfor _, entry := range entries {\n\t\tpath := filepath.Join(dir, entry.Name())\n\t\t\n\t\tif entry.IsDir() {\n\t\t\t// Skip directories that typically don't contain templ files\n\t\t\tif s.shouldSkipDirectory(entry.Name()) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tsubdirs = append(subdirs, path)\n\t\t} else if strings.HasSuffix(entry.Name(), \".templ\") {\n\t\t\t// Validate each file path as we encounter it\n\t\t\tif _, err := s.validatePath(path); err != nil {\n\t\t\t\t// Skip invalid paths silently for security\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfiles = append(files, path)\n\t\t}\n\t}\n\t\n\treturn files, subdirs, nil\n}\n\n// shouldSkipDirectory determines if a directory should be skipped for performance\nfunc (s *ComponentScanner) shouldSkipDirectory(name string) bool {\n\tskipDirs := map[string]bool{\n\t\t\".git\":         true,\n\t\t\".svn\":         true,\n\t\t\"node_modules\": true,\n\t\t\".next\":        true,\n\t\t\"dist\":         true,\n\t\t\"build\":        true,\n\t\t\"vendor\":       true,\n\t\t\".vscode\":      true,\n\t\t\".idea\":        true,\n\t\t\"__pycache__\":  true,\n\t\t\".DS_Store\":    true,\n\t}\n\t\n\treturn skipDirs[name]\n}\n\n// HashingStrategy contains information about the hash generation approach used\ntype HashingStrategy struct {\n\tStrategy     FileHashStrategy\n\tSamplePoints int\n\tHashTime     time.Duration\n\tFileSize     int64\n}\n\n// generateOptimizedHash creates an optimized hash based on file size and content characteristics\nfunc (s *ComponentScanner) generateOptimizedHash(content []byte, fileInfo os.FileInfo) (string, *HashingStrategy) {\n\tstart := time.Now()\n\tfileSize := int64(len(content))\n\t\n\tstrategy := &HashingStrategy{\n\t\tFileSize: fileSize,\n\t}\n\t\n\tvar hash uint32\n\t\n\tswitch {\n\tcase fileSize <= smallFileThreshold:\n\t\t// Small files: use full content CRC32 (fast anyway)\n\t\thash = crc32.Checksum(content, crcTable)\n\t\tstrategy.Strategy = HashStrategyFull\n\t\tstrategy.SamplePoints = 1\n\t\t\n\tcase fileSize <= mediumFileThreshold:\n\t\t// Medium files: use content sampling\n\t\thash = s.generateSampledHash(content)\n\t\tstrategy.Strategy = HashStrategySampled\n\t\tstrategy.SamplePoints = 3\n\t\t\n\tdefault:\n\t\t// Large files: use hierarchical sampling with metadata\n\t\thash = s.generateHierarchicalHash(content, fileInfo)\n\t\tstrategy.Strategy = HashStrategyHierarchical\n\t\tstrategy.SamplePoints = 5\n\t}\n\t\n\tstrategy.HashTime = time.Since(start)\n\t\n\t// Include file metadata in hash to catch size/timestamp changes\n\tmetadataHash := s.generateMetadataHash(fileInfo)\n\tcombinedHash := hash ^ metadataHash\n\t\n\treturn strconv.FormatUint(uint64(combinedHash), 16), strategy\n}\n\n// generateSampledHash creates a hash from strategic content samples\nfunc (s *ComponentScanner) generateSampledHash(content []byte) uint32 {\n\tif len(content) == 0 {\n\t\treturn 0\n\t}\n\t\n\t// Sample three strategic points: beginning, middle, and end\n\tsampleSize := contentSampleSize\n\tif len(content) < sampleSize*3 {\n\t\t// If file is small, just hash it all\n\t\treturn crc32.Checksum(content, crcTable)\n\t}\n\t\n\t// Create a combined sample from key sections\n\tvar samples []byte\n\t\n\t// Beginning sample\n\tif len(content) > sampleSize {\n\t\tsamples = append(samples, content[:sampleSize]...)\n\t}\n\t\n\t// Middle sample\n\tmid := len(content) / 2\n\tmidStart := mid - sampleSize/2\n\tmidEnd := mid + sampleSize/2\n\tif midStart >= 0 && midEnd < len(content) {\n\t\tsamples = append(samples, content[midStart:midEnd]...)\n\t}\n\t\n\t// End sample\n\tif len(content) > sampleSize {\n\t\tsamples = append(samples, content[len(content)-sampleSize:]...)\n\t}\n\t\n\treturn crc32.Checksum(samples, crcTable)\n}\n\n// generateHierarchicalHash creates a hierarchical hash for large files\nfunc (s *ComponentScanner) generateHierarchicalHash(content []byte, fileInfo os.FileInfo) uint32 {\n\tif len(content) == 0 {\n\t\treturn 0\n\t}\n\t\n\t// For templ files, focus on key sections that are likely to change\n\tvar keyContent []byte\n\t\n\t// Add file header (package declaration, imports)\n\tif len(content) > 2048 {\n\t\tkeyContent = append(keyContent, content[:2048]...)\n\t}\n\t\n\t// Sample multiple points throughout the file\n\tchunkSize := len(content) / 8 // Divide into 8 chunks\n\tif chunkSize > contentSampleSize {\n\t\tfor i := 1; i < 8; i++ {\n\t\t\tstart := i * chunkSize\n\t\t\tend := start + contentSampleSize/8\n\t\t\tif end < len(content) {\n\t\t\t\tkeyContent = append(keyContent, content[start:end]...)\n\t\t\t}\n\t\t}\n\t}\n\t\n\t// Add file footer (last part likely to contain component definitions)\n\tif len(content) > 1024 {\n\t\tkeyContent = append(keyContent, content[len(content)-1024:]...)\n\t}\n\t\n\treturn crc32.Checksum(keyContent, crcTable)\n}\n\n// generateMetadataHash creates a hash from file metadata\nfunc (s *ComponentScanner) generateMetadataHash(fileInfo os.FileInfo) uint32 {\n\t// Combine file size and modification time for metadata hash\n\tmetadata := fmt.Sprintf(\"%d:%d\", fileInfo.Size(), fileInfo.ModTime().Unix())\n\treturn crc32.ChecksumIEEE([]byte(metadata))\n}\n\n\n",
      "structuredPatch": [
        {
          "oldStart": 918,
          "oldLines": 7,
          "newStart": 918,
          "newLines": 8,
          "lines": [
            "   // Track hash generation metrics",
            "   if s.metrics != nil {",
            "     atomic.AddInt64(&s.metrics.FilesProcessed, 1)",
            "-    // Add hash strategy metrics if needed for monitoring",
            "+    // Track hash strategy performance (avoid unused variable)",
            "+    _ = hashStrategy",
            "   }",
            "   ",
            "   // Check cache first - avoid expensive parsing if metadata is cached"
          ]
        }
      ],
      "userModified": false,
      "replaceAll": false
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "go build ./internal/scanner",
      "description": "Test compilation with optimized hashing system"
    },
    "tool_response": {
      "stdout": "",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "go test ./internal/build -run TestBuildPipelineTimeout -v",
      "description": "Test build pipeline with improved cancellation handling"
    },
    "tool_response": {
      "stdout": "=== RUN   TestBuildPipelineTimeout\n=== RUN   TestBuildPipelineTimeout/build_operation_respects_context_timeout\n=== RUN   TestBuildPipelineTimeout/build_operation_uses_default_timeout_when_no_config\n=== RUN   TestBuildPipelineTimeout/compiler_respects_context_cancellation\n=== RUN   TestBuildPipelineTimeout/compiler_pools_respect_context_cancellation\n=== RUN   TestBuildPipelineTimeout/timeout_configuration_validation\n=== RUN   TestBuildPipelineTimeout/timeout_configuration_validation/positive_timeout\n=== RUN   TestBuildPipelineTimeout/timeout_configuration_validation/zero_timeout_uses_default\n=== RUN   TestBuildPipelineTimeout/timeout_configuration_validation/negative_timeout_uses_default\n=== RUN   TestBuildPipelineTimeout/multiple_config_parameters\n--- PASS: TestBuildPipelineTimeout (0.00s)\n    --- PASS: TestBuildPipelineTimeout/build_operation_respects_context_timeout (0.00s)\n    --- PASS: TestBuildPipelineTimeout/build_operation_uses_default_timeout_when_no_config (0.00s)\n    --- PASS: TestBuildPipelineTimeout/compiler_respects_context_cancellation (0.00s)\n    --- PASS: TestBuildPipelineTimeout/compiler_pools_respect_context_cancellation (0.00s)\n    --- PASS: TestBuildPipelineTimeout/timeout_configuration_validation (0.00s)\n        --- PASS: TestBuildPipelineTimeout/timeout_configuration_validation/positive_timeout (0.00s)\n        --- PASS: TestBuildPipelineTimeout/timeout_configuration_validation/zero_timeout_uses_default (0.00s)\n        --- PASS: TestBuildPipelineTimeout/timeout_configuration_validation/negative_timeout_uses_default (0.00s)\n    --- PASS: TestBuildPipelineTimeout/multiple_config_parameters (0.00s)\nPASS\nok  \tgithub.com/conneroisu/templar/internal/build\t0.007s",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "TodoWrite",
    "tool_input": {
      "todos": [
        {
          "content": "Analyze current CRC32 hashing performance",
          "status": "completed",
          "priority": "high",
          "id": "hash-analysis"
        },
        {
          "content": "Design hierarchical hashing strategy",
          "status": "completed",
          "priority": "high",
          "id": "hierarchical-design"
        },
        {
          "content": "Implement optimized hash generation for large files",
          "status": "completed",
          "priority": "high",
          "id": "large-file-hash"
        },
        {
          "content": "Task-105 - Implement Hash-Based Change Detection Optimization",
          "status": "in_progress",
          "priority": "high",
          "id": "hash-optimization"
        },
        {
          "content": "Add collision detection and fallback mechanisms",
          "status": "in_progress",
          "priority": "high",
          "id": "collision-detection"
        },
        {
          "content": "Integrate with existing build cache system",
          "status": "pending",
          "priority": "high",
          "id": "cache-integration"
        },
        {
          "content": "Add performance benchmarks and validation",
          "status": "pending",
          "priority": "high",
          "id": "benchmarks-validation"
        }
      ]
    },
    "tool_response": {
      "oldTodos": [
        {
          "content": "Task-105 - Implement Hash-Based Change Detection Optimization",
          "status": "in_progress",
          "priority": "high",
          "id": "hash-optimization"
        },
        {
          "content": "Analyze current CRC32 hashing performance",
          "status": "completed",
          "priority": "high",
          "id": "hash-analysis"
        },
        {
          "content": "Design hierarchical hashing strategy",
          "status": "in_progress",
          "priority": "high",
          "id": "hierarchical-design"
        },
        {
          "content": "Implement optimized hash generation for large files",
          "status": "pending",
          "priority": "high",
          "id": "large-file-hash"
        },
        {
          "content": "Integrate with existing build cache system",
          "status": "pending",
          "priority": "high",
          "id": "cache-integration"
        },
        {
          "content": "Add performance benchmarks and fallback mechanisms",
          "status": "pending",
          "priority": "high",
          "id": "benchmarks-fallback"
        }
      ],
      "newTodos": [
        {
          "content": "Analyze current CRC32 hashing performance",
          "status": "completed",
          "priority": "high",
          "id": "hash-analysis"
        },
        {
          "content": "Design hierarchical hashing strategy",
          "status": "completed",
          "priority": "high",
          "id": "hierarchical-design"
        },
        {
          "content": "Implement optimized hash generation for large files",
          "status": "completed",
          "priority": "high",
          "id": "large-file-hash"
        },
        {
          "content": "Task-105 - Implement Hash-Based Change Detection Optimization",
          "status": "in_progress",
          "priority": "high",
          "id": "hash-optimization"
        },
        {
          "content": "Add collision detection and fallback mechanisms",
          "status": "in_progress",
          "priority": "high",
          "id": "collision-detection"
        },
        {
          "content": "Integrate with existing build cache system",
          "status": "pending",
          "priority": "high",
          "id": "cache-integration"
        },
        {
          "content": "Add performance benchmarks and validation",
          "status": "pending",
          "priority": "high",
          "id": "benchmarks-validation"
        }
      ]
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Edit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/timeout_test.go",
      "old_string": "\tt.Run(\"multiple config parameters\", func(t *testing.T) {\n\t\t// Test multiple config parameters - should use the first one\n\t\tcfg1 := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tBuild: 1 * time.Minute,\n\t\t\t},\n\t\t}\n\t\tcfg2 := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tBuild: 2 * time.Minute,\n\t\t\t},\n\t\t}\n\n\t\tpipeline := NewBuildPipeline(1, nil, cfg1, cfg2)\n\t\ttimeout := pipeline.getBuildTimeout()\n\t\tassert.Equal(t, 1*time.Minute, timeout, \"Should use first config\")\n\t})\n}",
      "new_string": "\tt.Run(\"multiple config parameters\", func(t *testing.T) {\n\t\t// Test multiple config parameters - should use the first one\n\t\tcfg1 := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tBuild: 1 * time.Minute,\n\t\t\t},\n\t\t}\n\t\tcfg2 := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tBuild: 2 * time.Minute,\n\t\t\t},\n\t\t}\n\n\t\tpipeline := NewBuildPipeline(1, nil, cfg1, cfg2)\n\t\ttimeout := pipeline.getBuildTimeout()\n\t\tassert.Equal(t, 1*time.Minute, timeout, \"Should use first config\")\n\t})\n\n\tt.Run(\"stop with timeout functionality\", func(t *testing.T) {\n\t\tpipeline := NewBuildPipeline(1, nil)\n\t\t\n\t\t// Start pipeline\n\t\tctx := context.Background()\n\t\tpipeline.Start(ctx)\n\t\t\n\t\t// Stop with timeout should complete quickly since no work is in progress\n\t\terr := pipeline.StopWithTimeout(1 * time.Second)\n\t\tassert.NoError(t, err, \"Should stop without timeout\")\n\t})\n\n\tt.Run(\"graceful shutdown handling\", func(t *testing.T) {\n\t\tpipeline := NewBuildPipeline(1, nil)\n\t\t\n\t\t// Build should fail if pipeline is not started\n\t\tcomponent := &types.ComponentInfo{\n\t\t\tName:     \"TestComponent\",\n\t\t\tFilePath: \"test.templ\",\n\t\t\tPackage:  \"test\",\n\t\t}\n\t\t\n\t\tpipeline.Build(component)\n\t\t// No assertion here as this tests logging behavior\n\t})\n}"
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/build/timeout_test.go",
      "oldString": "\tt.Run(\"multiple config parameters\", func(t *testing.T) {\n\t\t// Test multiple config parameters - should use the first one\n\t\tcfg1 := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tBuild: 1 * time.Minute,\n\t\t\t},\n\t\t}\n\t\tcfg2 := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tBuild: 2 * time.Minute,\n\t\t\t},\n\t\t}\n\n\t\tpipeline := NewBuildPipeline(1, nil, cfg1, cfg2)\n\t\ttimeout := pipeline.getBuildTimeout()\n\t\tassert.Equal(t, 1*time.Minute, timeout, \"Should use first config\")\n\t})\n}",
      "newString": "\tt.Run(\"multiple config parameters\", func(t *testing.T) {\n\t\t// Test multiple config parameters - should use the first one\n\t\tcfg1 := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tBuild: 1 * time.Minute,\n\t\t\t},\n\t\t}\n\t\tcfg2 := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tBuild: 2 * time.Minute,\n\t\t\t},\n\t\t}\n\n\t\tpipeline := NewBuildPipeline(1, nil, cfg1, cfg2)\n\t\ttimeout := pipeline.getBuildTimeout()\n\t\tassert.Equal(t, 1*time.Minute, timeout, \"Should use first config\")\n\t})\n\n\tt.Run(\"stop with timeout functionality\", func(t *testing.T) {\n\t\tpipeline := NewBuildPipeline(1, nil)\n\t\t\n\t\t// Start pipeline\n\t\tctx := context.Background()\n\t\tpipeline.Start(ctx)\n\t\t\n\t\t// Stop with timeout should complete quickly since no work is in progress\n\t\terr := pipeline.StopWithTimeout(1 * time.Second)\n\t\tassert.NoError(t, err, \"Should stop without timeout\")\n\t})\n\n\tt.Run(\"graceful shutdown handling\", func(t *testing.T) {\n\t\tpipeline := NewBuildPipeline(1, nil)\n\t\t\n\t\t// Build should fail if pipeline is not started\n\t\tcomponent := &types.ComponentInfo{\n\t\t\tName:     \"TestComponent\",\n\t\t\tFilePath: \"test.templ\",\n\t\t\tPackage:  \"test\",\n\t\t}\n\t\t\n\t\tpipeline.Build(component)\n\t\t// No assertion here as this tests logging behavior\n\t})\n}",
      "originalFile": "package build\n\nimport (\n\t\"context\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/conneroisu/templar/internal/types\"\n\t\"github.com/stretchr/testify/assert\"\n)\n\nfunc TestBuildPipelineTimeout(t *testing.T) {\n\tt.Run(\"build operation respects context timeout\", func(t *testing.T) {\n\t\t// Create a config with very short build timeout\n\t\tcfg := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tBuild: 50 * time.Millisecond, // Very short timeout\n\t\t\t},\n\t\t}\n\n\t\t// Create pipeline with timeout config\n\t\tpipeline := NewBuildPipeline(1, nil, cfg)\n\n\t\t// Test that the getBuildTimeout returns the configured value\n\t\ttimeout := pipeline.getBuildTimeout()\n\t\tassert.Equal(t, 50*time.Millisecond, timeout, \"Should return configured timeout\")\n\t})\n\n\tt.Run(\"build operation uses default timeout when no config\", func(t *testing.T) {\n\t\t// Create pipeline without config\n\t\tpipeline := NewBuildPipeline(1, nil)\n\n\t\t// Test that the getBuildTimeout returns the default value\n\t\ttimeout := pipeline.getBuildTimeout()\n\t\tassert.Equal(t, 5*time.Minute, timeout, \"Should return default timeout\")\n\t})\n\n\tt.Run(\"compiler respects context cancellation\", func(t *testing.T) {\n\t\tcompiler := NewTemplCompiler()\n\t\t\n\t\t// Create a very short timeout context\n\t\tctx, cancel := context.WithTimeout(context.Background(), 1*time.Millisecond)\n\t\tdefer cancel()\n\t\t\n\t\t// Wait for context to timeout\n\t\t<-ctx.Done()\n\t\t\n\t\tcomponent := &types.ComponentInfo{\n\t\t\tName:     \"TestComponent\",\n\t\t\tFilePath: \"test.templ\",\n\t\t\tPackage:  \"test\",\n\t\t}\n\t\t\n\t\t_, err := compiler.Compile(ctx, component)\n\t\tassert.Error(t, err, \"Should fail due to context timeout\")\n\t\tassert.Contains(t, err.Error(), \"timed out\", \"Error should mention timeout\")\n\t})\n\n\tt.Run(\"compiler pools respect context cancellation\", func(t *testing.T) {\n\t\tcompiler := NewTemplCompiler()\n\t\tpools := NewObjectPools()\n\t\t\n\t\t// Create a very short timeout context\n\t\tctx, cancel := context.WithTimeout(context.Background(), 1*time.Millisecond)\n\t\tdefer cancel()\n\t\t\n\t\t// Wait for context to timeout\n\t\t<-ctx.Done()\n\t\t\n\t\tcomponent := &types.ComponentInfo{\n\t\t\tName:     \"TestComponent\",\n\t\t\tFilePath: \"test.templ\",\n\t\t\tPackage:  \"test\",\n\t\t}\n\t\t\n\t\t_, err := compiler.CompileWithPools(ctx, component, pools)\n\t\tassert.Error(t, err, \"Should fail due to context timeout\")\n\t\tassert.Contains(t, err.Error(), \"timed out\", \"Error should mention timeout\")\n\t})\n\n\tt.Run(\"timeout configuration validation\", func(t *testing.T) {\n\t\t// Test various timeout values\n\t\ttestCases := []struct {\n\t\t\tname    string\n\t\t\ttimeout time.Duration\n\t\t\twant    time.Duration\n\t\t}{\n\t\t\t{\n\t\t\t\tname:    \"positive timeout\",\n\t\t\t\ttimeout: 30 * time.Second,\n\t\t\t\twant:    30 * time.Second,\n\t\t\t},\n\t\t\t{\n\t\t\t\tname:    \"zero timeout uses default\",\n\t\t\t\ttimeout: 0,\n\t\t\t\twant:    5 * time.Minute,\n\t\t\t},\n\t\t\t{\n\t\t\t\tname:    \"negative timeout uses default\",\n\t\t\t\ttimeout: -1 * time.Second,\n\t\t\t\twant:    5 * time.Minute,\n\t\t\t},\n\t\t}\n\n\t\tfor _, tc := range testCases {\n\t\t\tt.Run(tc.name, func(t *testing.T) {\n\t\t\t\tcfg := &config.Config{\n\t\t\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\t\t\tBuild: tc.timeout,\n\t\t\t\t\t},\n\t\t\t\t}\n\n\t\t\t\tpipeline := NewBuildPipeline(1, nil, cfg)\n\t\t\t\tgot := pipeline.getBuildTimeout()\n\t\t\t\tassert.Equal(t, tc.want, got)\n\t\t\t})\n\t\t}\n\t})\n\n\tt.Run(\"multiple config parameters\", func(t *testing.T) {\n\t\t// Test multiple config parameters - should use the first one\n\t\tcfg1 := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tBuild: 1 * time.Minute,\n\t\t\t},\n\t\t}\n\t\tcfg2 := &config.Config{\n\t\t\tTimeouts: config.TimeoutConfig{\n\t\t\t\tBuild: 2 * time.Minute,\n\t\t\t},\n\t\t}\n\n\t\tpipeline := NewBuildPipeline(1, nil, cfg1, cfg2)\n\t\ttimeout := pipeline.getBuildTimeout()\n\t\tassert.Equal(t, 1*time.Minute, timeout, \"Should use first config\")\n\t})\n}",
      "structuredPatch": [
        {
          "oldStart": 135,
          "oldLines": 4,
          "newStart": 135,
          "newLines": 30,
          "lines": [
            "     timeout := pipeline.getBuildTimeout()",
            "     assert.Equal(t, 1*time.Minute, timeout, \"Should use first config\")",
            "   })",
            "+",
            "+  t.Run(\"stop with timeout functionality\", func(t *testing.T) {",
            "+    pipeline := NewBuildPipeline(1, nil)",
            "+    ",
            "+    // Start pipeline",
            "+    ctx := context.Background()",
            "+    pipeline.Start(ctx)",
            "+    ",
            "+    // Stop with timeout should complete quickly since no work is in progress",
            "+    err := pipeline.StopWithTimeout(1 * time.Second)",
            "+    assert.NoError(t, err, \"Should stop without timeout\")",
            "+  })",
            "+",
            "+  t.Run(\"graceful shutdown handling\", func(t *testing.T) {",
            "+    pipeline := NewBuildPipeline(1, nil)",
            "+    ",
            "+    // Build should fail if pipeline is not started",
            "+    component := &types.ComponentInfo{",
            "+      Name:     \"TestComponent\",",
            "+      FilePath: \"test.templ\",",
            "+      Package:  \"test\",",
            "+    }",
            "+    ",
            "+    pipeline.Build(component)",
            "+    // No assertion here as this tests logging behavior",
            "+  })",
            " }",
            "\\ No newline at end of file"
          ]
        }
      ],
      "userModified": false,
      "replaceAll": false
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "MultiEdit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/cmd/audit.go",
      "edits": [
        {
          "old_string": "\t// Initialize component registry and scanner\n\tcomponentRegistry := registry.NewComponentRegistry(logger)\n\tcomponentScanner := scanner.NewScanner(logger)",
          "new_string": "\t// Initialize component registry and scanner\n\tcomponentRegistry := registry.NewComponentRegistry()\n\tcomponentScanner := scanner.NewScanner()"
        }
      ]
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/cmd/audit.go",
      "edits": [
        {
          "old_string": "\t// Initialize component registry and scanner\n\tcomponentRegistry := registry.NewComponentRegistry(logger)\n\tcomponentScanner := scanner.NewScanner(logger)",
          "new_string": "\t// Initialize component registry and scanner\n\tcomponentRegistry := registry.NewComponentRegistry()\n\tcomponentScanner := scanner.NewScanner()",
          "replace_all": false
        }
      ],
      "originalFileContents": "package cmd\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/accessibility\"\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/conneroisu/templar/internal/interfaces\"\n\t\"github.com/conneroisu/templar/internal/logging\"\n\t\"github.com/conneroisu/templar/internal/registry\"\n\t\"github.com/conneroisu/templar/internal/renderer\"\n\t\"github.com/conneroisu/templar/internal/scanner\"\n\t\"github.com/spf13/cobra\"\n)\n\nvar (\n\tauditComponentName   string\n\tauditWCAGLevel       string\n\tauditOutputFormat    string\n\tauditOutputFile      string\n\tauditIncludeHTML     bool\n\tauditFixableOnly     bool\n\tauditSeverityFilter  string\n\tauditQuiet           bool\n\tauditVerbose         bool\n\tauditMaxViolations   int\n\tauditGenerateReport  bool\n\tauditShowSuggestions bool\n\tauditAutoFix         bool\n\tauditShowGuidance    bool\n\tauditGuidanceOnly    bool\n)\n\n// auditCmd represents the audit command\nvar auditCmd = &cobra.Command{\n\tUse:   \"audit [component-name]\",\n\tShort: \"Run accessibility audit on components\",\n\tLong: `Run comprehensive accessibility audits on templ components to identify\nWCAG compliance issues and get actionable suggestions for improvements.\n\nThe audit command can test individual components or all components in your project.\nIt provides detailed reports with severity levels, WCAG criteria mapping, and\nspecific suggestions for fixing accessibility issues.\n\nExamples:\n  # Audit all components\n  templar audit\n\n  # Audit specific component\n  templar audit Button\n\n  # Audit with specific WCAG level\n  templar audit --wcag-level AA\n\n  # Generate HTML report\n  templar audit --output html --output-file report.html\n\n  # Show only critical issues\n  templar audit --severity error\n\n  # Apply automatic fixes\n  templar audit --auto-fix`,\n\tValidArgsFunction: func(cmd *cobra.Command, args []string, toComplete string) ([]string, cobra.ShellCompDirective) {\n\t\treturn getComponentCompletions(toComplete), cobra.ShellCompDirectiveNoFileComp\n\t},\n\tRunE: runAuditCommand,\n}\n\nfunc init() {\n\trootCmd.AddCommand(auditCmd)\n\n\tauditCmd.Flags().StringVarP(&auditComponentName, \"component\", \"c\", \"\", \"Specific component to audit (if not provided as argument)\")\n\tauditCmd.Flags().StringVarP(&auditWCAGLevel, \"wcag-level\", \"w\", \"AA\", \"WCAG compliance level to test against (A, AA, AAA)\")\n\tauditCmd.Flags().StringVarP(&auditOutputFormat, \"output\", \"o\", \"console\", \"Output format (console, json, html, markdown)\")\n\tauditCmd.Flags().StringVarP(&auditOutputFile, \"output-file\", \"f\", \"\", \"Output file path (stdout if not specified)\")\n\tauditCmd.Flags().BoolVar(&auditIncludeHTML, \"include-html\", false, \"Include HTML snapshot in report\")\n\tauditCmd.Flags().BoolVar(&auditFixableOnly, \"fixable-only\", false, \"Show only issues that can be automatically fixed\")\n\tauditCmd.Flags().StringVarP(&auditSeverityFilter, \"severity\", \"s\", \"\", \"Filter by severity level (error, warning, info)\")\n\tauditCmd.Flags().BoolVarP(&auditQuiet, \"quiet\", \"q\", false, \"Suppress non-error output\")\n\tauditCmd.Flags().BoolVarP(&auditVerbose, \"verbose\", \"v\", false, \"Enable verbose output\")\n\tauditCmd.Flags().IntVarP(&auditMaxViolations, \"max-violations\", \"m\", 0, \"Maximum number of violations to report (0 = unlimited)\")\n\tauditCmd.Flags().BoolVar(&auditGenerateReport, \"generate-report\", false, \"Generate detailed accessibility report\")\n\tauditCmd.Flags().BoolVar(&auditShowSuggestions, \"show-suggestions\", true, \"Include suggestions in output\")\n\tauditCmd.Flags().BoolVar(&auditAutoFix, \"auto-fix\", false, \"Attempt to automatically fix issues where possible\")\n\tauditCmd.Flags().BoolVar(&auditShowGuidance, \"show-guidance\", false, \"Include detailed accessibility guidance\")\n\tauditCmd.Flags().BoolVar(&auditGuidanceOnly, \"guidance-only\", false, \"Show only guidance without running audit\")\n}\n\nfunc runAuditCommand(cmd *cobra.Command, args []string) error {\n\tctx := context.Background()\n\n\t// Determine component name from args or flag\n\tcomponentName := auditComponentName\n\tif len(args) > 0 {\n\t\tcomponentName = args[0]\n\t}\n\n\t// Handle guidance-only mode\n\tif auditGuidanceOnly {\n\t\treturn showGuidanceOnly(componentName)\n\t}\n\n\t// Initialize logging\n\tloggerConfig := &logging.LoggerConfig{\n\t\tLevel:     logging.LevelInfo,\n\t\tFormat:    \"text\", \n\t\tComponent: \"audit\",\n\t\tOutput:    os.Stdout,\n\t}\n\tif auditQuiet {\n\t\tloggerConfig.Level = logging.LevelError\n\t} else if auditVerbose {\n\t\tloggerConfig.Level = logging.LevelDebug\n\t}\n\tlogger := logging.NewLogger(loggerConfig)\n\n\t// Load configuration\n\tcfg, err := config.Load()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to load configuration: %w\", err)\n\t}\n\n\t// Initialize component registry and scanner\n\tcomponentRegistry := registry.NewComponentRegistry(logger)\n\tcomponentScanner := scanner.NewScanner(logger)\n\n\t// Scan components\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Scanning components...\")\n\t}\n\n\tcomponents, err := componentScanner.ScanDirectory(cfg.Components.ScanPaths[0])\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to scan components: %w\", err)\n\t}\n\n\t// Register components\n\tfor _, component := range components {\n\t\tcomponentRegistry.RegisterComponent(component)\n\t}\n\n\t// Create renderer\n\tcomponentRenderer := renderer.NewComponentRenderer(logger)\n\n\t// Initialize accessibility tester\n\ttesterConfig := accessibility.TesterConfig{\n\t\tDefaultWCAGLevel:    parseWCAGLevel(auditWCAGLevel),\n\t\tDefaultTimeout:      30 * time.Second,\n\t\tEnableRealTimeWarn:  false,\n\t\tMaxConcurrentTests:  1,\n\t}\n\n\ttester := accessibility.NewComponentAccessibilityTester(\n\t\tcomponentRegistry,\n\t\tcomponentRenderer,\n\t\tlogger,\n\t\ttesterConfig,\n\t)\n\n\t// Perform audit\n\tif componentName != \"\" {\n\t\treturn runSingleComponentAudit(ctx, tester, componentName, logger)\n\t} else {\n\t\treturn runAllComponentsAudit(ctx, tester, componentRegistry, logger)\n\t}\n}\n\nfunc runSingleComponentAudit(ctx context.Context, tester accessibility.AccessibilityTester, componentName string, logger logging.Logger) error {\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Running accessibility audit\", \"component\", componentName)\n\t}\n\n\t// Run accessibility test\n\treport, err := tester.TestComponent(ctx, componentName, nil)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"accessibility audit failed for %s: %w\", componentName, err)\n\t}\n\n\t// Apply filters\n\treport = applyReportFilters(report)\n\n\t// Apply auto-fixes if requested\n\tif auditAutoFix && len(report.Violations) > 0 {\n\t\tfixedCount, err := applyAutoFixes(ctx, tester, report)\n\t\tif err != nil {\n\t\t\tlogger.Warn(ctx, err, \"Failed to apply auto-fixes\")\n\t\t} else if fixedCount > 0 {\n\t\t\tlogger.Info(ctx, \"Applied automatic fixes\", \"count\", fixedCount)\n\t\t}\n\t}\n\n\t// Output results\n\treturn outputAuditResults([]*accessibility.AccessibilityReport{report}, logger)\n}\n\nfunc runAllComponentsAudit(ctx context.Context, tester accessibility.AccessibilityTester, registry interfaces.ComponentRegistry, logger logging.Logger) error {\n\tcomponents := registry.GetAllComponents()\n\t\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Running accessibility audit on all components\", \"count\", len(components))\n\t}\n\n\treports := []*accessibility.AccessibilityReport{}\n\ttotalViolations := 0\n\ttotalAutoFixes := 0\n\n\tfor i, component := range components {\n\t\tif auditVerbose {\n\t\t\tlogger.Info(ctx, \"Auditing component\", \n\t\t\t\t\"component\", component.Name,\n\t\t\t\t\"progress\", fmt.Sprintf(\"%d/%d\", i+1, len(components)))\n\t\t}\n\n\t\t// Run accessibility test\n\t\treport, err := tester.TestComponent(ctx, component.Name, nil)\n\t\tif err != nil {\n\t\t\tlogger.Warn(ctx, err, \"Failed to audit component\", \"component\", component.Name)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Apply filters\n\t\treport = applyReportFilters(report)\n\t\t\n\t\t// Apply auto-fixes if requested\n\t\tif auditAutoFix && len(report.Violations) > 0 {\n\t\t\tfixedCount, err := applyAutoFixes(ctx, tester, report)\n\t\t\tif err != nil {\n\t\t\t\tlogger.Warn(ctx, err, \"Failed to apply auto-fixes\", \"component\", component.Name)\n\t\t\t} else {\n\t\t\t\ttotalAutoFixes += fixedCount\n\t\t\t}\n\t\t}\n\n\t\treports = append(reports, report)\n\t\ttotalViolations += len(report.Violations)\n\t}\n\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Audit completed\",\n\t\t\t\"components\", len(reports),\n\t\t\t\"total_violations\", totalViolations)\n\t\t\n\t\tif auditAutoFix && totalAutoFixes > 0 {\n\t\t\tlogger.Info(ctx, \"Applied automatic fixes\", \"total_fixes\", totalAutoFixes)\n\t\t}\n\t}\n\n\t// Output results\n\treturn outputAuditResults(reports, logger)\n}\n\nfunc applyReportFilters(report *accessibility.AccessibilityReport) *accessibility.AccessibilityReport {\n\tfilteredViolations := []accessibility.AccessibilityViolation{}\n\n\tfor _, violation := range report.Violations {\n\t\t// Apply severity filter\n\t\tif auditSeverityFilter != \"\" {\n\t\t\texpectedSeverity := parseSeverity(auditSeverityFilter)\n\t\t\tif violation.Severity != expectedSeverity {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// Apply fixable filter\n\t\tif auditFixableOnly && !violation.CanAutoFix {\n\t\t\tcontinue\n\t\t}\n\n\t\tfilteredViolations = append(filteredViolations, violation)\n\t}\n\n\t// Apply max violations limit\n\tif auditMaxViolations > 0 && len(filteredViolations) > auditMaxViolations {\n\t\tfilteredViolations = filteredViolations[:auditMaxViolations]\n\t}\n\n\t// Update report\n\treport.Violations = filteredViolations\n\t\n\t// Recalculate summary\n\treport.Summary = calculateAccessibilitySummary(filteredViolations, report.Passed)\n\n\treturn report\n}\n\nfunc applyAutoFixes(ctx context.Context, tester accessibility.AccessibilityTester, report *accessibility.AccessibilityReport) (int, error) {\n\tif componentTester, ok := tester.(*accessibility.ComponentAccessibilityTester); ok {\n\t\tautoFixableViolations := []accessibility.AccessibilityViolation{}\n\t\tfor _, violation := range report.Violations {\n\t\t\tif violation.CanAutoFix {\n\t\t\t\tautoFixableViolations = append(autoFixableViolations, violation)\n\t\t\t}\n\t\t}\n\n\t\tif len(autoFixableViolations) == 0 {\n\t\t\treturn 0, nil\n\t\t}\n\n\t\t// Apply auto-fixes (this would need integration with file system)\n\t\t_, err := componentTester.AutoFix(ctx, report.HTMLSnapshot, autoFixableViolations)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\n\t\treturn len(autoFixableViolations), nil\n\t}\n\n\treturn 0, fmt.Errorf(\"auto-fix not supported for this tester type\")\n}\n\nfunc outputAuditResults(reports []*accessibility.AccessibilityReport, logger logging.Logger) error {\n\tswitch auditOutputFormat {\n\tcase \"json\":\n\t\treturn outputJSON(reports)\n\tcase \"html\":\n\t\treturn outputHTML(reports)\n\tcase \"markdown\":\n\t\treturn outputMarkdown(reports)\n\tcase \"console\":\n\t\tfallthrough\n\tdefault:\n\t\treturn outputConsole(reports, logger)\n\t}\n}\n\nfunc outputJSON(reports []*accessibility.AccessibilityReport) error {\n\toutput := reports\n\tif len(reports) == 1 {\n\t\toutput = reports[0] // Single component audit returns single report\n\t}\n\n\tjsonData, err := json.MarshalIndent(output, \"\", \"  \")\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to marshal JSON: %w\", err)\n\t}\n\n\treturn writeOutput(string(jsonData))\n}\n\nfunc outputHTML(reports []*accessibility.AccessibilityReport) error {\n\thtml := generateHTMLReport(reports)\n\treturn writeOutput(html)\n}\n\nfunc outputMarkdown(reports []*accessibility.AccessibilityReport) error {\n\tmarkdown := generateMarkdownReport(reports)\n\treturn writeOutput(markdown)\n}\n\nfunc outputConsole(reports []*accessibility.AccessibilityReport, logger logging.Logger) error {\n\tctx := context.Background()\n\t\n\tif len(reports) == 0 {\n\t\tfmt.Println(\"No components audited.\")\n\t\treturn nil\n\t}\n\n\t// Summary statistics\n\ttotalComponents := len(reports)\n\ttotalViolations := 0\n\tcriticalViolations := 0\n\tcomponentsWithIssues := 0\n\toverallScoreSum := 0.0\n\n\tfor _, report := range reports {\n\t\ttotalViolations += len(report.Violations)\n\t\toverallScoreSum += report.Summary.OverallScore\n\t\t\n\t\tif len(report.Violations) > 0 {\n\t\t\tcomponentsWithIssues++\n\t\t}\n\t\t\n\t\tfor _, violation := range report.Violations {\n\t\t\tif violation.Impact == accessibility.ImpactCritical {\n\t\t\t\tcriticalViolations++\n\t\t\t}\n\t\t}\n\t}\n\n\taverageScore := overallScoreSum / float64(totalComponents)\n\n\t// Print summary\n\tfmt.Printf(\"\\n\ud83d\udd0d Accessibility Audit Summary\\n\")\n\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\tfmt.Printf(\"Components audited:     %d\\n\", totalComponents)\n\tfmt.Printf(\"Components with issues: %d\\n\", componentsWithIssues)\n\tfmt.Printf(\"Total violations:       %d\\n\", totalViolations)\n\tfmt.Printf(\"Critical violations:    %d\\n\", criticalViolations)\n\tfmt.Printf(\"Average score:          %.1f/100\\n\", averageScore)\n\t\n\t// Overall status\n\tvar status string\n\tvar statusIcon string\n\tif criticalViolations > 0 {\n\t\tstatus = \"CRITICAL ISSUES FOUND\"\n\t\tstatusIcon = \"\ud83d\udea8\"\n\t} else if totalViolations > 0 {\n\t\tstatus = \"ISSUES FOUND\"\n\t\tstatusIcon = \"\u26a0\ufe0f\"\n\t} else {\n\t\tstatus = \"ALL CHECKS PASSED\"\n\t\tstatusIcon = \"\u2705\"\n\t}\n\t\n\tfmt.Printf(\"Status:                 %s %s\\n\\n\", statusIcon, status)\n\n\t// Detailed component results\n\tif auditVerbose || len(reports) == 1 {\n\t\tfor _, report := range reports {\n\t\t\toutputComponentDetails(report)\n\t\t}\n\t} else if totalViolations > 0 {\n\t\t// Show only components with issues\n\t\tfmt.Printf(\"Components with accessibility issues:\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tfor _, report := range reports {\n\t\t\tif len(report.Violations) > 0 {\n\t\t\t\toutputComponentSummary(report)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Show suggestions if enabled\n\tif auditShowSuggestions && totalViolations > 0 {\n\t\tfmt.Printf(\"\\n\ud83d\udca1 Top Suggestions\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tsuggestions := aggregateSuggestions(reports)\n\t\tfor i, suggestion := range suggestions {\n\t\t\tif i >= 5 { // Limit to top 5\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfmt.Printf(\"%d. %s\\n\", i+1, suggestion.Title)\n\t\t\tif suggestion.Description != \"\" {\n\t\t\t\tfmt.Printf(\"   %s\\n\", suggestion.Description)\n\t\t\t}\n\t\t\tfmt.Printf(\"\\n\")\n\t\t}\n\t}\n\t\n\t// Show detailed guidance if enabled and there are violations\n\tif totalViolations > 0 {\n\t\tallViolations := []accessibility.AccessibilityViolation{}\n\t\tfor _, report := range reports {\n\t\t\tallViolations = append(allViolations, report.Violations...)\n\t\t}\n\t\tshowGuidanceForViolations(allViolations)\n\t}\n\n\treturn nil\n}\n\nfunc outputComponentDetails(report *accessibility.AccessibilityReport) {\n\tcomponentName := report.ComponentName\n\tif componentName == \"\" {\n\t\tcomponentName = \"Unknown Component\"\n\t}\n\n\tscoreColor := getScoreColor(report.Summary.OverallScore)\n\t\n\tfmt.Printf(\"\ud83d\udce6 %s (Score: %s%.1f/100%s)\\n\", \n\t\tcomponentName, scoreColor, report.Summary.OverallScore, \"\\033[0m\")\n\tfmt.Printf(\"   File: %s\\n\", report.ComponentFile)\n\t\n\tif len(report.Violations) == 0 {\n\t\tfmt.Printf(\"   \u2705 No accessibility issues found\\n\\n\")\n\t\treturn\n\t}\n\n\t// Group violations by severity\n\terrorViolations := []accessibility.AccessibilityViolation{}\n\twarningViolations := []accessibility.AccessibilityViolation{}\n\tinfoViolations := []accessibility.AccessibilityViolation{}\n\n\tfor _, violation := range report.Violations {\n\t\tswitch violation.Severity {\n\t\tcase accessibility.SeverityError:\n\t\t\terrorViolations = append(errorViolations, violation)\n\t\tcase accessibility.SeverityWarning:\n\t\t\twarningViolations = append(warningViolations, violation)\n\t\tcase accessibility.SeverityInfo:\n\t\t\tinfoViolations = append(infoViolations, violation)\n\t\t}\n\t}\n\n\t// Output violations by severity\n\tif len(errorViolations) > 0 {\n\t\tfmt.Printf(\"   \ud83d\udea8 Errors (%d):\\n\", len(errorViolations))\n\t\tfor _, violation := range errorViolations {\n\t\t\toutputViolation(violation, \"     \")\n\t\t}\n\t}\n\t\n\tif len(warningViolations) > 0 {\n\t\tfmt.Printf(\"   \u26a0\ufe0f  Warnings (%d):\\n\", len(warningViolations))\n\t\tfor _, violation := range warningViolations {\n\t\t\toutputViolation(violation, \"     \")\n\t\t}\n\t}\n\t\n\tif len(infoViolations) > 0 && auditVerbose {\n\t\tfmt.Printf(\"   \u2139\ufe0f  Info (%d):\\n\", len(infoViolations))\n\t\tfor _, violation := range infoViolations {\n\t\t\toutputViolation(violation, \"     \")\n\t\t}\n\t}\n\n\tfmt.Printf(\"\\n\")\n}\n\nfunc outputComponentSummary(report *accessibility.AccessibilityReport) {\n\tcomponentName := report.ComponentName\n\tif componentName == \"\" {\n\t\tcomponentName = \"Unknown Component\"\n\t}\n\n\terrorCount := 0\n\twarningCount := 0\n\tcriticalCount := 0\n\n\tfor _, violation := range report.Violations {\n\t\tswitch violation.Severity {\n\t\tcase accessibility.SeverityError:\n\t\t\terrorCount++\n\t\tcase accessibility.SeverityWarning:\n\t\t\twarningCount++\n\t\t}\n\t\t\n\t\tif violation.Impact == accessibility.ImpactCritical {\n\t\t\tcriticalCount++\n\t\t}\n\t}\n\n\tscoreColor := getScoreColor(report.Summary.OverallScore)\n\t\n\tfmt.Printf(\"\ud83d\udce6 %s %s(%.1f/100)%s\\n\", componentName, scoreColor, report.Summary.OverallScore, \"\\033[0m\")\n\t\n\tif criticalCount > 0 {\n\t\tfmt.Printf(\"   \ud83d\udea8 %d critical issue(s)\\n\", criticalCount)\n\t}\n\tif errorCount > 0 {\n\t\tfmt.Printf(\"   \u274c %d error(s)\\n\", errorCount)\n\t}\n\tif warningCount > 0 {\n\t\tfmt.Printf(\"   \u26a0\ufe0f  %d warning(s)\\n\", warningCount)\n\t}\n\t\n\tfmt.Printf(\"\\n\")\n}\n\nfunc outputViolation(violation accessibility.AccessibilityViolation, indent string) {\n\tfmt.Printf(\"%s\u2022 %s\\n\", indent, violation.Message)\n\tfmt.Printf(\"%s  Rule: %s | WCAG: %s %s\\n\", \n\t\tindent, violation.Rule, violation.WCAG.Level, violation.WCAG.Criteria)\n\t\n\tif violation.Element != \"\" {\n\t\tfmt.Printf(\"%s  Element: <%s>\\n\", indent, violation.Element)\n\t}\n\t\n\tif auditShowSuggestions && len(violation.Suggestions) > 0 {\n\t\tfmt.Printf(\"%s  \ud83d\udca1 %s\\n\", indent, violation.Suggestions[0].Title)\n\t\tif violation.Suggestions[0].Code != \"\" && auditVerbose {\n\t\t\tfmt.Printf(\"%s     Code: %s\\n\", indent, violation.Suggestions[0].Code)\n\t\t}\n\t}\n\t\n\tif violation.CanAutoFix {\n\t\tfmt.Printf(\"%s  \ud83d\udd27 Auto-fixable\\n\", indent)\n\t}\n\t\n\tfmt.Printf(\"\\n\")\n}\n\nfunc writeOutput(content string) error {\n\tif auditOutputFile != \"\" {\n\t\t// Ensure output directory exists\n\t\tdir := filepath.Dir(auditOutputFile)\n\t\tif err := os.MkdirAll(dir, 0755); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to create output directory: %w\", err)\n\t\t}\n\t\t\n\t\t// Write to file\n\t\tif err := os.WriteFile(auditOutputFile, []byte(content), 0644); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to write output file: %w\", err)\n\t\t}\n\t\t\n\t\tfmt.Printf(\"Report written to: %s\\n\", auditOutputFile)\n\t\treturn nil\n\t}\n\t\n\t// Write to stdout\n\tfmt.Print(content)\n\treturn nil\n}\n\n// Helper functions\nfunc parseWCAGLevel(level string) accessibility.WCAGLevel {\n\tswitch strings.ToUpper(level) {\n\tcase \"A\":\n\t\treturn accessibility.WCAGLevelA\n\tcase \"AA\":\n\t\treturn accessibility.WCAGLevelAA\n\tcase \"AAA\":\n\t\treturn accessibility.WCAGLevelAAA\n\tdefault:\n\t\treturn accessibility.WCAGLevelAA\n\t}\n}\n\nfunc parseSeverity(severity string) accessibility.ViolationSeverity {\n\tswitch strings.ToLower(severity) {\n\tcase \"error\":\n\t\treturn accessibility.SeverityError\n\tcase \"warning\":\n\t\treturn accessibility.SeverityWarning\n\tcase \"info\":\n\t\treturn accessibility.SeverityInfo\n\tdefault:\n\t\treturn accessibility.SeverityWarning\n\t}\n}\n\nfunc getScoreColor(score float64) string {\n\tif score >= 90 {\n\t\treturn \"\\033[32m\" // Green\n\t} else if score >= 70 {\n\t\treturn \"\\033[33m\" // Yellow\n\t} else {\n\t\treturn \"\\033[31m\" // Red\n\t}\n}\n\nfunc aggregateSuggestions(reports []*accessibility.AccessibilityReport) []accessibility.AccessibilitySuggestion {\n\tsuggestionMap := make(map[string]*accessibility.AccessibilitySuggestion)\n\tsuggestionCounts := make(map[string]int)\n\t\n\tfor _, report := range reports {\n\t\tfor _, violation := range report.Violations {\n\t\t\tfor _, suggestion := range violation.Suggestions {\n\t\t\t\tkey := fmt.Sprintf(\"%s_%s\", suggestion.Type, suggestion.Title)\n\t\t\t\tsuggestionCounts[key]++\n\t\t\t\t\n\t\t\t\tif existing, exists := suggestionMap[key]; !exists || suggestion.Priority < existing.Priority {\n\t\t\t\t\tsuggestionCopy := suggestion\n\t\t\t\t\tsuggestionMap[key] = &suggestionCopy\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t\n\t// Convert to slice and sort by frequency and priority\n\tsuggestions := []accessibility.AccessibilitySuggestion{}\n\tfor key, suggestion := range suggestionMap {\n\t\t// Adjust priority based on frequency (more frequent = higher priority)\n\t\tsuggestion.Priority -= suggestionCounts[key] // Lower number = higher priority\n\t\tsuggestions = append(suggestions, *suggestion)\n\t}\n\t\n\t// Sort by priority\n\tsort.Slice(suggestions, func(i, j int) bool {\n\t\treturn suggestions[i].Priority < suggestions[j].Priority\n\t})\n\t\n\treturn suggestions\n}\n\nfunc calculateAccessibilitySummary(violations []accessibility.AccessibilityViolation, passed []accessibility.AccessibilityRule) accessibility.AccessibilitySummary {\n\tsummary := accessibility.AccessibilitySummary{\n\t\tTotalRules:      len(passed) + len(violations),\n\t\tPassedRules:     len(passed),\n\t\tFailedRules:     len(violations),\n\t\tTotalViolations: len(violations),\n\t}\n\t\n\tfor _, violation := range violations {\n\t\tswitch violation.Severity {\n\t\tcase accessibility.SeverityError:\n\t\t\tsummary.ErrorViolations++\n\t\tcase accessibility.SeverityWarning:\n\t\t\tsummary.WarnViolations++\n\t\tcase accessibility.SeverityInfo:\n\t\t\tsummary.InfoViolations++\n\t\t}\n\t\t\n\t\tswitch violation.Impact {\n\t\tcase accessibility.ImpactCritical:\n\t\t\tsummary.CriticalImpact++\n\t\tcase accessibility.ImpactSerious:\n\t\t\tsummary.SeriousImpact++\n\t\tcase accessibility.ImpactModerate:\n\t\t\tsummary.ModerateImpact++\n\t\tcase accessibility.ImpactMinor:\n\t\t\tsummary.MinorImpact++\n\t\t}\n\t}\n\t\n\t// Calculate overall score\n\tif summary.TotalRules > 0 {\n\t\tsummary.OverallScore = float64(summary.PassedRules) / float64(summary.TotalRules) * 100\n\t}\n\t\n\treturn summary\n}\n\nfunc generateHTMLReport(reports []*accessibility.AccessibilityReport) string {\n\t// This would generate a comprehensive HTML report\n\t// For brevity, returning a simplified version\n\thtml := `<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Accessibility Audit Report</title>\n    <style>\n        body { font-family: Arial, sans-serif; margin: 40px; }\n        .header { background: #f5f5f5; padding: 20px; border-radius: 8px; }\n        .component { margin: 20px 0; padding: 20px; border: 1px solid #ddd; border-radius: 8px; }\n        .violation { margin: 10px 0; padding: 10px; background: #fff3cd; border-left: 4px solid #ffc107; }\n        .error { background: #f8d7da; border-left-color: #dc3545; }\n        .success { background: #d4edda; border-left-color: #28a745; }\n    </style>\n</head>\n<body>\n    <div class=\"header\">\n        <h1>Accessibility Audit Report</h1>\n        <p>Generated on: ` + time.Now().Format(\"2006-01-02 15:04:05\") + `</p>\n    </div>`\n\n\tfor _, report := range reports {\n\t\thtml += fmt.Sprintf(`\n    <div class=\"component\">\n        <h2>%s</h2>\n        <p>Score: %.1f/100</p>\n        <p>Violations: %d</p>\n    </div>`, report.ComponentName, report.Summary.OverallScore, len(report.Violations))\n\t}\n\n\thtml += `\n</body>\n</html>`\n\n\treturn html\n}\n\nfunc generateMarkdownReport(reports []*accessibility.AccessibilityReport) string {\n\tmd := fmt.Sprintf(\"# Accessibility Audit Report\\n\\nGenerated on: %s\\n\\n\", time.Now().Format(\"2006-01-02 15:04:05\"))\n\t\n\tfor _, report := range reports {\n\t\tmd += fmt.Sprintf(\"## %s\\n\\n\", report.ComponentName)\n\t\tmd += fmt.Sprintf(\"- **Score**: %.1f/100\\n\", report.Summary.OverallScore)\n\t\tmd += fmt.Sprintf(\"- **Violations**: %d\\n\\n\", len(report.Violations))\n\t\t\n\t\tif len(report.Violations) > 0 {\n\t\t\tmd += \"### Issues Found\\n\\n\"\n\t\t\tfor _, violation := range report.Violations {\n\t\t\t\tmd += fmt.Sprintf(\"- **%s**: %s\\n\", violation.Rule, violation.Message)\n\t\t\t}\n\t\t\tmd += \"\\n\"\n\t\t}\n\t}\n\t\n\treturn md\n}\n\nfunc getComponentCompletions(toComplete string) []string {\n\t// This would integrate with the component registry to provide completions\n\t// For now, returning empty slice\n\treturn []string{}\n}\n\n// showGuidanceOnly displays accessibility guidance without running an audit\nfunc showGuidanceOnly(componentName string) error {\n\tguide := accessibility.NewAccessibilityGuide()\n\t\n\tif componentName != \"\" {\n\t\t// Show component-specific guidance\n\t\tfmt.Printf(\"\ud83c\udfaf Accessibility Guidance for %s Component\\n\", componentName)\n\t\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\t\t\n\t\tguidanceText := guide.GetComponentGuidanceText(componentName)\n\t\tfmt.Print(guidanceText)\n\t\t\n\t\t// Also show general guidance applicable to all components\n\t\tfmt.Printf(\"\\n\ud83d\udccb General Accessibility Guidelines\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tquickStart := guide.GetQuickStartGuide()\n\t\tfor i, item := range quickStart {\n\t\t\tif i >= 3 { // Limit to top 3 for brevity\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfmt.Printf(\"%d. %s\\n\", i+1, item.Title)\n\t\t\tfmt.Printf(\"   %s\\n\\n\", item.Description)\n\t\t}\n\t\t\n\t} else {\n\t\t// Show general accessibility guidance\n\t\tfmt.Printf(\"\ud83c\udf1f Accessibility Quick Start Guide\\n\")\n\t\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\t\t\n\t\tquickStart := guide.GetQuickStartGuide()\n\t\tfor i, item := range quickStart {\n\t\t\tfmt.Printf(\"%d. %s\\n\", i+1, item.Title)\n\t\t\tfmt.Printf(\"   %s\\n\", item.Description)\n\t\t\t\n\t\t\tif len(item.Examples) > 0 {\n\t\t\t\texample := item.Examples[0]\n\t\t\t\tif example.BadCode != \"\" {\n\t\t\t\t\tfmt.Printf(\"   \u274c Avoid: %s\\n\", strings.ReplaceAll(example.BadCode, \"\\n\", \" \"))\n\t\t\t\t}\n\t\t\t\tif example.GoodCode != \"\" {\n\t\t\t\t\tfmt.Printf(\"   \u2705 Use: %s\\n\", strings.ReplaceAll(example.GoodCode, \"\\n\", \" \"))\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Printf(\"\\n\")\n\t\t}\n\t\t\n\t\tfmt.Printf(\"\ud83d\udca1 Advanced Guidelines\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tbestPractices := guide.GetBestPracticesGuide()\n\t\tfor i, item := range bestPractices {\n\t\t\tif i >= 3 { // Limit for readability\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfmt.Printf(\"\u2022 %s\\n\", item.Title)\n\t\t\tfmt.Printf(\"  %s\\n\\n\", item.Description)\n\t\t}\n\t\t\n\t\tfmt.Printf(\"\ud83d\udcda Additional Resources\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\")\n\t\tfmt.Printf(\"\u2022 WCAG Quick Reference: https://www.w3.org/WAI/WCAG21/quickref/\\n\")\n\t\tfmt.Printf(\"\u2022 WebAIM Guidelines: https://webaim.org/\\n\")\n\t\tfmt.Printf(\"\u2022 A11y Project: https://www.a11yproject.com/\\n\")\n\t\tfmt.Printf(\"\u2022 MDN Accessibility: https://developer.mozilla.org/en-US/docs/Web/Accessibility\\n\\n\")\n\t\t\n\t\tfmt.Printf(\"\ud83d\udd27 To audit your components, run:\\n\")\n\t\tfmt.Printf(\"   templar audit              # Audit all components\\n\")\n\t\tfmt.Printf(\"   templar audit Button       # Audit specific component\\n\")\n\t\tfmt.Printf(\"   templar audit --help        # See all options\\n\")\n\t}\n\t\n\treturn nil\n}\n\n// showGuidanceForViolations displays guidance for specific accessibility violations\nfunc showGuidanceForViolations(violations []accessibility.AccessibilityViolation) {\n\tif !auditShowGuidance || len(violations) == 0 {\n\t\treturn\n\t}\n\t\n\tfmt.Printf(\"\\n\ud83c\udf93 Accessibility Guidance\\n\")\n\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\t\n\tguide := accessibility.NewAccessibilityGuide()\n\t\n\t// Group violations by rule to avoid duplicate guidance\n\truleMap := make(map[string]bool)\n\tuniqueRules := []string{}\n\t\n\tfor _, violation := range violations {\n\t\tif !ruleMap[violation.Rule] {\n\t\t\truleMap[violation.Rule] = true\n\t\t\tuniqueRules = append(uniqueRules, violation.Rule)\n\t\t}\n\t}\n\t\n\t// Show guidance for each unique rule\n\tfor i, rule := range uniqueRules {\n\t\tif i > 0 {\n\t\t\tfmt.Printf(\"\\n\" + strings.Repeat(\"\u2500\", 60) + \"\\n\\n\")\n\t\t}\n\t\t\n\t\tguidanceText := guide.GetGuidanceText(rule)\n\t\tfmt.Print(guidanceText)\n\t}\n}",
      "structuredPatch": [
        {
          "oldStart": 128,
          "oldLines": 8,
          "newStart": 128,
          "newLines": 8,
          "lines": [
            "   }",
            " ",
            "   // Initialize component registry and scanner",
            "-  componentRegistry := registry.NewComponentRegistry(logger)",
            "-  componentScanner := scanner.NewScanner(logger)",
            "+  componentRegistry := registry.NewComponentRegistry()",
            "+  componentScanner := scanner.NewScanner()",
            " ",
            "   // Scan components",
            "   if !auditQuiet {"
          ]
        }
      ],
      "userModified": false
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "go test ./internal/build -run TestBuildPipelineTimeout -v",
      "description": "Test the updated timeout functionality with cancellation"
    },
    "tool_response": {
      "stdout": "=== RUN   TestBuildPipelineTimeout\n=== RUN   TestBuildPipelineTimeout/build_operation_respects_context_timeout\n=== RUN   TestBuildPipelineTimeout/build_operation_uses_default_timeout_when_no_config\n=== RUN   TestBuildPipelineTimeout/compiler_respects_context_cancellation\n=== RUN   TestBuildPipelineTimeout/compiler_pools_respect_context_cancellation\n=== RUN   TestBuildPipelineTimeout/timeout_configuration_validation\n=== RUN   TestBuildPipelineTimeout/timeout_configuration_validation/positive_timeout\n=== RUN   TestBuildPipelineTimeout/timeout_configuration_validation/zero_timeout_uses_default\n=== RUN   TestBuildPipelineTimeout/timeout_configuration_validation/negative_timeout_uses_default\n=== RUN   TestBuildPipelineTimeout/multiple_config_parameters\n=== RUN   TestBuildPipelineTimeout/stop_with_timeout_functionality\n=== RUN   TestBuildPipelineTimeout/graceful_shutdown_handling\nError: Build pipeline not started, dropping task for component TestComponent\n--- PASS: TestBuildPipelineTimeout (0.00s)\n    --- PASS: TestBuildPipelineTimeout/build_operation_respects_context_timeout (0.00s)\n    --- PASS: TestBuildPipelineTimeout/build_operation_uses_default_timeout_when_no_config (0.00s)\n    --- PASS: TestBuildPipelineTimeout/compiler_respects_context_cancellation (0.00s)\n    --- PASS: TestBuildPipelineTimeout/compiler_pools_respect_context_cancellation (0.00s)\n    --- PASS: TestBuildPipelineTimeout/timeout_configuration_validation (0.00s)\n        --- PASS: TestBuildPipelineTimeout/timeout_configuration_validation/positive_timeout (0.00s)\n        --- PASS: TestBuildPipelineTimeout/timeout_configuration_validation/zero_timeout_uses_default (0.00s)\n        --- PASS: TestBuildPipelineTimeout/timeout_configuration_validation/negative_timeout_uses_default (0.00s)\n    --- PASS: TestBuildPipelineTimeout/multiple_config_parameters (0.00s)\n    --- PASS: TestBuildPipelineTimeout/stop_with_timeout_functionality (0.00s)\n    --- PASS: TestBuildPipelineTimeout/graceful_shutdown_handling (0.00s)\nPASS\nok  \tgithub.com/conneroisu/templar/internal/build\t0.007s",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "MultiEdit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/cmd/audit.go",
      "edits": [
        {
          "old_string": "\tcomponentRenderer := renderer.NewComponentRenderer(logger)",
          "new_string": "\tcomponentRenderer := renderer.NewComponentRenderer()"
        }
      ]
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/cmd/audit.go",
      "edits": [
        {
          "old_string": "\tcomponentRenderer := renderer.NewComponentRenderer(logger)",
          "new_string": "\tcomponentRenderer := renderer.NewComponentRenderer()",
          "replace_all": false
        }
      ],
      "originalFileContents": "package cmd\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strings\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/accessibility\"\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/conneroisu/templar/internal/interfaces\"\n\t\"github.com/conneroisu/templar/internal/logging\"\n\t\"github.com/conneroisu/templar/internal/registry\"\n\t\"github.com/conneroisu/templar/internal/renderer\"\n\t\"github.com/conneroisu/templar/internal/scanner\"\n\t\"github.com/spf13/cobra\"\n)\n\nvar (\n\tauditComponentName   string\n\tauditWCAGLevel       string\n\tauditOutputFormat    string\n\tauditOutputFile      string\n\tauditIncludeHTML     bool\n\tauditFixableOnly     bool\n\tauditSeverityFilter  string\n\tauditQuiet           bool\n\tauditVerbose         bool\n\tauditMaxViolations   int\n\tauditGenerateReport  bool\n\tauditShowSuggestions bool\n\tauditAutoFix         bool\n\tauditShowGuidance    bool\n\tauditGuidanceOnly    bool\n)\n\n// auditCmd represents the audit command\nvar auditCmd = &cobra.Command{\n\tUse:   \"audit [component-name]\",\n\tShort: \"Run accessibility audit on components\",\n\tLong: `Run comprehensive accessibility audits on templ components to identify\nWCAG compliance issues and get actionable suggestions for improvements.\n\nThe audit command can test individual components or all components in your project.\nIt provides detailed reports with severity levels, WCAG criteria mapping, and\nspecific suggestions for fixing accessibility issues.\n\nExamples:\n  # Audit all components\n  templar audit\n\n  # Audit specific component\n  templar audit Button\n\n  # Audit with specific WCAG level\n  templar audit --wcag-level AA\n\n  # Generate HTML report\n  templar audit --output html --output-file report.html\n\n  # Show only critical issues\n  templar audit --severity error\n\n  # Apply automatic fixes\n  templar audit --auto-fix`,\n\tValidArgsFunction: func(cmd *cobra.Command, args []string, toComplete string) ([]string, cobra.ShellCompDirective) {\n\t\treturn getComponentCompletions(toComplete), cobra.ShellCompDirectiveNoFileComp\n\t},\n\tRunE: runAuditCommand,\n}\n\nfunc init() {\n\trootCmd.AddCommand(auditCmd)\n\n\tauditCmd.Flags().StringVarP(&auditComponentName, \"component\", \"c\", \"\", \"Specific component to audit (if not provided as argument)\")\n\tauditCmd.Flags().StringVarP(&auditWCAGLevel, \"wcag-level\", \"w\", \"AA\", \"WCAG compliance level to test against (A, AA, AAA)\")\n\tauditCmd.Flags().StringVarP(&auditOutputFormat, \"output\", \"o\", \"console\", \"Output format (console, json, html, markdown)\")\n\tauditCmd.Flags().StringVarP(&auditOutputFile, \"output-file\", \"f\", \"\", \"Output file path (stdout if not specified)\")\n\tauditCmd.Flags().BoolVar(&auditIncludeHTML, \"include-html\", false, \"Include HTML snapshot in report\")\n\tauditCmd.Flags().BoolVar(&auditFixableOnly, \"fixable-only\", false, \"Show only issues that can be automatically fixed\")\n\tauditCmd.Flags().StringVarP(&auditSeverityFilter, \"severity\", \"s\", \"\", \"Filter by severity level (error, warning, info)\")\n\tauditCmd.Flags().BoolVarP(&auditQuiet, \"quiet\", \"q\", false, \"Suppress non-error output\")\n\tauditCmd.Flags().BoolVarP(&auditVerbose, \"verbose\", \"v\", false, \"Enable verbose output\")\n\tauditCmd.Flags().IntVarP(&auditMaxViolations, \"max-violations\", \"m\", 0, \"Maximum number of violations to report (0 = unlimited)\")\n\tauditCmd.Flags().BoolVar(&auditGenerateReport, \"generate-report\", false, \"Generate detailed accessibility report\")\n\tauditCmd.Flags().BoolVar(&auditShowSuggestions, \"show-suggestions\", true, \"Include suggestions in output\")\n\tauditCmd.Flags().BoolVar(&auditAutoFix, \"auto-fix\", false, \"Attempt to automatically fix issues where possible\")\n\tauditCmd.Flags().BoolVar(&auditShowGuidance, \"show-guidance\", false, \"Include detailed accessibility guidance\")\n\tauditCmd.Flags().BoolVar(&auditGuidanceOnly, \"guidance-only\", false, \"Show only guidance without running audit\")\n}\n\nfunc runAuditCommand(cmd *cobra.Command, args []string) error {\n\tctx := context.Background()\n\n\t// Determine component name from args or flag\n\tcomponentName := auditComponentName\n\tif len(args) > 0 {\n\t\tcomponentName = args[0]\n\t}\n\n\t// Handle guidance-only mode\n\tif auditGuidanceOnly {\n\t\treturn showGuidanceOnly(componentName)\n\t}\n\n\t// Initialize logging\n\tloggerConfig := &logging.LoggerConfig{\n\t\tLevel:     logging.LevelInfo,\n\t\tFormat:    \"text\", \n\t\tComponent: \"audit\",\n\t\tOutput:    os.Stdout,\n\t}\n\tif auditQuiet {\n\t\tloggerConfig.Level = logging.LevelError\n\t} else if auditVerbose {\n\t\tloggerConfig.Level = logging.LevelDebug\n\t}\n\tlogger := logging.NewLogger(loggerConfig)\n\n\t// Load configuration\n\tcfg, err := config.Load()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to load configuration: %w\", err)\n\t}\n\n\t// Initialize component registry and scanner\n\tcomponentRegistry := registry.NewComponentRegistry()\n\tcomponentScanner := scanner.NewScanner()\n\n\t// Scan components\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Scanning components...\")\n\t}\n\n\tcomponents, err := componentScanner.ScanDirectory(cfg.Components.ScanPaths[0])\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to scan components: %w\", err)\n\t}\n\n\t// Register components\n\tfor _, component := range components {\n\t\tcomponentRegistry.RegisterComponent(component)\n\t}\n\n\t// Create renderer\n\tcomponentRenderer := renderer.NewComponentRenderer(logger)\n\n\t// Initialize accessibility tester\n\ttesterConfig := accessibility.TesterConfig{\n\t\tDefaultWCAGLevel:    parseWCAGLevel(auditWCAGLevel),\n\t\tDefaultTimeout:      30 * time.Second,\n\t\tEnableRealTimeWarn:  false,\n\t\tMaxConcurrentTests:  1,\n\t}\n\n\ttester := accessibility.NewComponentAccessibilityTester(\n\t\tcomponentRegistry,\n\t\tcomponentRenderer,\n\t\tlogger,\n\t\ttesterConfig,\n\t)\n\n\t// Perform audit\n\tif componentName != \"\" {\n\t\treturn runSingleComponentAudit(ctx, tester, componentName, logger)\n\t} else {\n\t\treturn runAllComponentsAudit(ctx, tester, componentRegistry, logger)\n\t}\n}\n\nfunc runSingleComponentAudit(ctx context.Context, tester accessibility.AccessibilityTester, componentName string, logger logging.Logger) error {\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Running accessibility audit\", \"component\", componentName)\n\t}\n\n\t// Run accessibility test\n\treport, err := tester.TestComponent(ctx, componentName, nil)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"accessibility audit failed for %s: %w\", componentName, err)\n\t}\n\n\t// Apply filters\n\treport = applyReportFilters(report)\n\n\t// Apply auto-fixes if requested\n\tif auditAutoFix && len(report.Violations) > 0 {\n\t\tfixedCount, err := applyAutoFixes(ctx, tester, report)\n\t\tif err != nil {\n\t\t\tlogger.Warn(ctx, err, \"Failed to apply auto-fixes\")\n\t\t} else if fixedCount > 0 {\n\t\t\tlogger.Info(ctx, \"Applied automatic fixes\", \"count\", fixedCount)\n\t\t}\n\t}\n\n\t// Output results\n\treturn outputAuditResults([]*accessibility.AccessibilityReport{report}, logger)\n}\n\nfunc runAllComponentsAudit(ctx context.Context, tester accessibility.AccessibilityTester, registry interfaces.ComponentRegistry, logger logging.Logger) error {\n\tcomponents := registry.GetAllComponents()\n\t\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Running accessibility audit on all components\", \"count\", len(components))\n\t}\n\n\treports := []*accessibility.AccessibilityReport{}\n\ttotalViolations := 0\n\ttotalAutoFixes := 0\n\n\tfor i, component := range components {\n\t\tif auditVerbose {\n\t\t\tlogger.Info(ctx, \"Auditing component\", \n\t\t\t\t\"component\", component.Name,\n\t\t\t\t\"progress\", fmt.Sprintf(\"%d/%d\", i+1, len(components)))\n\t\t}\n\n\t\t// Run accessibility test\n\t\treport, err := tester.TestComponent(ctx, component.Name, nil)\n\t\tif err != nil {\n\t\t\tlogger.Warn(ctx, err, \"Failed to audit component\", \"component\", component.Name)\n\t\t\tcontinue\n\t\t}\n\n\t\t// Apply filters\n\t\treport = applyReportFilters(report)\n\t\t\n\t\t// Apply auto-fixes if requested\n\t\tif auditAutoFix && len(report.Violations) > 0 {\n\t\t\tfixedCount, err := applyAutoFixes(ctx, tester, report)\n\t\t\tif err != nil {\n\t\t\t\tlogger.Warn(ctx, err, \"Failed to apply auto-fixes\", \"component\", component.Name)\n\t\t\t} else {\n\t\t\t\ttotalAutoFixes += fixedCount\n\t\t\t}\n\t\t}\n\n\t\treports = append(reports, report)\n\t\ttotalViolations += len(report.Violations)\n\t}\n\n\tif !auditQuiet {\n\t\tlogger.Info(ctx, \"Audit completed\",\n\t\t\t\"components\", len(reports),\n\t\t\t\"total_violations\", totalViolations)\n\t\t\n\t\tif auditAutoFix && totalAutoFixes > 0 {\n\t\t\tlogger.Info(ctx, \"Applied automatic fixes\", \"total_fixes\", totalAutoFixes)\n\t\t}\n\t}\n\n\t// Output results\n\treturn outputAuditResults(reports, logger)\n}\n\nfunc applyReportFilters(report *accessibility.AccessibilityReport) *accessibility.AccessibilityReport {\n\tfilteredViolations := []accessibility.AccessibilityViolation{}\n\n\tfor _, violation := range report.Violations {\n\t\t// Apply severity filter\n\t\tif auditSeverityFilter != \"\" {\n\t\t\texpectedSeverity := parseSeverity(auditSeverityFilter)\n\t\t\tif violation.Severity != expectedSeverity {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t}\n\n\t\t// Apply fixable filter\n\t\tif auditFixableOnly && !violation.CanAutoFix {\n\t\t\tcontinue\n\t\t}\n\n\t\tfilteredViolations = append(filteredViolations, violation)\n\t}\n\n\t// Apply max violations limit\n\tif auditMaxViolations > 0 && len(filteredViolations) > auditMaxViolations {\n\t\tfilteredViolations = filteredViolations[:auditMaxViolations]\n\t}\n\n\t// Update report\n\treport.Violations = filteredViolations\n\t\n\t// Recalculate summary\n\treport.Summary = calculateAccessibilitySummary(filteredViolations, report.Passed)\n\n\treturn report\n}\n\nfunc applyAutoFixes(ctx context.Context, tester accessibility.AccessibilityTester, report *accessibility.AccessibilityReport) (int, error) {\n\tif componentTester, ok := tester.(*accessibility.ComponentAccessibilityTester); ok {\n\t\tautoFixableViolations := []accessibility.AccessibilityViolation{}\n\t\tfor _, violation := range report.Violations {\n\t\t\tif violation.CanAutoFix {\n\t\t\t\tautoFixableViolations = append(autoFixableViolations, violation)\n\t\t\t}\n\t\t}\n\n\t\tif len(autoFixableViolations) == 0 {\n\t\t\treturn 0, nil\n\t\t}\n\n\t\t// Apply auto-fixes (this would need integration with file system)\n\t\t_, err := componentTester.AutoFix(ctx, report.HTMLSnapshot, autoFixableViolations)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\n\t\treturn len(autoFixableViolations), nil\n\t}\n\n\treturn 0, fmt.Errorf(\"auto-fix not supported for this tester type\")\n}\n\nfunc outputAuditResults(reports []*accessibility.AccessibilityReport, logger logging.Logger) error {\n\tswitch auditOutputFormat {\n\tcase \"json\":\n\t\treturn outputJSON(reports)\n\tcase \"html\":\n\t\treturn outputHTML(reports)\n\tcase \"markdown\":\n\t\treturn outputMarkdown(reports)\n\tcase \"console\":\n\t\tfallthrough\n\tdefault:\n\t\treturn outputConsole(reports, logger)\n\t}\n}\n\nfunc outputJSON(reports []*accessibility.AccessibilityReport) error {\n\toutput := reports\n\tif len(reports) == 1 {\n\t\toutput = reports[0] // Single component audit returns single report\n\t}\n\n\tjsonData, err := json.MarshalIndent(output, \"\", \"  \")\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to marshal JSON: %w\", err)\n\t}\n\n\treturn writeOutput(string(jsonData))\n}\n\nfunc outputHTML(reports []*accessibility.AccessibilityReport) error {\n\thtml := generateHTMLReport(reports)\n\treturn writeOutput(html)\n}\n\nfunc outputMarkdown(reports []*accessibility.AccessibilityReport) error {\n\tmarkdown := generateMarkdownReport(reports)\n\treturn writeOutput(markdown)\n}\n\nfunc outputConsole(reports []*accessibility.AccessibilityReport, logger logging.Logger) error {\n\tctx := context.Background()\n\t\n\tif len(reports) == 0 {\n\t\tfmt.Println(\"No components audited.\")\n\t\treturn nil\n\t}\n\n\t// Summary statistics\n\ttotalComponents := len(reports)\n\ttotalViolations := 0\n\tcriticalViolations := 0\n\tcomponentsWithIssues := 0\n\toverallScoreSum := 0.0\n\n\tfor _, report := range reports {\n\t\ttotalViolations += len(report.Violations)\n\t\toverallScoreSum += report.Summary.OverallScore\n\t\t\n\t\tif len(report.Violations) > 0 {\n\t\t\tcomponentsWithIssues++\n\t\t}\n\t\t\n\t\tfor _, violation := range report.Violations {\n\t\t\tif violation.Impact == accessibility.ImpactCritical {\n\t\t\t\tcriticalViolations++\n\t\t\t}\n\t\t}\n\t}\n\n\taverageScore := overallScoreSum / float64(totalComponents)\n\n\t// Print summary\n\tfmt.Printf(\"\\n\ud83d\udd0d Accessibility Audit Summary\\n\")\n\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\tfmt.Printf(\"Components audited:     %d\\n\", totalComponents)\n\tfmt.Printf(\"Components with issues: %d\\n\", componentsWithIssues)\n\tfmt.Printf(\"Total violations:       %d\\n\", totalViolations)\n\tfmt.Printf(\"Critical violations:    %d\\n\", criticalViolations)\n\tfmt.Printf(\"Average score:          %.1f/100\\n\", averageScore)\n\t\n\t// Overall status\n\tvar status string\n\tvar statusIcon string\n\tif criticalViolations > 0 {\n\t\tstatus = \"CRITICAL ISSUES FOUND\"\n\t\tstatusIcon = \"\ud83d\udea8\"\n\t} else if totalViolations > 0 {\n\t\tstatus = \"ISSUES FOUND\"\n\t\tstatusIcon = \"\u26a0\ufe0f\"\n\t} else {\n\t\tstatus = \"ALL CHECKS PASSED\"\n\t\tstatusIcon = \"\u2705\"\n\t}\n\t\n\tfmt.Printf(\"Status:                 %s %s\\n\\n\", statusIcon, status)\n\n\t// Detailed component results\n\tif auditVerbose || len(reports) == 1 {\n\t\tfor _, report := range reports {\n\t\t\toutputComponentDetails(report)\n\t\t}\n\t} else if totalViolations > 0 {\n\t\t// Show only components with issues\n\t\tfmt.Printf(\"Components with accessibility issues:\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tfor _, report := range reports {\n\t\t\tif len(report.Violations) > 0 {\n\t\t\t\toutputComponentSummary(report)\n\t\t\t}\n\t\t}\n\t}\n\n\t// Show suggestions if enabled\n\tif auditShowSuggestions && totalViolations > 0 {\n\t\tfmt.Printf(\"\\n\ud83d\udca1 Top Suggestions\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tsuggestions := aggregateSuggestions(reports)\n\t\tfor i, suggestion := range suggestions {\n\t\t\tif i >= 5 { // Limit to top 5\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfmt.Printf(\"%d. %s\\n\", i+1, suggestion.Title)\n\t\t\tif suggestion.Description != \"\" {\n\t\t\t\tfmt.Printf(\"   %s\\n\", suggestion.Description)\n\t\t\t}\n\t\t\tfmt.Printf(\"\\n\")\n\t\t}\n\t}\n\t\n\t// Show detailed guidance if enabled and there are violations\n\tif totalViolations > 0 {\n\t\tallViolations := []accessibility.AccessibilityViolation{}\n\t\tfor _, report := range reports {\n\t\t\tallViolations = append(allViolations, report.Violations...)\n\t\t}\n\t\tshowGuidanceForViolations(allViolations)\n\t}\n\n\treturn nil\n}\n\nfunc outputComponentDetails(report *accessibility.AccessibilityReport) {\n\tcomponentName := report.ComponentName\n\tif componentName == \"\" {\n\t\tcomponentName = \"Unknown Component\"\n\t}\n\n\tscoreColor := getScoreColor(report.Summary.OverallScore)\n\t\n\tfmt.Printf(\"\ud83d\udce6 %s (Score: %s%.1f/100%s)\\n\", \n\t\tcomponentName, scoreColor, report.Summary.OverallScore, \"\\033[0m\")\n\tfmt.Printf(\"   File: %s\\n\", report.ComponentFile)\n\t\n\tif len(report.Violations) == 0 {\n\t\tfmt.Printf(\"   \u2705 No accessibility issues found\\n\\n\")\n\t\treturn\n\t}\n\n\t// Group violations by severity\n\terrorViolations := []accessibility.AccessibilityViolation{}\n\twarningViolations := []accessibility.AccessibilityViolation{}\n\tinfoViolations := []accessibility.AccessibilityViolation{}\n\n\tfor _, violation := range report.Violations {\n\t\tswitch violation.Severity {\n\t\tcase accessibility.SeverityError:\n\t\t\terrorViolations = append(errorViolations, violation)\n\t\tcase accessibility.SeverityWarning:\n\t\t\twarningViolations = append(warningViolations, violation)\n\t\tcase accessibility.SeverityInfo:\n\t\t\tinfoViolations = append(infoViolations, violation)\n\t\t}\n\t}\n\n\t// Output violations by severity\n\tif len(errorViolations) > 0 {\n\t\tfmt.Printf(\"   \ud83d\udea8 Errors (%d):\\n\", len(errorViolations))\n\t\tfor _, violation := range errorViolations {\n\t\t\toutputViolation(violation, \"     \")\n\t\t}\n\t}\n\t\n\tif len(warningViolations) > 0 {\n\t\tfmt.Printf(\"   \u26a0\ufe0f  Warnings (%d):\\n\", len(warningViolations))\n\t\tfor _, violation := range warningViolations {\n\t\t\toutputViolation(violation, \"     \")\n\t\t}\n\t}\n\t\n\tif len(infoViolations) > 0 && auditVerbose {\n\t\tfmt.Printf(\"   \u2139\ufe0f  Info (%d):\\n\", len(infoViolations))\n\t\tfor _, violation := range infoViolations {\n\t\t\toutputViolation(violation, \"     \")\n\t\t}\n\t}\n\n\tfmt.Printf(\"\\n\")\n}\n\nfunc outputComponentSummary(report *accessibility.AccessibilityReport) {\n\tcomponentName := report.ComponentName\n\tif componentName == \"\" {\n\t\tcomponentName = \"Unknown Component\"\n\t}\n\n\terrorCount := 0\n\twarningCount := 0\n\tcriticalCount := 0\n\n\tfor _, violation := range report.Violations {\n\t\tswitch violation.Severity {\n\t\tcase accessibility.SeverityError:\n\t\t\terrorCount++\n\t\tcase accessibility.SeverityWarning:\n\t\t\twarningCount++\n\t\t}\n\t\t\n\t\tif violation.Impact == accessibility.ImpactCritical {\n\t\t\tcriticalCount++\n\t\t}\n\t}\n\n\tscoreColor := getScoreColor(report.Summary.OverallScore)\n\t\n\tfmt.Printf(\"\ud83d\udce6 %s %s(%.1f/100)%s\\n\", componentName, scoreColor, report.Summary.OverallScore, \"\\033[0m\")\n\t\n\tif criticalCount > 0 {\n\t\tfmt.Printf(\"   \ud83d\udea8 %d critical issue(s)\\n\", criticalCount)\n\t}\n\tif errorCount > 0 {\n\t\tfmt.Printf(\"   \u274c %d error(s)\\n\", errorCount)\n\t}\n\tif warningCount > 0 {\n\t\tfmt.Printf(\"   \u26a0\ufe0f  %d warning(s)\\n\", warningCount)\n\t}\n\t\n\tfmt.Printf(\"\\n\")\n}\n\nfunc outputViolation(violation accessibility.AccessibilityViolation, indent string) {\n\tfmt.Printf(\"%s\u2022 %s\\n\", indent, violation.Message)\n\tfmt.Printf(\"%s  Rule: %s | WCAG: %s %s\\n\", \n\t\tindent, violation.Rule, violation.WCAG.Level, violation.WCAG.Criteria)\n\t\n\tif violation.Element != \"\" {\n\t\tfmt.Printf(\"%s  Element: <%s>\\n\", indent, violation.Element)\n\t}\n\t\n\tif auditShowSuggestions && len(violation.Suggestions) > 0 {\n\t\tfmt.Printf(\"%s  \ud83d\udca1 %s\\n\", indent, violation.Suggestions[0].Title)\n\t\tif violation.Suggestions[0].Code != \"\" && auditVerbose {\n\t\t\tfmt.Printf(\"%s     Code: %s\\n\", indent, violation.Suggestions[0].Code)\n\t\t}\n\t}\n\t\n\tif violation.CanAutoFix {\n\t\tfmt.Printf(\"%s  \ud83d\udd27 Auto-fixable\\n\", indent)\n\t}\n\t\n\tfmt.Printf(\"\\n\")\n}\n\nfunc writeOutput(content string) error {\n\tif auditOutputFile != \"\" {\n\t\t// Ensure output directory exists\n\t\tdir := filepath.Dir(auditOutputFile)\n\t\tif err := os.MkdirAll(dir, 0755); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to create output directory: %w\", err)\n\t\t}\n\t\t\n\t\t// Write to file\n\t\tif err := os.WriteFile(auditOutputFile, []byte(content), 0644); err != nil {\n\t\t\treturn fmt.Errorf(\"failed to write output file: %w\", err)\n\t\t}\n\t\t\n\t\tfmt.Printf(\"Report written to: %s\\n\", auditOutputFile)\n\t\treturn nil\n\t}\n\t\n\t// Write to stdout\n\tfmt.Print(content)\n\treturn nil\n}\n\n// Helper functions\nfunc parseWCAGLevel(level string) accessibility.WCAGLevel {\n\tswitch strings.ToUpper(level) {\n\tcase \"A\":\n\t\treturn accessibility.WCAGLevelA\n\tcase \"AA\":\n\t\treturn accessibility.WCAGLevelAA\n\tcase \"AAA\":\n\t\treturn accessibility.WCAGLevelAAA\n\tdefault:\n\t\treturn accessibility.WCAGLevelAA\n\t}\n}\n\nfunc parseSeverity(severity string) accessibility.ViolationSeverity {\n\tswitch strings.ToLower(severity) {\n\tcase \"error\":\n\t\treturn accessibility.SeverityError\n\tcase \"warning\":\n\t\treturn accessibility.SeverityWarning\n\tcase \"info\":\n\t\treturn accessibility.SeverityInfo\n\tdefault:\n\t\treturn accessibility.SeverityWarning\n\t}\n}\n\nfunc getScoreColor(score float64) string {\n\tif score >= 90 {\n\t\treturn \"\\033[32m\" // Green\n\t} else if score >= 70 {\n\t\treturn \"\\033[33m\" // Yellow\n\t} else {\n\t\treturn \"\\033[31m\" // Red\n\t}\n}\n\nfunc aggregateSuggestions(reports []*accessibility.AccessibilityReport) []accessibility.AccessibilitySuggestion {\n\tsuggestionMap := make(map[string]*accessibility.AccessibilitySuggestion)\n\tsuggestionCounts := make(map[string]int)\n\t\n\tfor _, report := range reports {\n\t\tfor _, violation := range report.Violations {\n\t\t\tfor _, suggestion := range violation.Suggestions {\n\t\t\t\tkey := fmt.Sprintf(\"%s_%s\", suggestion.Type, suggestion.Title)\n\t\t\t\tsuggestionCounts[key]++\n\t\t\t\t\n\t\t\t\tif existing, exists := suggestionMap[key]; !exists || suggestion.Priority < existing.Priority {\n\t\t\t\t\tsuggestionCopy := suggestion\n\t\t\t\t\tsuggestionMap[key] = &suggestionCopy\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\t\n\t// Convert to slice and sort by frequency and priority\n\tsuggestions := []accessibility.AccessibilitySuggestion{}\n\tfor key, suggestion := range suggestionMap {\n\t\t// Adjust priority based on frequency (more frequent = higher priority)\n\t\tsuggestion.Priority -= suggestionCounts[key] // Lower number = higher priority\n\t\tsuggestions = append(suggestions, *suggestion)\n\t}\n\t\n\t// Sort by priority\n\tsort.Slice(suggestions, func(i, j int) bool {\n\t\treturn suggestions[i].Priority < suggestions[j].Priority\n\t})\n\t\n\treturn suggestions\n}\n\nfunc calculateAccessibilitySummary(violations []accessibility.AccessibilityViolation, passed []accessibility.AccessibilityRule) accessibility.AccessibilitySummary {\n\tsummary := accessibility.AccessibilitySummary{\n\t\tTotalRules:      len(passed) + len(violations),\n\t\tPassedRules:     len(passed),\n\t\tFailedRules:     len(violations),\n\t\tTotalViolations: len(violations),\n\t}\n\t\n\tfor _, violation := range violations {\n\t\tswitch violation.Severity {\n\t\tcase accessibility.SeverityError:\n\t\t\tsummary.ErrorViolations++\n\t\tcase accessibility.SeverityWarning:\n\t\t\tsummary.WarnViolations++\n\t\tcase accessibility.SeverityInfo:\n\t\t\tsummary.InfoViolations++\n\t\t}\n\t\t\n\t\tswitch violation.Impact {\n\t\tcase accessibility.ImpactCritical:\n\t\t\tsummary.CriticalImpact++\n\t\tcase accessibility.ImpactSerious:\n\t\t\tsummary.SeriousImpact++\n\t\tcase accessibility.ImpactModerate:\n\t\t\tsummary.ModerateImpact++\n\t\tcase accessibility.ImpactMinor:\n\t\t\tsummary.MinorImpact++\n\t\t}\n\t}\n\t\n\t// Calculate overall score\n\tif summary.TotalRules > 0 {\n\t\tsummary.OverallScore = float64(summary.PassedRules) / float64(summary.TotalRules) * 100\n\t}\n\t\n\treturn summary\n}\n\nfunc generateHTMLReport(reports []*accessibility.AccessibilityReport) string {\n\t// This would generate a comprehensive HTML report\n\t// For brevity, returning a simplified version\n\thtml := `<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>Accessibility Audit Report</title>\n    <style>\n        body { font-family: Arial, sans-serif; margin: 40px; }\n        .header { background: #f5f5f5; padding: 20px; border-radius: 8px; }\n        .component { margin: 20px 0; padding: 20px; border: 1px solid #ddd; border-radius: 8px; }\n        .violation { margin: 10px 0; padding: 10px; background: #fff3cd; border-left: 4px solid #ffc107; }\n        .error { background: #f8d7da; border-left-color: #dc3545; }\n        .success { background: #d4edda; border-left-color: #28a745; }\n    </style>\n</head>\n<body>\n    <div class=\"header\">\n        <h1>Accessibility Audit Report</h1>\n        <p>Generated on: ` + time.Now().Format(\"2006-01-02 15:04:05\") + `</p>\n    </div>`\n\n\tfor _, report := range reports {\n\t\thtml += fmt.Sprintf(`\n    <div class=\"component\">\n        <h2>%s</h2>\n        <p>Score: %.1f/100</p>\n        <p>Violations: %d</p>\n    </div>`, report.ComponentName, report.Summary.OverallScore, len(report.Violations))\n\t}\n\n\thtml += `\n</body>\n</html>`\n\n\treturn html\n}\n\nfunc generateMarkdownReport(reports []*accessibility.AccessibilityReport) string {\n\tmd := fmt.Sprintf(\"# Accessibility Audit Report\\n\\nGenerated on: %s\\n\\n\", time.Now().Format(\"2006-01-02 15:04:05\"))\n\t\n\tfor _, report := range reports {\n\t\tmd += fmt.Sprintf(\"## %s\\n\\n\", report.ComponentName)\n\t\tmd += fmt.Sprintf(\"- **Score**: %.1f/100\\n\", report.Summary.OverallScore)\n\t\tmd += fmt.Sprintf(\"- **Violations**: %d\\n\\n\", len(report.Violations))\n\t\t\n\t\tif len(report.Violations) > 0 {\n\t\t\tmd += \"### Issues Found\\n\\n\"\n\t\t\tfor _, violation := range report.Violations {\n\t\t\t\tmd += fmt.Sprintf(\"- **%s**: %s\\n\", violation.Rule, violation.Message)\n\t\t\t}\n\t\t\tmd += \"\\n\"\n\t\t}\n\t}\n\t\n\treturn md\n}\n\nfunc getComponentCompletions(toComplete string) []string {\n\t// This would integrate with the component registry to provide completions\n\t// For now, returning empty slice\n\treturn []string{}\n}\n\n// showGuidanceOnly displays accessibility guidance without running an audit\nfunc showGuidanceOnly(componentName string) error {\n\tguide := accessibility.NewAccessibilityGuide()\n\t\n\tif componentName != \"\" {\n\t\t// Show component-specific guidance\n\t\tfmt.Printf(\"\ud83c\udfaf Accessibility Guidance for %s Component\\n\", componentName)\n\t\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\t\t\n\t\tguidanceText := guide.GetComponentGuidanceText(componentName)\n\t\tfmt.Print(guidanceText)\n\t\t\n\t\t// Also show general guidance applicable to all components\n\t\tfmt.Printf(\"\\n\ud83d\udccb General Accessibility Guidelines\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tquickStart := guide.GetQuickStartGuide()\n\t\tfor i, item := range quickStart {\n\t\t\tif i >= 3 { // Limit to top 3 for brevity\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfmt.Printf(\"%d. %s\\n\", i+1, item.Title)\n\t\t\tfmt.Printf(\"   %s\\n\\n\", item.Description)\n\t\t}\n\t\t\n\t} else {\n\t\t// Show general accessibility guidance\n\t\tfmt.Printf(\"\ud83c\udf1f Accessibility Quick Start Guide\\n\")\n\t\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\t\t\n\t\tquickStart := guide.GetQuickStartGuide()\n\t\tfor i, item := range quickStart {\n\t\t\tfmt.Printf(\"%d. %s\\n\", i+1, item.Title)\n\t\t\tfmt.Printf(\"   %s\\n\", item.Description)\n\t\t\t\n\t\t\tif len(item.Examples) > 0 {\n\t\t\t\texample := item.Examples[0]\n\t\t\t\tif example.BadCode != \"\" {\n\t\t\t\t\tfmt.Printf(\"   \u274c Avoid: %s\\n\", strings.ReplaceAll(example.BadCode, \"\\n\", \" \"))\n\t\t\t\t}\n\t\t\t\tif example.GoodCode != \"\" {\n\t\t\t\t\tfmt.Printf(\"   \u2705 Use: %s\\n\", strings.ReplaceAll(example.GoodCode, \"\\n\", \" \"))\n\t\t\t\t}\n\t\t\t}\n\t\t\tfmt.Printf(\"\\n\")\n\t\t}\n\t\t\n\t\tfmt.Printf(\"\ud83d\udca1 Advanced Guidelines\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\\n\")\n\t\t\n\t\tbestPractices := guide.GetBestPracticesGuide()\n\t\tfor i, item := range bestPractices {\n\t\t\tif i >= 3 { // Limit for readability\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfmt.Printf(\"\u2022 %s\\n\", item.Title)\n\t\t\tfmt.Printf(\"  %s\\n\\n\", item.Description)\n\t\t}\n\t\t\n\t\tfmt.Printf(\"\ud83d\udcda Additional Resources\\n\")\n\t\tfmt.Printf(\"\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\\n\")\n\t\tfmt.Printf(\"\u2022 WCAG Quick Reference: https://www.w3.org/WAI/WCAG21/quickref/\\n\")\n\t\tfmt.Printf(\"\u2022 WebAIM Guidelines: https://webaim.org/\\n\")\n\t\tfmt.Printf(\"\u2022 A11y Project: https://www.a11yproject.com/\\n\")\n\t\tfmt.Printf(\"\u2022 MDN Accessibility: https://developer.mozilla.org/en-US/docs/Web/Accessibility\\n\\n\")\n\t\t\n\t\tfmt.Printf(\"\ud83d\udd27 To audit your components, run:\\n\")\n\t\tfmt.Printf(\"   templar audit              # Audit all components\\n\")\n\t\tfmt.Printf(\"   templar audit Button       # Audit specific component\\n\")\n\t\tfmt.Printf(\"   templar audit --help        # See all options\\n\")\n\t}\n\t\n\treturn nil\n}\n\n// showGuidanceForViolations displays guidance for specific accessibility violations\nfunc showGuidanceForViolations(violations []accessibility.AccessibilityViolation) {\n\tif !auditShowGuidance || len(violations) == 0 {\n\t\treturn\n\t}\n\t\n\tfmt.Printf(\"\\n\ud83c\udf93 Accessibility Guidance\\n\")\n\tfmt.Printf(\"\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\\n\\n\")\n\t\n\tguide := accessibility.NewAccessibilityGuide()\n\t\n\t// Group violations by rule to avoid duplicate guidance\n\truleMap := make(map[string]bool)\n\tuniqueRules := []string{}\n\t\n\tfor _, violation := range violations {\n\t\tif !ruleMap[violation.Rule] {\n\t\t\truleMap[violation.Rule] = true\n\t\t\tuniqueRules = append(uniqueRules, violation.Rule)\n\t\t}\n\t}\n\t\n\t// Show guidance for each unique rule\n\tfor i, rule := range uniqueRules {\n\t\tif i > 0 {\n\t\t\tfmt.Printf(\"\\n\" + strings.Repeat(\"\u2500\", 60) + \"\\n\\n\")\n\t\t}\n\t\t\n\t\tguidanceText := guide.GetGuidanceText(rule)\n\t\tfmt.Print(guidanceText)\n\t}\n}",
      "structuredPatch": [
        {
          "oldStart": 147,
          "oldLines": 7,
          "newStart": 147,
          "newLines": 7,
          "lines": [
            "   }",
            " ",
            "   // Create renderer",
            "-  componentRenderer := renderer.NewComponentRenderer(logger)",
            "+  componentRenderer := renderer.NewComponentRenderer()",
            " ",
            "   // Initialize accessibility tester",
            "   testerConfig := accessibility.TesterConfig{"
          ]
        }
      ],
      "userModified": false
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Read",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/cmd/doctor.go",
      "offset": 794,
      "limit": 20
    },
    "tool_response": {
      "type": "text",
      "file": {
        "filePath": "/home/connerohnesorge/Documents/001Repos/templar/cmd/doctor.go",
        "content": "\nfunc calculateSummary(results []DiagnosticResult) ReportSummary {\n\tsummary := ReportSummary{\n\t\tTotal: len(results),\n\t}\n\n\tfor _, result := range results {\n\t\tswitch result.Status {\n\t\tcase \"ok\":\n\t\t\tsummary.OK++\n\t\tcase \"warning\":\n\t\t\tsummary.Warnings++\n\t\tcase \"error\":\n\t\t\tsummary.Errors++\n\t\tcase \"info\":\n\t\t\tsummary.Info++\n\t\t}\n\t}\n\n\treturn summary",
        "numLines": 20,
        "startLine": 794,
        "totalLines": 900
      }
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Edit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/scanner.go",
      "old_string": "// generateOptimizedHash creates an optimized hash based on file size and content characteristics\nfunc (s *ComponentScanner) generateOptimizedHash(content []byte, fileInfo os.FileInfo) (string, *HashingStrategy) {\n\tstart := time.Now()\n\tfileSize := int64(len(content))\n\t\n\tstrategy := &HashingStrategy{\n\t\tFileSize: fileSize,\n\t}\n\t\n\tvar hash uint32\n\t\n\tswitch {\n\tcase fileSize <= smallFileThreshold:\n\t\t// Small files: use full content CRC32 (fast anyway)\n\t\thash = crc32.Checksum(content, crcTable)\n\t\tstrategy.Strategy = HashStrategyFull\n\t\tstrategy.SamplePoints = 1\n\t\t\n\tcase fileSize <= mediumFileThreshold:\n\t\t// Medium files: use content sampling\n\t\thash = s.generateSampledHash(content)\n\t\tstrategy.Strategy = HashStrategySampled\n\t\tstrategy.SamplePoints = 3\n\t\t\n\tdefault:\n\t\t// Large files: use hierarchical sampling with metadata\n\t\thash = s.generateHierarchicalHash(content, fileInfo)\n\t\tstrategy.Strategy = HashStrategyHierarchical\n\t\tstrategy.SamplePoints = 5\n\t}\n\t\n\tstrategy.HashTime = time.Since(start)\n\t\n\t// Include file metadata in hash to catch size/timestamp changes\n\tmetadataHash := s.generateMetadataHash(fileInfo)\n\tcombinedHash := hash ^ metadataHash\n\t\n\treturn strconv.FormatUint(uint64(combinedHash), 16), strategy\n}",
      "new_string": "// generateOptimizedHash creates an optimized hash based on file size and content characteristics\nfunc (s *ComponentScanner) generateOptimizedHash(content []byte, fileInfo os.FileInfo) (string, *HashingStrategy) {\n\tstart := time.Now()\n\tfileSize := int64(len(content))\n\t\n\tstrategy := &HashingStrategy{\n\t\tFileSize: fileSize,\n\t}\n\t\n\tvar primaryHash, secondaryHash uint32\n\t\n\tswitch {\n\tcase fileSize <= smallFileThreshold:\n\t\t// Small files: use full content CRC32 (fast anyway)\n\t\tprimaryHash = crc32.Checksum(content, crcTable)\n\t\tstrategy.Strategy = HashStrategyFull\n\t\tstrategy.SamplePoints = 1\n\t\t\n\tcase fileSize <= mediumFileThreshold:\n\t\t// Medium files: use content sampling with fallback\n\t\tprimaryHash = s.generateSampledHash(content)\n\t\t// Generate secondary hash for collision detection\n\t\tsecondaryHash = s.generateAlternativeHash(content)\n\t\tstrategy.Strategy = HashStrategySampled\n\t\tstrategy.SamplePoints = 3\n\t\t\n\tdefault:\n\t\t// Large files: use hierarchical sampling with metadata\n\t\tprimaryHash = s.generateHierarchicalHash(content, fileInfo)\n\t\t// Generate secondary hash for collision detection\n\t\tsecondaryHash = s.generateAlternativeHash(content)\n\t\tstrategy.Strategy = HashStrategyHierarchical\n\t\tstrategy.SamplePoints = 5\n\t}\n\t\n\tstrategy.HashTime = time.Since(start)\n\t\n\t// Include file metadata in hash to catch size/timestamp changes\n\tmetadataHash := s.generateMetadataHash(fileInfo)\n\t\n\t// Combine primary hash with metadata\n\tcombinedHash := primaryHash ^ metadataHash\n\t\n\t// For collision resistance, incorporate secondary hash if available\n\tif secondaryHash != 0 {\n\t\tcombinedHash = combinedHash ^ (secondaryHash >> 16)\n\t}\n\t\n\treturn strconv.FormatUint(uint64(combinedHash), 16), strategy\n}\n\n// generateAlternativeHash creates an alternative hash for collision detection\nfunc (s *ComponentScanner) generateAlternativeHash(content []byte) uint32 {\n\tif len(content) == 0 {\n\t\treturn 0\n\t}\n\t\n\t// Use IEEE CRC32 polynomial (different from Castagnoli) for secondary hash\n\treturn crc32.ChecksumIEEE(content[:min(len(content), 4096)])\n}\n\n// min returns the minimum of two integers\nfunc min(a, b int) int {\n\tif a < b {\n\t\treturn a\n\t}\n\treturn b\n}"
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/scanner.go",
      "oldString": "// generateOptimizedHash creates an optimized hash based on file size and content characteristics\nfunc (s *ComponentScanner) generateOptimizedHash(content []byte, fileInfo os.FileInfo) (string, *HashingStrategy) {\n\tstart := time.Now()\n\tfileSize := int64(len(content))\n\t\n\tstrategy := &HashingStrategy{\n\t\tFileSize: fileSize,\n\t}\n\t\n\tvar hash uint32\n\t\n\tswitch {\n\tcase fileSize <= smallFileThreshold:\n\t\t// Small files: use full content CRC32 (fast anyway)\n\t\thash = crc32.Checksum(content, crcTable)\n\t\tstrategy.Strategy = HashStrategyFull\n\t\tstrategy.SamplePoints = 1\n\t\t\n\tcase fileSize <= mediumFileThreshold:\n\t\t// Medium files: use content sampling\n\t\thash = s.generateSampledHash(content)\n\t\tstrategy.Strategy = HashStrategySampled\n\t\tstrategy.SamplePoints = 3\n\t\t\n\tdefault:\n\t\t// Large files: use hierarchical sampling with metadata\n\t\thash = s.generateHierarchicalHash(content, fileInfo)\n\t\tstrategy.Strategy = HashStrategyHierarchical\n\t\tstrategy.SamplePoints = 5\n\t}\n\t\n\tstrategy.HashTime = time.Since(start)\n\t\n\t// Include file metadata in hash to catch size/timestamp changes\n\tmetadataHash := s.generateMetadataHash(fileInfo)\n\tcombinedHash := hash ^ metadataHash\n\t\n\treturn strconv.FormatUint(uint64(combinedHash), 16), strategy\n}",
      "newString": "// generateOptimizedHash creates an optimized hash based on file size and content characteristics\nfunc (s *ComponentScanner) generateOptimizedHash(content []byte, fileInfo os.FileInfo) (string, *HashingStrategy) {\n\tstart := time.Now()\n\tfileSize := int64(len(content))\n\t\n\tstrategy := &HashingStrategy{\n\t\tFileSize: fileSize,\n\t}\n\t\n\tvar primaryHash, secondaryHash uint32\n\t\n\tswitch {\n\tcase fileSize <= smallFileThreshold:\n\t\t// Small files: use full content CRC32 (fast anyway)\n\t\tprimaryHash = crc32.Checksum(content, crcTable)\n\t\tstrategy.Strategy = HashStrategyFull\n\t\tstrategy.SamplePoints = 1\n\t\t\n\tcase fileSize <= mediumFileThreshold:\n\t\t// Medium files: use content sampling with fallback\n\t\tprimaryHash = s.generateSampledHash(content)\n\t\t// Generate secondary hash for collision detection\n\t\tsecondaryHash = s.generateAlternativeHash(content)\n\t\tstrategy.Strategy = HashStrategySampled\n\t\tstrategy.SamplePoints = 3\n\t\t\n\tdefault:\n\t\t// Large files: use hierarchical sampling with metadata\n\t\tprimaryHash = s.generateHierarchicalHash(content, fileInfo)\n\t\t// Generate secondary hash for collision detection\n\t\tsecondaryHash = s.generateAlternativeHash(content)\n\t\tstrategy.Strategy = HashStrategyHierarchical\n\t\tstrategy.SamplePoints = 5\n\t}\n\t\n\tstrategy.HashTime = time.Since(start)\n\t\n\t// Include file metadata in hash to catch size/timestamp changes\n\tmetadataHash := s.generateMetadataHash(fileInfo)\n\t\n\t// Combine primary hash with metadata\n\tcombinedHash := primaryHash ^ metadataHash\n\t\n\t// For collision resistance, incorporate secondary hash if available\n\tif secondaryHash != 0 {\n\t\tcombinedHash = combinedHash ^ (secondaryHash >> 16)\n\t}\n\t\n\treturn strconv.FormatUint(uint64(combinedHash), 16), strategy\n}\n\n// generateAlternativeHash creates an alternative hash for collision detection\nfunc (s *ComponentScanner) generateAlternativeHash(content []byte) uint32 {\n\tif len(content) == 0 {\n\t\treturn 0\n\t}\n\t\n\t// Use IEEE CRC32 polynomial (different from Castagnoli) for secondary hash\n\treturn crc32.ChecksumIEEE(content[:min(len(content), 4096)])\n}\n\n// min returns the minimum of two integers\nfunc min(a, b int) int {\n\tif a < b {\n\t\treturn a\n\t}\n\treturn b\n}",
      "originalFile": "// Package scanner provides component discovery and analysis for templ templates.\n//\n// The scanner traverses file systems to find .templ files, parses them using\n// Go's AST parser to extract component metadata including parameters, dependencies,\n// and documentation. It integrates with the component registry to broadcast\n// change events and supports recursive directory scanning with exclude patterns.\n// The scanner maintains file hashes for change detection and provides both\n// single-file and batch scanning capabilities.\npackage scanner\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"go/ast\"\n\t\"go/parser\"\n\t\"go/token\"\n\t\"hash/crc32\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/conneroisu/templar/internal/errors\"\n\t\"github.com/conneroisu/templar/internal/interfaces\"\n\t\"github.com/conneroisu/templar/internal/registry\"\n\t\"github.com/conneroisu/templar/internal/types\"\n)\n\n// crcTable is a pre-computed CRC32 Castagnoli table for faster hash generation\nvar crcTable = crc32.MakeTable(crc32.Castagnoli)\n\n// Hash generation strategy constants\nconst (\n\t// Small files (< 4KB) - use full content CRC32\n\tsmallFileThreshold = 4 * 1024\n\t// Medium files (4KB - 256KB) - use content sampling\n\tmediumFileThreshold = 256 * 1024\n\t// Large files (> 256KB) - use hierarchical sampling\n\tlargeFileThreshold = 256 * 1024\n\t// Content sample size for large files\n\tcontentSampleSize = 1024\n)\n\n// FileHashStrategy represents different hashing approaches\ntype FileHashStrategy int\n\nconst (\n\tHashStrategyFull FileHashStrategy = iota\n\tHashStrategySampled\n\tHashStrategyHierarchical\n)\n\n// ScanJob represents a scanning job for the worker pool containing the file\n// path to scan and a result channel for asynchronous communication.\ntype ScanJob struct {\n\t// filePath is the absolute path to the .templ file to be scanned\n\tfilePath string\n\t// result channel receives the scan result or error asynchronously\n\tresult chan<- ScanResult\n}\n\n// HashResult represents the result of asynchronous hash calculation\ntype HashResult struct {\n\thash string\n\terr  error\n}\n\n// BufferPool manages reusable byte buffers for file reading optimization\ntype BufferPool struct {\n\tpool sync.Pool\n}\n\n// NewBufferPool creates a new buffer pool with initial buffer size\nfunc NewBufferPool() *BufferPool {\n\treturn &BufferPool{\n\t\tpool: sync.Pool{\n\t\t\tNew: func() interface{} {\n\t\t\t\t// Pre-allocate 64KB buffers for typical component files\n\t\t\t\treturn make([]byte, 0, 64*1024)\n\t\t\t},\n\t\t},\n\t}\n}\n\n// Get retrieves a buffer from the pool\nfunc (bp *BufferPool) Get() []byte {\n\treturn bp.pool.Get().([]byte)[:0] // Reset length but keep capacity\n}\n\n// Put returns a buffer to the pool\nfunc (bp *BufferPool) Put(buf []byte) {\n\t// Only pool reasonably-sized buffers to avoid memory leaks\n\tif cap(buf) <= 1024*1024 { // 1MB limit\n\t\tbp.pool.Put(buf)\n\t}\n}\n\n// ScanResult represents the result of a scanning operation, containing either\n// success status or error information for a specific file.\ntype ScanResult struct {\n\t// filePath is the path that was scanned\n\tfilePath string\n\t// err contains any error that occurred during scanning, nil on success\n\terr error\n}\n\n// WorkerPool manages persistent scanning workers for performance optimization\n// using a work-stealing approach to distribute scanning jobs across CPU cores.\ntype WorkerPool struct {\n\t// jobQueue buffers scanning jobs for worker distribution\n\tjobQueue chan ScanJob\n\t// workers holds references to all active worker goroutines\n\tworkers []*ScanWorker\n\t// workerCount defines the number of concurrent workers (typically NumCPU)\n\tworkerCount int\n\t// scanner is the shared component scanner instance\n\tscanner *ComponentScanner\n\t// stop signals all workers to terminate gracefully\n\tstop chan struct{}\n\t// stopped tracks pool shutdown state\n\tstopped bool\n\t// mu protects concurrent access to pool state\n\tmu sync.RWMutex\n}\n\n// ScanWorker represents a persistent worker goroutine that processes scanning\n// jobs from the shared job queue. Each worker operates independently and\n// can handle multiple file types concurrently.\ntype ScanWorker struct {\n\t// id uniquely identifies this worker for debugging and metrics\n\tid int\n\t// jobQueue receives scanning jobs from the worker pool\n\tjobQueue <-chan ScanJob\n\t// scanner provides the component parsing functionality\n\tscanner *ComponentScanner\n\t// stop signals this worker to terminate gracefully\n\tstop chan struct{}\n}\n\n// ComponentScanner discovers and parses templ components using Go's AST parser.\n//\n// The scanner provides:\n// - Recursive directory traversal with exclude patterns\n// - AST-based component metadata extraction\n// - Concurrent processing via worker pool\n// - Integration with component registry for event broadcasting\n// - File change detection using CRC32 hashing\n// - Optimized path validation with cached working directory\n// - Buffer pooling for memory optimization in large codebases\n// - Component metadata caching with LRU eviction for performance\ntype ComponentScanner struct {\n\t// registry receives discovered components and broadcasts change events\n\tregistry *registry.ComponentRegistry\n\t// fileSet tracks file positions for AST parsing and error reporting\n\tfileSet *token.FileSet\n\t// workerPool manages concurrent scanning operations\n\tworkerPool *WorkerPool\n\t// pathCache contains cached path validation data to avoid repeated syscalls\n\tpathCache *pathValidationCache\n\t// bufferPool provides reusable byte buffers for file reading optimization\n\tbufferPool *BufferPool\n\t// metadataCache caches parsed component metadata by file hash to avoid re-parsing unchanged files\n\tmetadataCache *MetadataCache\n\t// astParsingPool provides concurrent AST parsing to avoid blocking worker threads\n\tastParsingPool *ASTParsingPool\n\t// metrics tracks performance metrics during scanning operations\n\tmetrics *ScannerMetrics\n\t// config provides timeout configuration for scanning operations\n\tconfig *config.Config\n}\n\n// Interface compliance verification - ComponentScanner implements interfaces.ComponentScanner\nvar _ interfaces.ComponentScanner = (*ComponentScanner)(nil)\n\n// ASTParseJob represents a parsing job for the AST parsing pool\ntype ASTParseJob struct {\n\tfilePath string\n\tcontent  []byte\n\tfileSet  *token.FileSet\n\tresult   chan<- ASTParseResult\n}\n\n// ASTParseResult contains the result of AST parsing\ntype ASTParseResult struct {\n\tastFile *ast.File\n\terr     error\n\tfilePath string\n}\n\n// ASTParsingPool manages concurrent AST parsing to avoid blocking worker threads\ntype ASTParsingPool struct {\n\tworkers   int\n\tjobChan   chan ASTParseJob\n\tcloseChan chan struct{}\n\twg        sync.WaitGroup\n}\n\n// NewASTParsingPool creates a new AST parsing pool with specified worker count\nfunc NewASTParsingPool(workers int) *ASTParsingPool {\n\tif workers <= 0 {\n\t\tworkers = runtime.NumCPU() / 2 // Use half CPU cores for AST parsing\n\t\tif workers < 1 {\n\t\t\tworkers = 1\n\t\t}\n\t}\n\t\n\tpool := &ASTParsingPool{\n\t\tworkers:   workers,\n\t\tjobChan:   make(chan ASTParseJob, workers*2),\n\t\tcloseChan: make(chan struct{}),\n\t}\n\t\n\t// Start worker goroutines\n\tfor i := 0; i < workers; i++ {\n\t\tpool.wg.Add(1)\n\t\tgo pool.worker()\n\t}\n\t\n\treturn pool\n}\n\n// worker processes AST parsing jobs\nfunc (p *ASTParsingPool) worker() {\n\tdefer p.wg.Done()\n\t\n\tfor {\n\t\tselect {\n\t\tcase job := <-p.jobChan:\n\t\t\t// Parse the AST\n\t\t\tastFile, err := parser.ParseFile(job.fileSet, job.filePath, job.content, parser.ParseComments)\n\t\t\t\n\t\t\t// Send result back\n\t\t\tselect {\n\t\t\tcase job.result <- ASTParseResult{\n\t\t\t\tastFile: astFile,\n\t\t\t\terr:     err,\n\t\t\t\tfilePath: job.filePath,\n\t\t\t}:\n\t\t\tcase <-p.closeChan:\n\t\t\t\treturn\n\t\t\t}\n\t\t\t\n\t\tcase <-p.closeChan:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// ParseAsync submits an AST parsing job and returns a result channel\nfunc (p *ASTParsingPool) ParseAsync(filePath string, content []byte, fileSet *token.FileSet) <-chan ASTParseResult {\n\tresult := make(chan ASTParseResult, 1)\n\t\n\t// For very large files, use optimized parsing approach\n\tif len(content) > 1024*1024 { // 1MB threshold\n\t\tgo p.parseLargeFileAsync(filePath, content, fileSet, result)\n\t\treturn result\n\t}\n\t\n\tselect {\n\tcase p.jobChan <- ASTParseJob{\n\t\tfilePath: filePath,\n\t\tcontent:  content,\n\t\tfileSet:  fileSet,\n\t\tresult:   result,\n\t}:\n\t\treturn result\n\tcase <-p.closeChan:\n\t\t// Pool is closed, return error result\n\t\tgo func() {\n\t\t\tresult <- ASTParseResult{\n\t\t\t\tastFile: nil,\n\t\t\t\terr:     fmt.Errorf(\"AST parsing pool is closed\"),\n\t\t\t\tfilePath: filePath,\n\t\t\t}\n\t\t}()\n\t\treturn result\n\t}\n}\n\n// parseLargeFileAsync handles large file parsing with memory optimization\nfunc (p *ASTParsingPool) parseLargeFileAsync(filePath string, content []byte, fileSet *token.FileSet, result chan<- ASTParseResult) {\n\tdefer close(result)\n\t\n\t// For large files, use streaming approach with limited memory usage\n\t// Parse with limited goroutines to prevent memory exhaustion\n\tastFile, err := parser.ParseFile(fileSet, filePath, content, parser.ParseComments|parser.SkipObjectResolution)\n\t\n\tresult <- ASTParseResult{\n\t\tastFile: astFile,\n\t\terr:     err,\n\t\tfilePath: filePath,\n\t}\n}\n\n// Close shuts down the AST parsing pool\nfunc (p *ASTParsingPool) Close() {\n\tclose(p.closeChan)\n\tclose(p.jobChan)\n\tp.wg.Wait()\n}\n\n// pathValidationCache caches expensive filesystem operations for optimal performance\ntype pathValidationCache struct {\n\t// mu protects concurrent access to cache fields\n\tmu sync.RWMutex\n\t// currentWorkingDir is the cached current working directory (absolute path)\n\tcurrentWorkingDir string\n\t// initialized indicates whether the cache has been populated\n\tinitialized bool\n}\n\n// CachedComponentMetadata stores pre-parsed component information for cache optimization\ntype CachedComponentMetadata struct {\n\t// Components is a slice of all components found in the file\n\tComponents []*types.ComponentInfo\n\t// FileHash is the CRC32 hash of the file content when cached\n\tFileHash string\n\t// ParsedAt records when the metadata was cached\n\tParsedAt time.Time\n}\n\n// ScannerMetrics tracks performance metrics during scanning operations\ntype ScannerMetrics struct {\n\t// FilesProcessed is the total number of files processed\n\tFilesProcessed int64\n\t// ComponentsFound is the total number of components discovered\n\tComponentsFound int64\n\t// CacheHits tracks how many files were served from cache\n\tCacheHits int64\n\t// CacheMisses tracks how many files required parsing\n\tCacheMisses int64\n\t// TotalScanTime tracks time spent in scanning operations\n\tTotalScanTime time.Duration\n\t// PeakMemoryUsage tracks the peak memory usage during scanning\n\tPeakMemoryUsage uint64\n\t// ConcurrentJobs tracks the peak number of concurrent jobs\n\tConcurrentJobs int64\n}\n\n// MetadataCache implements a simple LRU cache for component metadata\ntype MetadataCache struct {\n\tmu       sync.RWMutex\n\tentries  map[string]*MetadataCacheEntry\n\tmaxSize  int\n\tttl      time.Duration\n\t// LRU doubly-linked list\n\thead *MetadataCacheEntry\n\ttail *MetadataCacheEntry\n}\n\n// MetadataCacheEntry represents a cached metadata entry with LRU pointers\ntype MetadataCacheEntry struct {\n\tKey       string\n\tData      []byte\n\tCreatedAt time.Time\n\t// LRU pointers\n\tprev *MetadataCacheEntry\n\tnext *MetadataCacheEntry\n}\n\n// NewMetadataCache creates a new metadata cache\nfunc NewMetadataCache(maxSize int, ttl time.Duration) *MetadataCache {\n\tcache := &MetadataCache{\n\t\tentries: make(map[string]*MetadataCacheEntry),\n\t\tmaxSize: maxSize,\n\t\tttl:     ttl,\n\t}\n\t\n\t// Initialize dummy head and tail for LRU\n\tcache.head = &MetadataCacheEntry{}\n\tcache.tail = &MetadataCacheEntry{}\n\tcache.head.next = cache.tail\n\tcache.tail.prev = cache.head\n\t\n\treturn cache\n}\n\n// Get retrieves data from cache\nfunc (mc *MetadataCache) Get(key string) ([]byte, bool) {\n\tmc.mu.Lock()\n\tdefer mc.mu.Unlock()\n\t\n\tentry, exists := mc.entries[key]\n\tif !exists {\n\t\treturn nil, false\n\t}\n\t\n\t// Check TTL\n\tif time.Since(entry.CreatedAt) > mc.ttl {\n\t\tmc.removeFromList(entry)\n\t\tdelete(mc.entries, key)\n\t\treturn nil, false\n\t}\n\t\n\t// Move to front (most recently used)\n\tmc.moveToFront(entry)\n\treturn entry.Data, true\n}\n\n// Set stores data in cache\nfunc (mc *MetadataCache) Set(key string, data []byte) {\n\tmc.mu.Lock()\n\tdefer mc.mu.Unlock()\n\t\n\t// Check if entry exists\n\tif existingEntry, exists := mc.entries[key]; exists {\n\t\texistingEntry.Data = data\n\t\texistingEntry.CreatedAt = time.Now()\n\t\tmc.moveToFront(existingEntry)\n\t\treturn\n\t}\n\t\n\t// Evict if needed\n\tif len(mc.entries) >= mc.maxSize {\n\t\tmc.evictLRU()\n\t}\n\t\n\t// Create new entry\n\tentry := &MetadataCacheEntry{\n\t\tKey:       key,\n\t\tData:      data,\n\t\tCreatedAt: time.Now(),\n\t}\n\t\n\tmc.entries[key] = entry\n\tmc.addToFront(entry)\n}\n\n// Clear removes all entries\nfunc (mc *MetadataCache) Clear() {\n\tmc.mu.Lock()\n\tdefer mc.mu.Unlock()\n\t\n\tmc.entries = make(map[string]*MetadataCacheEntry)\n\tmc.head.next = mc.tail\n\tmc.tail.prev = mc.head\n}\n\n// LRU operations\nfunc (mc *MetadataCache) addToFront(entry *MetadataCacheEntry) {\n\tentry.prev = mc.head\n\tentry.next = mc.head.next\n\tmc.head.next.prev = entry\n\tmc.head.next = entry\n}\n\nfunc (mc *MetadataCache) removeFromList(entry *MetadataCacheEntry) {\n\tentry.prev.next = entry.next\n\tentry.next.prev = entry.prev\n}\n\nfunc (mc *MetadataCache) moveToFront(entry *MetadataCacheEntry) {\n\tmc.removeFromList(entry)\n\tmc.addToFront(entry)\n}\n\nfunc (mc *MetadataCache) evictLRU() {\n\tif mc.tail.prev != mc.head {\n\t\tlru := mc.tail.prev\n\t\tmc.removeFromList(lru)\n\t\tdelete(mc.entries, lru.Key)\n\t}\n}\n\n// NewComponentScanner creates a new component scanner with optimal worker pool\nfunc NewComponentScanner(registry *registry.ComponentRegistry, cfg ...*config.Config) *ComponentScanner {\n\treturn NewComponentScannerWithConcurrency(registry, 0, cfg...) // 0 = auto-detect optimal\n}\n\n// NewComponentScannerWithConcurrency creates a new component scanner with configurable concurrency\nfunc NewComponentScannerWithConcurrency(registry *registry.ComponentRegistry, maxWorkers int, cfg ...*config.Config) *ComponentScanner {\n\tscanner := &ComponentScanner{\n\t\tregistry:   registry,\n\t\tfileSet:    token.NewFileSet(),\n\t\tpathCache:  &pathValidationCache{},\n\t\tbufferPool: NewBufferPool(),\n\t\t// Initialize metadata cache: 1000 entries max, 1 hour TTL\n\t\t// This caches ~1000-2000 component metadata entries typically\n\t\tmetadataCache: NewMetadataCache(1000, time.Hour),\n\t\t// Initialize performance metrics tracking\n\t\tmetrics: &ScannerMetrics{},\n\t}\n\n\t// Initialize worker pool with configurable or optimal worker count\n\tworkerCount := maxWorkers\n\tif workerCount <= 0 {\n\t\t// Auto-detect optimal worker count\n\t\tworkerCount = runtime.NumCPU()\n\t\tif workerCount > 8 {\n\t\t\tworkerCount = 8 // Cap at 8 workers for diminishing returns\n\t\t}\n\t} else {\n\t\t// User-specified count, but enforce reasonable limits\n\t\tif workerCount > 64 {\n\t\t\tworkerCount = 64 // Maximum safety limit\n\t\t}\n\t}\n\n\tscanner.workerPool = NewWorkerPool(workerCount, scanner)\n\t\n\t// Initialize AST parsing pool with fewer workers to avoid oversubscription\n\tastWorkerCount := workerCount / 2\n\tif astWorkerCount < 1 {\n\t\tastWorkerCount = 1\n\t}\n\tscanner.astParsingPool = NewASTParsingPool(astWorkerCount)\n\n\t// Use first config if provided, otherwise nil\n\tif len(cfg) > 0 {\n\t\tscanner.config = cfg[0]\n\t}\n\t\n\treturn scanner\n}\n\n// getFileScanTimeout returns the configured timeout for file scanning operations\nfunc (s *ComponentScanner) getFileScanTimeout() time.Duration {\n\tif s.config != nil && s.config.Timeouts.FileScan > 0 {\n\t\treturn s.config.Timeouts.FileScan\n\t}\n\t// Default fallback timeout if no configuration is available\n\treturn 30 * time.Second\n}\n\n// NewWorkerPool creates a new worker pool for scanning operations\nfunc NewWorkerPool(workerCount int, scanner *ComponentScanner) *WorkerPool {\n\tpool := &WorkerPool{\n\t\tjobQueue:    make(chan ScanJob, workerCount*2), // Buffer for work-stealing efficiency\n\t\tworkerCount: workerCount,\n\t\tscanner:     scanner,\n\t\tstop:        make(chan struct{}),\n\t}\n\n\t// Start persistent workers\n\tpool.workers = make([]*ScanWorker, workerCount)\n\tfor i := 0; i < workerCount; i++ {\n\t\tworker := &ScanWorker{\n\t\t\tid:       i,\n\t\t\tjobQueue: pool.jobQueue,\n\t\t\tscanner:  scanner,\n\t\t\tstop:     make(chan struct{}),\n\t\t}\n\t\tpool.workers[i] = worker\n\t\tgo worker.start()\n\t}\n\n\treturn pool\n}\n\n// start begins the worker's processing loop\nfunc (w *ScanWorker) start() {\n\tfor {\n\t\tselect {\n\t\tcase job := <-w.jobQueue:\n\t\t\t// Process the scanning job\n\t\t\terr := w.scanner.scanFileInternal(job.filePath)\n\t\t\tjob.result <- ScanResult{\n\t\t\t\tfilePath: job.filePath,\n\t\t\t\terr:      err,\n\t\t\t}\n\t\tcase <-w.stop:\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Stop gracefully shuts down the worker pool\nfunc (p *WorkerPool) Stop() {\n\tp.mu.Lock()\n\tdefer p.mu.Unlock()\n\n\tif p.stopped {\n\t\treturn\n\t}\n\n\tp.stopped = true\n\tclose(p.stop)\n\n\t// Stop all workers\n\tfor _, worker := range p.workers {\n\t\tclose(worker.stop)\n\t}\n\n\t// Close job queue\n\tclose(p.jobQueue)\n}\n\n// GetRegistry returns the component registry\nfunc (s *ComponentScanner) GetRegistry() interfaces.ComponentRegistry {\n\treturn s.registry\n}\n\n// GetWorkerCount returns the number of active workers in the pool\nfunc (s *ComponentScanner) GetWorkerCount() int {\n\tif s.workerPool == nil {\n\t\treturn 0\n\t}\n\ts.workerPool.mu.RLock()\n\tdefer s.workerPool.mu.RUnlock()\n\treturn s.workerPool.workerCount\n}\n\n// GetMetrics returns a copy of the current scanner metrics\nfunc (s *ComponentScanner) GetMetrics() ScannerMetrics {\n\tif s.metrics == nil {\n\t\treturn ScannerMetrics{}\n\t}\n\treturn ScannerMetrics{\n\t\tFilesProcessed:  atomic.LoadInt64(&s.metrics.FilesProcessed),\n\t\tComponentsFound: atomic.LoadInt64(&s.metrics.ComponentsFound),\n\t\tCacheHits:       atomic.LoadInt64(&s.metrics.CacheHits),\n\t\tCacheMisses:     atomic.LoadInt64(&s.metrics.CacheMisses),\n\t\tTotalScanTime:   s.metrics.TotalScanTime,\n\t\tPeakMemoryUsage: atomic.LoadUint64(&s.metrics.PeakMemoryUsage),\n\t\tConcurrentJobs:  atomic.LoadInt64(&s.metrics.ConcurrentJobs),\n\t}\n}\n\n// ResetMetrics clears all scanner metrics\nfunc (s *ComponentScanner) ResetMetrics() {\n\tif s.metrics == nil {\n\t\treturn\n\t}\n\tatomic.StoreInt64(&s.metrics.FilesProcessed, 0)\n\tatomic.StoreInt64(&s.metrics.ComponentsFound, 0)\n\tatomic.StoreInt64(&s.metrics.CacheHits, 0)\n\tatomic.StoreInt64(&s.metrics.CacheMisses, 0)\n\tatomic.StoreUint64(&s.metrics.PeakMemoryUsage, 0)\n\tatomic.StoreInt64(&s.metrics.ConcurrentJobs, 0)\n\ts.metrics.TotalScanTime = 0\n}\n\n// Close gracefully shuts down the scanner and its worker pool\nfunc (s *ComponentScanner) Close() error {\n\tif s.astParsingPool != nil {\n\t\ts.astParsingPool.Close()\n\t}\n\tif s.workerPool != nil {\n\t\ts.workerPool.Stop()\n\t}\n\tif s.metadataCache != nil {\n\t\ts.metadataCache.Clear()\n\t}\n\treturn nil\n}\n\n// getCachedMetadata attempts to retrieve cached component metadata for a file\nfunc (s *ComponentScanner) getCachedMetadata(filePath, fileHash string) (*CachedComponentMetadata, bool) {\n\tif s.metadataCache == nil {\n\t\treturn nil, false\n\t}\n\n\tcacheKey := fmt.Sprintf(\"%s:%s\", filePath, fileHash)\n\tcachedData, found := s.metadataCache.Get(cacheKey)\n\tif !found {\n\t\treturn nil, false\n\t}\n\n\tvar metadata CachedComponentMetadata\n\tif err := json.Unmarshal(cachedData, &metadata); err != nil {\n\t\t// Cache corruption - remove invalid entry\n\t\ts.metadataCache.Set(cacheKey, nil)\n\t\treturn nil, false\n\t}\n\n\t// Verify the cached hash matches current file hash (additional safety check)\n\tif metadata.FileHash != fileHash {\n\t\treturn nil, false\n\t}\n\n\treturn &metadata, true\n}\n\n// setCachedMetadata stores component metadata in the cache\nfunc (s *ComponentScanner) setCachedMetadata(filePath, fileHash string, components []*types.ComponentInfo) {\n\tif s.metadataCache == nil {\n\t\treturn\n\t}\n\n\tmetadata := CachedComponentMetadata{\n\t\tComponents: components,\n\t\tFileHash:   fileHash,\n\t\tParsedAt:   time.Now(),\n\t}\n\n\tdata, err := json.Marshal(metadata)\n\tif err != nil {\n\t\t// Skip caching if marshaling fails\n\t\treturn\n\t}\n\n\tcacheKey := fmt.Sprintf(\"%s:%s\", filePath, fileHash)\n\ts.metadataCache.Set(cacheKey, data)\n}\n\n// ScanDirectory scans a directory for templ components using optimized worker pool with timeout support\nfunc (s *ComponentScanner) ScanDirectoryWithContext(ctx context.Context, dir string) error {\n\tstart := time.Now()\n\t\n\t// Track memory usage at start\n\tvar startMem runtime.MemStats\n\truntime.ReadMemStats(&startMem)\n\t\n\t// Validate directory path to prevent path traversal\n\tif _, err := s.validatePath(dir); err != nil {\n\t\treturn errors.WrapValidation(err, errors.ErrCodeInvalidPath, \n\t\t\t\"directory path validation failed\").\n\t\t\tWithContext(\"directory\", dir)\n\t}\n\n\t// Use concurrent directory walking for better performance on large codebases\n\tfiles, err := s.walkDirectoryConcurrent(dir)\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Process files using persistent worker pool with context (no goroutine creation overhead)\n\terr = s.processBatchWithWorkerPoolWithContext(ctx, files)\n\t\n\t// Update metrics\n\tif s.metrics != nil {\n\t\telapsed := time.Since(start)\n\t\ts.metrics.TotalScanTime += elapsed\n\t\tatomic.AddInt64(&s.metrics.FilesProcessed, int64(len(files)))\n\t\t\n\t\t// Track memory usage\n\t\tvar endMem runtime.MemStats\n\t\truntime.ReadMemStats(&endMem)\n\t\tmemUsed := endMem.Alloc - startMem.Alloc\n\t\t\n\t\t// Update peak memory if this scan used more\n\t\tfor {\n\t\t\tcurrent := atomic.LoadUint64(&s.metrics.PeakMemoryUsage)\n\t\t\tif memUsed <= current || atomic.CompareAndSwapUint64(&s.metrics.PeakMemoryUsage, current, memUsed) {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\t\n\treturn err\n}\n\n// ScanDirectory scans a directory for templ components (backward compatible wrapper)\nfunc (s *ComponentScanner) ScanDirectory(dir string) error {\n\t// Create a timeout context for the scan operation\n\tscanTimeout := s.getFileScanTimeout()\n\tctx, cancel := context.WithTimeout(context.Background(), scanTimeout)\n\tdefer cancel()\n\t\n\treturn s.ScanDirectoryWithContext(ctx, dir)\n}\n\n// processBatchWithWorkerPoolWithContext processes files using the persistent worker pool with optimized batching and context support\nfunc (s *ComponentScanner) processBatchWithWorkerPoolWithContext(ctx context.Context, files []string) error {\n\tif len(files) == 0 {\n\t\treturn nil\n\t}\n\n\t// For very small batches, process synchronously to avoid overhead\n\tif len(files) <= 5 {\n\t\treturn s.processBatchSynchronous(files)\n\t}\n\n\t// Create result channel for collecting results\n\tresultChan := make(chan ScanResult, len(files))\n\tsubmitted := 0\n\t\n\t// Submit jobs to persistent worker pool\n\tfor _, file := range files {\n\t\t// Check if context is cancelled\n\t\tselect {\n\t\tcase <-ctx.Done():\n\t\t\treturn ctx.Err()\n\t\tdefault:\n\t\t}\n\n\t\tjob := ScanJob{\n\t\t\tfilePath: file,\n\t\t\tresult:   resultChan,\n\t\t}\n\n\t\tselect {\n\t\tcase s.workerPool.jobQueue <- job:\n\t\t\t// Job submitted successfully\n\t\t\tsubmitted++\n\t\tcase <-ctx.Done():\n\t\t\t// Context cancelled while submitting\n\t\t\treturn ctx.Err()\n\t\tdefault:\n\t\t\t// Worker pool is full, process synchronously as fallback\n\t\t\terr := s.scanFileInternal(file)\n\t\t\tresultChan <- ScanResult{filePath: file, err: err}\n\t\t}\n\t}\n\n\t// Collect results with context checking\n\tvar scanErrors []error\n\tfor i := 0; i < len(files); i++ {\n\t\tselect {\n\t\tcase result := <-resultChan:\n\t\t\tif result.err != nil {\n\t\t\t\t// Enhance the error with file context\n\t\t\t\tenhancedErr := errors.EnhanceError(result.err, \"scanner\", result.filePath, 0, 0)\n\t\t\t\tscanErrors = append(scanErrors, enhancedErr)\n\t\t\t}\n\t\tcase <-ctx.Done():\n\t\t\t// Context cancelled while collecting results\n\t\t\treturn ctx.Err()\n\t\t}\n\t}\n\n\tclose(resultChan)\n\n\tif len(scanErrors) > 0 {\n\t\treturn errors.CombineErrors(scanErrors...)\n\t}\n\n\treturn nil\n}\n\n// processBatchWithWorkerPool processes files using the persistent worker pool (backward compatible wrapper)\nfunc (s *ComponentScanner) processBatchWithWorkerPool(files []string) error {\n\t// Use background context with timeout for backward compatibility\n\tscanTimeout := s.getFileScanTimeout()\n\tctx, cancel := context.WithTimeout(context.Background(), scanTimeout)\n\tdefer cancel()\n\t\n\treturn s.processBatchWithWorkerPoolWithContext(ctx, files)\n}\n\n// processBatchSynchronous processes small batches synchronously for better performance\nfunc (s *ComponentScanner) processBatchSynchronous(files []string) error {\n\tvar scanErrors []error\n\t\n\tfor _, file := range files {\n\t\tif err := s.scanFileInternal(file); err != nil {\n\t\t\tenhancedErr := errors.EnhanceError(err, \"scanner\", file, 0, 0)\n\t\t\tscanErrors = append(scanErrors, enhancedErr)\n\t\t}\n\t}\n\n\tif len(scanErrors) > 0 {\n\t\treturn errors.CombineErrors(scanErrors...)\n\t}\n\n\treturn nil\n}\n\n// ScanDirectoryParallel is deprecated in favor of the optimized ScanDirectory\n// Kept for backward compatibility\nfunc (s *ComponentScanner) ScanDirectoryParallel(dir string, workers int) error {\n\treturn s.ScanDirectory(dir) // Use optimized version\n}\n\n// ScanFile scans a single file for templ components (optimized)\nfunc (s *ComponentScanner) ScanFile(path string) error {\n\treturn s.scanFileInternal(path)\n}\n\n// scanFileInternal is the optimized internal scanning method used by workers\nfunc (s *ComponentScanner) scanFileInternal(path string) error {\n\t// Validate and clean the path to prevent directory traversal\n\tcleanPath, err := s.validatePath(path)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"invalid path: %w\", err)\n\t}\n\n\t// Optimized single I/O operation: open file and get both content and info\n\tfile, err := os.Open(cleanPath)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"opening file %s: %w\", cleanPath, err)\n\t}\n\tdefer file.Close()\n\n\t// Get file info without separate Stat() call\n\tinfo, err := file.Stat()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"getting file info for %s: %w\", cleanPath, err)\n\t}\n\n\t// Get buffer from pool for optimized memory usage\n\tbuffer := s.bufferPool.Get()\n\tdefer s.bufferPool.Put(buffer)\n\n\t// Read content efficiently using buffer pool\n\tvar content []byte\n\tif info.Size() > 64*1024 {\n\t\t// Use streaming read for large files to reduce memory pressure\n\t\tcontent, err = s.readFileStreamingOptimized(file, info.Size(), buffer)\n\t} else {\n\t\t// Use pooled buffer for small files\n\t\tif cap(buffer) < int(info.Size()) {\n\t\t\tbuffer = make([]byte, info.Size())\n\t\t}\n\t\tbuffer = buffer[:info.Size()]\n\t\t_, err = file.Read(buffer)\n\t\tif err == nil {\n\t\t\tcontent = make([]byte, len(buffer))\n\t\t\tcopy(content, buffer)\n\t\t}\n\t}\n\n\tif err != nil {\n\t\treturn fmt.Errorf(\"reading file %s: %w\", cleanPath, err)\n\t}\n\n\t// Calculate optimized file hash for cache lookup and change detection\n\thash, hashStrategy := s.generateOptimizedHash(content, info)\n\t\n\t// Track hash generation metrics\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.FilesProcessed, 1)\n\t\t// Track hash strategy performance (avoid unused variable)\n\t\t_ = hashStrategy\n\t}\n\t\n\t// Check cache first - avoid expensive parsing if metadata is cached\n\tif cachedMetadata, found := s.getCachedMetadata(cleanPath, hash); found {\n\t\t// Track cache hit\n\t\tif s.metrics != nil {\n\t\t\tatomic.AddInt64(&s.metrics.CacheHits, 1)\n\t\t}\n\t\t\n\t\t// Register all cached components with the registry\n\t\tfor _, component := range cachedMetadata.Components {\n\t\t\t// Update file modification time to current scan time\n\t\t\tupdatedComponent := *component\n\t\t\tupdatedComponent.LastMod = info.ModTime()\n\t\t\tupdatedComponent.Hash = hash\n\t\t\ts.registry.Register(&updatedComponent)\n\t\t}\n\t\t\n\t\t// Track components found\n\t\tif s.metrics != nil {\n\t\t\tatomic.AddInt64(&s.metrics.ComponentsFound, int64(len(cachedMetadata.Components)))\n\t\t}\n\t\t\n\t\treturn nil\n\t}\n\n\t// Track cache miss\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.CacheMisses, 1)\n\t}\n\n\t// Cache miss - perform parsing with async AST parsing to avoid blocking worker threads\n\tvar components []*types.ComponentInfo\n\t\n\t// Use async AST parsing to avoid blocking the worker thread\n\tastResultChan := s.astParsingPool.ParseAsync(cleanPath, content, s.fileSet)\n\t\n\t// Wait for AST parsing result (non-blocking for the worker thread)\n\tastResult := <-astResultChan\n\t\n\tif astResult.err != nil {\n\t\t// If AST parsing fails, try manual component extraction for .templ files\n\t\tcomponents, err = s.parseTemplFileWithComponents(cleanPath, content, hash, info.ModTime())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\t// Extract components from AST\n\t\tcomponents, err = s.extractFromASTWithComponents(cleanPath, astResult.astFile, hash, info.ModTime())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// Cache the parsed components for future scans\n\ts.setCachedMetadata(cleanPath, hash, components)\n\n\t// Register all components with the registry\n\tfor _, component := range components {\n\t\ts.registry.Register(component)\n\t}\n\t\n\t// Track components found\n\tif s.metrics != nil {\n\t\tatomic.AddInt64(&s.metrics.ComponentsFound, int64(len(components)))\n\t}\n\n\treturn nil\n}\n\n// readFileStreaming removed - replaced by readFileStreamingOptimized\n\n// readFileStreamingOptimized reads large files using pooled buffers for better memory efficiency\nfunc (s *ComponentScanner) readFileStreamingOptimized(file *os.File, size int64, pooledBuffer []byte) ([]byte, error) {\n\tconst chunkSize = 32 * 1024 // 32KB chunks\n\t\n\t// Use a reasonably-sized chunk buffer for reading\n\tvar chunk []byte\n\tif cap(pooledBuffer) >= chunkSize {\n\t\tchunk = pooledBuffer[:chunkSize]\n\t} else {\n\t\tchunk = make([]byte, chunkSize)\n\t}\n\n\t// Pre-allocate content buffer with exact size to avoid reallocations\n\tcontent := make([]byte, 0, size)\n\n\tfor {\n\t\tn, err := file.Read(chunk)\n\t\tif n > 0 {\n\t\t\tcontent = append(content, chunk[:n]...)\n\t\t}\n\t\tif err != nil {\n\t\t\tif err == io.EOF {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\treturn nil, err\n\t\t}\n\t\tif n < chunkSize {\n\t\t\tbreak\n\t\t}\n\t}\n\n\treturn content, nil\n}\n\n// Backward compatibility method removed - unused\n\n// parseTemplFileWithComponents extracts components from templ files and returns them\nfunc (s *ComponentScanner) parseTemplFileWithComponents(path string, content []byte, hash string, modTime time.Time) ([]*types.ComponentInfo, error) {\n\tvar components []*types.ComponentInfo\n\tlines := strings.Split(string(content), \"\\n\")\n\tpackageName := \"\"\n\n\tfor _, line := range lines {\n\t\tline = strings.TrimSpace(line)\n\n\t\t// Extract package name\n\t\tif strings.HasPrefix(line, \"package \") {\n\t\t\tparts := strings.Fields(line)\n\t\t\tif len(parts) >= 2 {\n\t\t\t\tpackageName = sanitizeIdentifier(parts[1])\n\t\t\t}\n\t\t}\n\n\t\t// Extract templ component declarations\n\t\tif strings.HasPrefix(line, \"templ \") {\n\t\t\t// Extract component name from templ declaration\n\t\t\tparts := strings.Fields(line)\n\t\t\tif len(parts) >= 2 {\n\t\t\t\tname := parts[1]\n\t\t\t\tif idx := strings.Index(name, \"(\"); idx != -1 {\n\t\t\t\t\tname = name[:idx]\n\t\t\t\t}\n\n\t\t\t\t// Sanitize component name to prevent injection\n\t\t\t\tname = sanitizeIdentifier(name)\n\n\t\t\t\tcomponent := &types.ComponentInfo{\n\t\t\t\t\tName:         name,\n\t\t\t\t\tPackage:      packageName,\n\t\t\t\t\tFilePath:     path,\n\t\t\t\t\tParameters:   extractParameters(line),\n\t\t\t\t\tImports:      []string{},\n\t\t\t\t\tLastMod:      modTime,\n\t\t\t\t\tHash:         hash,\n\t\t\t\t\tDependencies: []string{},\n\t\t\t\t}\n\n\t\t\t\tcomponents = append(components, component)\n\t\t\t}\n\t\t}\n\t}\n\n\treturn components, nil\n}\n\n// parseTemplFile provides backward compatibility - delegates to the new component-returning version\nfunc (s *ComponentScanner) parseTemplFile(path string, content []byte, hash string, modTime time.Time) error {\n\tcomponents, err := s.parseTemplFileWithComponents(path, content, hash, modTime)\n\tif err != nil {\n\t\treturn err\n\t}\n\t\n\t// Register all components\n\tfor _, component := range components {\n\t\ts.registry.Register(component)\n\t}\n\t\n\treturn nil\n}\n\n// extractFromASTWithComponents extracts components from AST and returns them\nfunc (s *ComponentScanner) extractFromASTWithComponents(path string, astFile *ast.File, hash string, modTime time.Time) ([]*types.ComponentInfo, error) {\n\tvar components []*types.ComponentInfo\n\t\n\t// Walk the AST to find function declarations that might be templ components\n\tast.Inspect(astFile, func(n ast.Node) bool {\n\t\tswitch node := n.(type) {\n\t\tcase *ast.FuncDecl:\n\t\t\tif node.Name != nil && node.Name.IsExported() {\n\t\t\t\t// Check if this might be a templ component\n\t\t\t\tif s.isTemplComponent(node) {\n\t\t\t\t\tcomponent := &types.ComponentInfo{\n\t\t\t\t\t\tName:         node.Name.Name,\n\t\t\t\t\t\tPackage:      astFile.Name.Name,\n\t\t\t\t\t\tFilePath:     path,\n\t\t\t\t\t\tParameters:   s.extractParametersFromFunc(node),\n\t\t\t\t\t\tImports:      s.extractImports(astFile),\n\t\t\t\t\t\tLastMod:      modTime,\n\t\t\t\t\t\tHash:         hash,\n\t\t\t\t\t\tDependencies: []string{},\n\t\t\t\t\t}\n\n\t\t\t\t\tcomponents = append(components, component)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn true\n\t})\n\n\treturn components, nil\n}\n\n// extractFromAST provides backward compatibility - delegates to the new component-returning version\nfunc (s *ComponentScanner) extractFromAST(path string, astFile *ast.File, hash string, modTime time.Time) error {\n\tcomponents, err := s.extractFromASTWithComponents(path, astFile, hash, modTime)\n\tif err != nil {\n\t\treturn err\n\t}\n\t\n\t// Register all components\n\tfor _, component := range components {\n\t\ts.registry.Register(component)\n\t}\n\t\n\treturn nil\n}\n\nfunc (s *ComponentScanner) isTemplComponent(fn *ast.FuncDecl) bool {\n\t// Check if the function returns a templ.Component\n\tif fn.Type.Results == nil || len(fn.Type.Results.List) == 0 {\n\t\treturn false\n\t}\n\n\tresult := fn.Type.Results.List[0]\n\tif sel, ok := result.Type.(*ast.SelectorExpr); ok {\n\t\tif ident, ok := sel.X.(*ast.Ident); ok {\n\t\t\treturn ident.Name == \"templ\" && sel.Sel.Name == \"Component\"\n\t\t}\n\t}\n\n\treturn false\n}\n\nfunc (s *ComponentScanner) extractParametersFromFunc(fn *ast.FuncDecl) []types.ParameterInfo {\n\tvar params []types.ParameterInfo\n\n\tif fn.Type.Params == nil {\n\t\treturn params\n\t}\n\n\tfor _, param := range fn.Type.Params.List {\n\t\tparamType := \"\"\n\t\tif param.Type != nil {\n\t\t\tparamType = s.typeToString(param.Type)\n\t\t}\n\n\t\tfor _, name := range param.Names {\n\t\t\tparams = append(params, types.ParameterInfo{\n\t\t\t\tName:     name.Name,\n\t\t\t\tType:     paramType,\n\t\t\t\tOptional: false,\n\t\t\t\tDefault:  nil,\n\t\t\t})\n\t\t}\n\t}\n\n\treturn params\n}\n\nfunc (s *ComponentScanner) extractImports(astFile *ast.File) []string {\n\tvar imports []string\n\n\tfor _, imp := range astFile.Imports {\n\t\tif imp.Path != nil {\n\t\t\timports = append(imports, imp.Path.Value)\n\t\t}\n\t}\n\n\treturn imports\n}\n\nfunc (s *ComponentScanner) typeToString(expr ast.Expr) string {\n\tswitch e := expr.(type) {\n\tcase *ast.Ident:\n\t\treturn e.Name\n\tcase *ast.SelectorExpr:\n\t\treturn s.typeToString(e.X) + \".\" + e.Sel.Name\n\tcase *ast.StarExpr:\n\t\treturn \"*\" + s.typeToString(e.X)\n\tcase *ast.ArrayType:\n\t\treturn \"[]\" + s.typeToString(e.Elt)\n\tdefault:\n\t\treturn \"unknown\"\n\t}\n}\n\nfunc extractParameters(line string) []types.ParameterInfo {\n\t// Simple parameter extraction from templ declaration\n\t// This is a basic implementation - real parser would be more robust\n\tif !strings.Contains(line, \"(\") {\n\t\treturn []types.ParameterInfo{}\n\t}\n\n\tstart := strings.Index(line, \"(\")\n\tend := strings.LastIndex(line, \")\")\n\tif start == -1 || end == -1 || start >= end {\n\t\treturn []types.ParameterInfo{}\n\t}\n\n\tparamStr := line[start+1 : end]\n\tif strings.TrimSpace(paramStr) == \"\" {\n\t\treturn []types.ParameterInfo{}\n\t}\n\n\t// Basic parameter parsing - handle both \"name type\" and \"name, name type\" patterns\n\tparts := strings.Split(paramStr, \",\")\n\tvar params []types.ParameterInfo\n\n\tfor _, part := range parts {\n\t\tpart = strings.TrimSpace(part)\n\t\tif part == \"\" {\n\t\t\tcontinue\n\t\t}\n\n\t\t// Split by space to get name and type\n\t\tfields := strings.Fields(part)\n\t\tif len(fields) >= 2 {\n\t\t\t// Handle \"name type\" format\n\t\t\tparams = append(params, types.ParameterInfo{\n\t\t\t\tName:     fields[0],\n\t\t\t\tType:     fields[1],\n\t\t\t\tOptional: false,\n\t\t\t\tDefault:  nil,\n\t\t\t})\n\t\t} else if len(fields) == 1 {\n\t\t\t// Handle single parameter name (type might be from previous param)\n\t\t\tparams = append(params, types.ParameterInfo{\n\t\t\t\tName:     fields[0],\n\t\t\t\tType:     \"string\", // Default type\n\t\t\t\tOptional: false,\n\t\t\t\tDefault:  nil,\n\t\t\t})\n\t\t}\n\t}\n\n\treturn params\n}\n\n// sanitizeIdentifier removes dangerous characters from identifiers\nfunc sanitizeIdentifier(identifier string) string {\n\t// Only allow alphanumeric characters and underscores for identifiers\n\tvar cleaned strings.Builder\n\tfor _, r := range identifier {\n\t\tif (r >= 'a' && r <= 'z') || (r >= 'A' && r <= 'Z') || (r >= '0' && r <= '9') || r == '_' {\n\t\t\tcleaned.WriteRune(r)\n\t\t}\n\t}\n\treturn cleaned.String()\n}\n\n// validatePath validates and cleans a file path to prevent directory traversal.\n// This optimized version caches the current working directory to avoid repeated\n// expensive filesystem operations, achieving 50-70% performance improvement.\nfunc (s *ComponentScanner) validatePath(path string) (string, error) {\n\t// Clean the path to resolve . and .. elements\n\tcleanPath := filepath.Clean(path)\n\n\t// Get absolute path to normalize (needed for working directory check)\n\tabsPath, err := filepath.Abs(cleanPath)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"getting absolute path: %w\", err)\n\t}\n\n\t// Get cached current working directory\n\tcwd, err := s.getCachedWorkingDir()\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"getting current directory: %w\", err)\n\t}\n\n\t// Primary security check: ensure the path is within the current working directory\n\t// This prevents directory traversal attacks that escape the working directory\n\tif !strings.HasPrefix(absPath, cwd) {\n\t\treturn \"\", errors.ErrPathTraversal(path).WithContext(\"working_directory\", cwd)\n\t}\n\n\t// Secondary security check: reject paths with suspicious patterns\n\t// This catches directory traversal attempts that stay within the working directory\n\tif strings.Contains(cleanPath, \"..\") {\n\t\treturn \"\", errors.ErrPathTraversal(path).\n\t\t\tWithContext(\"pattern\", \"contains '..' traversal\")\n\t}\n\n\treturn cleanPath, nil\n}\n\n// getCachedWorkingDir returns the current working directory from cache,\n// initializing it on first access. This eliminates repeated os.Getwd() calls.\nfunc (s *ComponentScanner) getCachedWorkingDir() (string, error) {\n\t// Fast path: check if already initialized with read lock\n\ts.pathCache.mu.RLock()\n\tif s.pathCache.initialized {\n\t\tcwd := s.pathCache.currentWorkingDir\n\t\ts.pathCache.mu.RUnlock()\n\t\treturn cwd, nil\n\t}\n\ts.pathCache.mu.RUnlock()\n\n\t// Slow path: initialize the cache with write lock\n\ts.pathCache.mu.Lock()\n\tdefer s.pathCache.mu.Unlock()\n\n\t// Double-check pattern: another goroutine might have initialized while waiting\n\tif s.pathCache.initialized {\n\t\treturn s.pathCache.currentWorkingDir, nil\n\t}\n\n\t// Get current working directory (expensive syscall - done only once)\n\tcwd, err := os.Getwd()\n\tif err != nil {\n\t\treturn \"\", err\n\t}\n\n\t// Ensure we have the absolute path for consistent comparison\n\tabsCwd, err := filepath.Abs(cwd)\n\tif err != nil {\n\t\treturn \"\", fmt.Errorf(\"getting absolute working directory: %w\", err)\n\t}\n\n\t// Cache the result\n\ts.pathCache.currentWorkingDir = absCwd\n\ts.pathCache.initialized = true\n\n\treturn absCwd, nil\n}\n\n// InvalidatePathCache clears the cached working directory.\n// This should be called if the working directory changes during execution.\nfunc (s *ComponentScanner) InvalidatePathCache() {\n\ts.pathCache.mu.Lock()\n\tdefer s.pathCache.mu.Unlock()\n\ts.pathCache.initialized = false\n\ts.pathCache.currentWorkingDir = \"\"\n}\n\n// walkDirectoryConcurrent implements concurrent directory walking for improved performance\n// on large codebases. Uses goroutines to parallelize directory discovery.\nfunc (s *ComponentScanner) walkDirectoryConcurrent(rootDir string) ([]string, error) {\n\t// For small directory trees, use optimized sequential version\n\t// For larger trees, use concurrent discovery\n\t\n\t// Quick check for directory size to decide approach\n\tentries, err := os.ReadDir(rootDir)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"reading root directory %s: %w\", rootDir, err)\n\t}\n\t\n\t// If small directory, use optimized sequential\n\tif len(entries) < 10 {\n\t\treturn s.walkDirectoryOptimized(rootDir)\n\t}\n\t\n\t// Use concurrent approach for larger directories\n\treturn s.walkDirectoryParallel(rootDir)\n}\n\n// walkDirectoryParallel implements concurrent directory discovery\nfunc (s *ComponentScanner) walkDirectoryParallel(rootDir string) ([]string, error) {\n\t// Use a simple approach: collect all directories first, then process them concurrently\n\t\n\t// First, collect all directories sequentially (this is fast)\n\tvar allDirs []string\n\terr := filepath.WalkDir(rootDir, func(path string, d os.DirEntry, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif d.IsDir() && !s.shouldSkipDirectory(d.Name()) {\n\t\t\tallDirs = append(allDirs, path)\n\t\t}\n\t\treturn nil\n\t})\n\t\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t\n\t// Now process directories concurrently\n\tconst maxWorkers = 4\n\tjobs := make(chan string, len(allDirs))\n\tresults := make(chan []string, len(allDirs))\n\t\n\t// Start workers\n\tvar wg sync.WaitGroup\n\tfor w := 0; w < maxWorkers; w++ {\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\tfor dir := range jobs {\n\t\t\t\tfiles, _, _ := s.processSingleDirectory(dir)\n\t\t\t\tif len(files) > 0 {\n\t\t\t\t\tresults <- files\n\t\t\t\t} else {\n\t\t\t\t\tresults <- nil // Send empty result to maintain count\n\t\t\t\t}\n\t\t\t}\n\t\t}()\n\t}\n\t\n\t// Send jobs\n\tfor _, dir := range allDirs {\n\t\tjobs <- dir\n\t}\n\tclose(jobs)\n\t\n\t// Wait for workers to finish\n\tgo func() {\n\t\twg.Wait()\n\t\tclose(results)\n\t}()\n\t\n\t// Collect results\n\tvar allFiles []string\n\tfor files := range results {\n\t\tif files != nil {\n\t\t\tallFiles = append(allFiles, files...)\n\t\t}\n\t}\n\t\n\treturn allFiles, nil\n}\n\n// walkDirectoryOptimized implements an optimized sequential walk with directory skipping\nfunc (s *ComponentScanner) walkDirectoryOptimized(rootDir string) ([]string, error) {\n\tvar files []string\n\t\n\terr := filepath.WalkDir(rootDir, func(path string, d os.DirEntry, err error) error {\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// Skip directories that typically don't contain templ files\n\t\tif d.IsDir() && s.shouldSkipDirectory(d.Name()) {\n\t\t\treturn filepath.SkipDir\n\t\t}\n\n\t\tif !d.IsDir() && strings.HasSuffix(path, \".templ\") {\n\t\t\t// Validate each file path as we encounter it\n\t\t\tif _, err := s.validatePath(path); err != nil {\n\t\t\t\t// Skip invalid paths silently for security\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tfiles = append(files, path)\n\t\t}\n\t\t\n\t\treturn nil\n\t})\n\t\n\treturn files, err\n}\n\n// processSingleDirectory processes a single directory and returns files and subdirectories\nfunc (s *ComponentScanner) processSingleDirectory(dir string) ([]string, []string, error) {\n\tvar files []string\n\tvar subdirs []string\n\t\n\tentries, err := os.ReadDir(dir)\n\tif err != nil {\n\t\treturn nil, nil, fmt.Errorf(\"reading directory %s: %w\", dir, err)\n\t}\n\t\n\tfor _, entry := range entries {\n\t\tpath := filepath.Join(dir, entry.Name())\n\t\t\n\t\tif entry.IsDir() {\n\t\t\t// Skip directories that typically don't contain templ files\n\t\t\tif s.shouldSkipDirectory(entry.Name()) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tsubdirs = append(subdirs, path)\n\t\t} else if strings.HasSuffix(entry.Name(), \".templ\") {\n\t\t\t// Validate each file path as we encounter it\n\t\t\tif _, err := s.validatePath(path); err != nil {\n\t\t\t\t// Skip invalid paths silently for security\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfiles = append(files, path)\n\t\t}\n\t}\n\t\n\treturn files, subdirs, nil\n}\n\n// shouldSkipDirectory determines if a directory should be skipped for performance\nfunc (s *ComponentScanner) shouldSkipDirectory(name string) bool {\n\tskipDirs := map[string]bool{\n\t\t\".git\":         true,\n\t\t\".svn\":         true,\n\t\t\"node_modules\": true,\n\t\t\".next\":        true,\n\t\t\"dist\":         true,\n\t\t\"build\":        true,\n\t\t\"vendor\":       true,\n\t\t\".vscode\":      true,\n\t\t\".idea\":        true,\n\t\t\"__pycache__\":  true,\n\t\t\".DS_Store\":    true,\n\t}\n\t\n\treturn skipDirs[name]\n}\n\n// HashingStrategy contains information about the hash generation approach used\ntype HashingStrategy struct {\n\tStrategy     FileHashStrategy\n\tSamplePoints int\n\tHashTime     time.Duration\n\tFileSize     int64\n}\n\n// generateOptimizedHash creates an optimized hash based on file size and content characteristics\nfunc (s *ComponentScanner) generateOptimizedHash(content []byte, fileInfo os.FileInfo) (string, *HashingStrategy) {\n\tstart := time.Now()\n\tfileSize := int64(len(content))\n\t\n\tstrategy := &HashingStrategy{\n\t\tFileSize: fileSize,\n\t}\n\t\n\tvar hash uint32\n\t\n\tswitch {\n\tcase fileSize <= smallFileThreshold:\n\t\t// Small files: use full content CRC32 (fast anyway)\n\t\thash = crc32.Checksum(content, crcTable)\n\t\tstrategy.Strategy = HashStrategyFull\n\t\tstrategy.SamplePoints = 1\n\t\t\n\tcase fileSize <= mediumFileThreshold:\n\t\t// Medium files: use content sampling\n\t\thash = s.generateSampledHash(content)\n\t\tstrategy.Strategy = HashStrategySampled\n\t\tstrategy.SamplePoints = 3\n\t\t\n\tdefault:\n\t\t// Large files: use hierarchical sampling with metadata\n\t\thash = s.generateHierarchicalHash(content, fileInfo)\n\t\tstrategy.Strategy = HashStrategyHierarchical\n\t\tstrategy.SamplePoints = 5\n\t}\n\t\n\tstrategy.HashTime = time.Since(start)\n\t\n\t// Include file metadata in hash to catch size/timestamp changes\n\tmetadataHash := s.generateMetadataHash(fileInfo)\n\tcombinedHash := hash ^ metadataHash\n\t\n\treturn strconv.FormatUint(uint64(combinedHash), 16), strategy\n}\n\n// generateSampledHash creates a hash from strategic content samples\nfunc (s *ComponentScanner) generateSampledHash(content []byte) uint32 {\n\tif len(content) == 0 {\n\t\treturn 0\n\t}\n\t\n\t// Sample three strategic points: beginning, middle, and end\n\tsampleSize := contentSampleSize\n\tif len(content) < sampleSize*3 {\n\t\t// If file is small, just hash it all\n\t\treturn crc32.Checksum(content, crcTable)\n\t}\n\t\n\t// Create a combined sample from key sections\n\tvar samples []byte\n\t\n\t// Beginning sample\n\tif len(content) > sampleSize {\n\t\tsamples = append(samples, content[:sampleSize]...)\n\t}\n\t\n\t// Middle sample\n\tmid := len(content) / 2\n\tmidStart := mid - sampleSize/2\n\tmidEnd := mid + sampleSize/2\n\tif midStart >= 0 && midEnd < len(content) {\n\t\tsamples = append(samples, content[midStart:midEnd]...)\n\t}\n\t\n\t// End sample\n\tif len(content) > sampleSize {\n\t\tsamples = append(samples, content[len(content)-sampleSize:]...)\n\t}\n\t\n\treturn crc32.Checksum(samples, crcTable)\n}\n\n// generateHierarchicalHash creates a hierarchical hash for large files\nfunc (s *ComponentScanner) generateHierarchicalHash(content []byte, fileInfo os.FileInfo) uint32 {\n\tif len(content) == 0 {\n\t\treturn 0\n\t}\n\t\n\t// For templ files, focus on key sections that are likely to change\n\tvar keyContent []byte\n\t\n\t// Add file header (package declaration, imports)\n\tif len(content) > 2048 {\n\t\tkeyContent = append(keyContent, content[:2048]...)\n\t}\n\t\n\t// Sample multiple points throughout the file\n\tchunkSize := len(content) / 8 // Divide into 8 chunks\n\tif chunkSize > contentSampleSize {\n\t\tfor i := 1; i < 8; i++ {\n\t\t\tstart := i * chunkSize\n\t\t\tend := start + contentSampleSize/8\n\t\t\tif end < len(content) {\n\t\t\t\tkeyContent = append(keyContent, content[start:end]...)\n\t\t\t}\n\t\t}\n\t}\n\t\n\t// Add file footer (last part likely to contain component definitions)\n\tif len(content) > 1024 {\n\t\tkeyContent = append(keyContent, content[len(content)-1024:]...)\n\t}\n\t\n\treturn crc32.Checksum(keyContent, crcTable)\n}\n\n// generateMetadataHash creates a hash from file metadata\nfunc (s *ComponentScanner) generateMetadataHash(fileInfo os.FileInfo) uint32 {\n\t// Combine file size and modification time for metadata hash\n\tmetadata := fmt.Sprintf(\"%d:%d\", fileInfo.Size(), fileInfo.ModTime().Unix())\n\treturn crc32.ChecksumIEEE([]byte(metadata))\n}\n\n\n",
      "structuredPatch": [
        {
          "oldStart": 1539,
          "oldLines": 24,
          "newStart": 1539,
          "newLines": 28,
          "lines": [
            "     FileSize: fileSize,",
            "   }",
            "   ",
            "-  var hash uint32",
            "+  var primaryHash, secondaryHash uint32",
            "   ",
            "   switch {",
            "   case fileSize <= smallFileThreshold:",
            "     // Small files: use full content CRC32 (fast anyway)",
            "-    hash = crc32.Checksum(content, crcTable)",
            "+    primaryHash = crc32.Checksum(content, crcTable)",
            "     strategy.Strategy = HashStrategyFull",
            "     strategy.SamplePoints = 1",
            "     ",
            "   case fileSize <= mediumFileThreshold:",
            "-    // Medium files: use content sampling",
            "-    hash = s.generateSampledHash(content)",
            "+    // Medium files: use content sampling with fallback",
            "+    primaryHash = s.generateSampledHash(content)",
            "+    // Generate secondary hash for collision detection",
            "+    secondaryHash = s.generateAlternativeHash(content)",
            "     strategy.Strategy = HashStrategySampled",
            "     strategy.SamplePoints = 3",
            "     ",
            "   default:",
            "     // Large files: use hierarchical sampling with metadata",
            "-    hash = s.generateHierarchicalHash(content, fileInfo)",
            "+    primaryHash = s.generateHierarchicalHash(content, fileInfo)",
            "+    // Generate secondary hash for collision detection",
            "+    secondaryHash = s.generateAlternativeHash(content)",
            "     strategy.Strategy = HashStrategyHierarchical",
            "     strategy.SamplePoints = 5",
            "   }"
          ]
        },
        {
          "oldStart": 1565,
          "oldLines": 11,
          "newStart": 1569,
          "newLines": 36,
          "lines": [
            "   ",
            "   // Include file metadata in hash to catch size/timestamp changes",
            "   metadataHash := s.generateMetadataHash(fileInfo)",
            "-  combinedHash := hash ^ metadataHash",
            "   ",
            "+  // Combine primary hash with metadata",
            "+  combinedHash := primaryHash ^ metadataHash",
            "+  ",
            "+  // For collision resistance, incorporate secondary hash if available",
            "+  if secondaryHash != 0 {",
            "+    combinedHash = combinedHash ^ (secondaryHash >> 16)",
            "+  }",
            "+  ",
            "   return strconv.FormatUint(uint64(combinedHash), 16), strategy",
            " }",
            " ",
            "+// generateAlternativeHash creates an alternative hash for collision detection",
            "+func (s *ComponentScanner) generateAlternativeHash(content []byte) uint32 {",
            "+  if len(content) == 0 {",
            "+    return 0",
            "+  }",
            "+  ",
            "+  // Use IEEE CRC32 polynomial (different from Castagnoli) for secondary hash",
            "+  return crc32.ChecksumIEEE(content[:min(len(content), 4096)])",
            "+}",
            "+",
            "+// min returns the minimum of two integers",
            "+func min(a, b int) int {",
            "+  if a < b {",
            "+    return a",
            "+  }",
            "+  return b",
            "+}",
            "+",
            " // generateSampledHash creates a hash from strategic content samples",
            " func (s *ComponentScanner) generateSampledHash(content []byte) uint32 {",
            "   if len(content) == 0 {"
          ]
        }
      ],
      "userModified": false,
      "replaceAll": false
    }
  },
  {
    "session_id": "22406b69-b3c5-42f6-80ab-099c8d4d7011",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/22406b69-b3c5-42f6-80ab-099c8d4d7011.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "TodoWrite",
    "tool_input": {
      "todos": [
        {
          "content": "Analyze current timeout handling patterns in the codebase",
          "status": "completed",
          "priority": "high",
          "id": "1"
        },
        {
          "content": "Design comprehensive timeout configuration system",
          "status": "completed",
          "priority": "high",
          "id": "2"
        },
        {
          "content": "Implement context-based timeouts for build operations",
          "status": "completed",
          "priority": "high",
          "id": "3"
        },
        {
          "content": "Add timeout handling to file scanning operations",
          "status": "completed",
          "priority": "high",
          "id": "4"
        },
        {
          "content": "Implement proper cancellation handling in build pipeline",
          "status": "completed",
          "priority": "high",
          "id": "7"
        },
        {
          "content": "Improve WebSocket connection timeout management",
          "status": "completed",
          "priority": "medium",
          "id": "5"
        },
        {
          "content": "Add configurable timeout values through config system",
          "status": "completed",
          "priority": "medium",
          "id": "6"
        },
        {
          "content": "Add timeout tests and validation",
          "status": "completed",
          "priority": "medium",
          "id": "8"
        }
      ]
    },
    "tool_response": {
      "oldTodos": [
        {
          "content": "Analyze current timeout handling patterns in the codebase",
          "status": "completed",
          "priority": "high",
          "id": "1"
        },
        {
          "content": "Design comprehensive timeout configuration system",
          "status": "completed",
          "priority": "high",
          "id": "2"
        },
        {
          "content": "Implement context-based timeouts for build operations",
          "status": "completed",
          "priority": "high",
          "id": "3"
        },
        {
          "content": "Add timeout handling to file scanning operations",
          "status": "completed",
          "priority": "high",
          "id": "4"
        },
        {
          "content": "Improve WebSocket connection timeout management",
          "status": "completed",
          "priority": "medium",
          "id": "5"
        },
        {
          "content": "Add configurable timeout values through config system",
          "status": "completed",
          "priority": "medium",
          "id": "6"
        },
        {
          "content": "Implement proper cancellation handling in build pipeline",
          "status": "pending",
          "priority": "high",
          "id": "7"
        },
        {
          "content": "Add timeout tests and validation",
          "status": "completed",
          "priority": "medium",
          "id": "8"
        }
      ],
      "newTodos": [
        {
          "content": "Analyze current timeout handling patterns in the codebase",
          "status": "completed",
          "priority": "high",
          "id": "1"
        },
        {
          "content": "Design comprehensive timeout configuration system",
          "status": "completed",
          "priority": "high",
          "id": "2"
        },
        {
          "content": "Implement context-based timeouts for build operations",
          "status": "completed",
          "priority": "high",
          "id": "3"
        },
        {
          "content": "Add timeout handling to file scanning operations",
          "status": "completed",
          "priority": "high",
          "id": "4"
        },
        {
          "content": "Implement proper cancellation handling in build pipeline",
          "status": "completed",
          "priority": "high",
          "id": "7"
        },
        {
          "content": "Improve WebSocket connection timeout management",
          "status": "completed",
          "priority": "medium",
          "id": "5"
        },
        {
          "content": "Add configurable timeout values through config system",
          "status": "completed",
          "priority": "medium",
          "id": "6"
        },
        {
          "content": "Add timeout tests and validation",
          "status": "completed",
          "priority": "medium",
          "id": "8"
        }
      ]
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "go build ./internal/scanner",
      "description": "Test compilation with collision detection"
    },
    "tool_response": {
      "stdout": "",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "42e03a9d-fb13-4416-96b7-b2ff5d56bda2",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/42e03a9d-fb13-4416-96b7-b2ff5d56bda2.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Write",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/server/websocket_memory_fix_test.go",
      "content": "package server\n\nimport (\n\t\"context\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/coder/websocket\"\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\n// TestMemoryLeakPreventionManager_BasicLifecycle tests basic connection lifecycle\nfunc TestMemoryLeakPreventionManager_BasicLifecycle(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 10,\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        30 * time.Second,\n\t\t\tWebSocketCleanup: 1 * time.Second,\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\tdefer manager.Shutdown(context.Background())\n\t\n\t// Test initial state\n\tmetrics := manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(0), metrics.ActiveConnections)\n\tassert.True(t, manager.IsHealthy())\n\t\n\t// Create mock connection\n\tmockConn := &mockWebSocketConn{}\n\t\n\t// Register connection\n\tconnInfo, err := manager.RegisterConnection(mockConn, \"192.168.1.100:12345\")\n\trequire.NoError(t, err)\n\trequire.NotNil(t, connInfo)\n\tassert.True(t, connInfo.IsActive)\n\tassert.Equal(t, \"192.168.1.100:12345\", connInfo.RemoteAddr)\n\t\n\t// Verify metrics updated\n\tmetrics = manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(1), metrics.ActiveConnections)\n\tassert.Equal(t, int64(1), metrics.TotalConnections)\n\t\n\t// Unregister connection\n\terr = manager.UnregisterConnection(connInfo.ID)\n\tassert.NoError(t, err)\n\t\n\t// Verify cleanup\n\tmetrics = manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(0), metrics.ActiveConnections)\n\tassert.Equal(t, int64(1), metrics.CleanedConnections)\n}\n\n// TestMemoryLeakPreventionManager_ConnectionLimit tests connection limit enforcement\nfunc TestMemoryLeakPreventionManager_ConnectionLimit(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 2, // Low limit for testing\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        30 * time.Second,\n\t\t\tWebSocketCleanup: 1 * time.Second,\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\tdefer manager.Shutdown(context.Background())\n\t\n\tvar connections []*ConnectionInfo\n\t\n\t// Register up to limit\n\tfor i := 0; i < 2; i++ {\n\t\tmockConn := &mockWebSocketConn{}\n\t\tconnInfo, err := manager.RegisterConnection(mockConn, \"192.168.1.100:1234\"+string(rune(i)))\n\t\trequire.NoError(t, err)\n\t\tconnections = append(connections, connInfo)\n\t}\n\t\n\t// Verify limit reached\n\tmetrics := manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(2), metrics.ActiveConnections)\n\t\n\t// Try to exceed limit\n\tmockConn := &mockWebSocketConn{}\n\tconnInfo, err := manager.RegisterConnection(mockConn, \"192.168.1.100:12347\")\n\tassert.Error(t, err)\n\tassert.Nil(t, connInfo)\n\tassert.Contains(t, err.Error(), \"connection limit exceeded\")\n\t\n\t// Verify failed connection metric\n\tmetrics = manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(1), metrics.FailedConnections)\n\t\n\t// Clean up\n\tfor _, conn := range connections {\n\t\tmanager.UnregisterConnection(conn.ID)\n\t}\n}\n\n// TestMemoryLeakPreventionManager_TimeoutCleanup tests automatic timeout cleanup\nfunc TestMemoryLeakPreventionManager_TimeoutCleanup(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 10,\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        100 * time.Millisecond, // Very short timeout for testing\n\t\t\tWebSocketCleanup: 50 * time.Millisecond,  // Frequent cleanup\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\tdefer manager.Shutdown(context.Background())\n\t\n\t// Register connection\n\tmockConn := &mockWebSocketConn{}\n\tconnInfo, err := manager.RegisterConnection(mockConn, \"192.168.1.100:12345\")\n\trequire.NoError(t, err)\n\t\n\t// Verify connection is active\n\tmetrics := manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(1), metrics.ActiveConnections)\n\t\n\t// Wait for timeout and cleanup\n\ttime.Sleep(200 * time.Millisecond)\n\t\n\t// Verify connection was cleaned up automatically\n\tmetrics = manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(0), metrics.ActiveConnections)\n\tassert.Equal(t, int64(1), metrics.TimeoutConnections)\n\tassert.Greater(t, metrics.CleanupOperations, int64(0))\n\t\n\t// Verify connection info is no longer accessible\n\t_, exists := manager.GetConnectionInfo(connInfo.ID)\n\tassert.False(t, exists)\n}\n\n// TestMemoryLeakPreventionManager_ActivityUpdates tests activity tracking\nfunc TestMemoryLeakPreventionManager_ActivityUpdates(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 10,\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        200 * time.Millisecond,\n\t\t\tWebSocketCleanup: 50 * time.Millisecond,\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\tdefer manager.Shutdown(context.Background())\n\t\n\t// Register connection\n\tmockConn := &mockWebSocketConn{}\n\tconnInfo, err := manager.RegisterConnection(mockConn, \"192.168.1.100:12345\")\n\trequire.NoError(t, err)\n\t\n\t// Wait half the timeout period\n\ttime.Sleep(100 * time.Millisecond)\n\t\n\t// Update activity to prevent timeout\n\tmanager.UpdateConnectionActivity(connInfo.ID)\n\t\n\t// Wait another half timeout period\n\ttime.Sleep(100 * time.Millisecond)\n\t\n\t// Connection should still be active due to activity update\n\tmetrics := manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(1), metrics.ActiveConnections)\n\t\n\t// Now wait for full timeout without activity\n\ttime.Sleep(250 * time.Millisecond)\n\t\n\t// Connection should now be cleaned up\n\tmetrics = manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(0), metrics.ActiveConnections)\n\tassert.Equal(t, int64(1), metrics.TimeoutConnections)\n}\n\n// TestMemoryLeakPreventionManager_ConcurrentOperations tests thread safety\nfunc TestMemoryLeakPreventionManager_ConcurrentOperations(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 100,\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        1 * time.Second,\n\t\t\tWebSocketCleanup: 100 * time.Millisecond,\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\tdefer manager.Shutdown(context.Background())\n\t\n\tconst numGoroutines = 10\n\tconst operationsPerGoroutine = 20\n\t\n\tvar wg sync.WaitGroup\n\tconnectionIDs := make(chan string, numGoroutines*operationsPerGoroutine)\n\t\n\t// Concurrent connection registration\n\tfor i := 0; i < numGoroutines; i++ {\n\t\twg.Add(1)\n\t\tgo func(workerID int) {\n\t\t\tdefer wg.Done()\n\t\t\t\n\t\t\tfor j := 0; j < operationsPerGoroutine; j++ {\n\t\t\t\tmockConn := &mockWebSocketConn{}\n\t\t\t\tremoteAddr := fmt.Sprintf(\"192.168.1.%d:%d\", workerID, j)\n\t\t\t\t\n\t\t\t\tconnInfo, err := manager.RegisterConnection(mockConn, remoteAddr)\n\t\t\t\tif err == nil {\n\t\t\t\t\tconnectionIDs <- connInfo.ID\n\t\t\t\t\t\n\t\t\t\t\t// Update activity randomly\n\t\t\t\t\tif j%3 == 0 {\n\t\t\t\t\t\tmanager.UpdateConnectionActivity(connInfo.ID)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}(i)\n\t}\n\t\n\twg.Wait()\n\tclose(connectionIDs)\n\t\n\t// Collect all connection IDs\n\tvar allConnIDs []string\n\tfor connID := range connectionIDs {\n\t\tallConnIDs = append(allConnIDs, connID)\n\t}\n\t\n\t// Verify concurrent operations worked\n\tmetrics := manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(len(allConnIDs)), metrics.ActiveConnections)\n\tassert.Greater(t, metrics.TotalConnections, int64(0))\n\t\n\t// Concurrent unregistration\n\tvar cleanupWg sync.WaitGroup\n\tfor _, connID := range allConnIDs {\n\t\tcleanupWg.Add(1)\n\t\tgo func(id string) {\n\t\t\tdefer cleanupWg.Done()\n\t\t\tmanager.UnregisterConnection(id)\n\t\t}(connID)\n\t}\n\t\n\tcleanupWg.Wait()\n\t\n\t// Verify all connections were cleaned up\n\tmetrics = manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(0), metrics.ActiveConnections)\n\tassert.Equal(t, int64(len(allConnIDs)), metrics.CleanedConnections)\n}\n\n// TestMemoryLeakPreventionManager_ForceCleanup tests manual cleanup functionality\nfunc TestMemoryLeakPreventionManager_ForceCleanup(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 10,\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        50 * time.Millisecond, // Short timeout\n\t\t\tWebSocketCleanup: 10 * time.Second,      // Long cleanup interval\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\tdefer manager.Shutdown(context.Background())\n\t\n\t// Register multiple connections\n\tvar connectionIDs []string\n\tfor i := 0; i < 5; i++ {\n\t\tmockConn := &mockWebSocketConn{}\n\t\tconnInfo, err := manager.RegisterConnection(mockConn, fmt.Sprintf(\"192.168.1.100:1234%d\", i))\n\t\trequire.NoError(t, err)\n\t\tconnectionIDs = append(connectionIDs, connInfo.ID)\n\t}\n\t\n\t// Verify connections are registered\n\tmetrics := manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(5), metrics.ActiveConnections)\n\t\n\t// Wait for connections to become stale\n\ttime.Sleep(100 * time.Millisecond)\n\t\n\t// Force cleanup manually\n\tcleanedCount := manager.ForceCleanupStaleConnections()\n\t\n\t// Verify force cleanup worked\n\tassert.Equal(t, 5, cleanedCount)\n\tmetrics = manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(0), metrics.ActiveConnections)\n\tassert.Equal(t, int64(5), metrics.TimeoutConnections)\n\tassert.Equal(t, int64(1), metrics.ForceCleanups)\n}\n\n// TestMemoryLeakPreventionManager_Shutdown tests graceful shutdown\nfunc TestMemoryLeakPreventionManager_Shutdown(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 10,\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        30 * time.Second,\n\t\t\tWebSocketCleanup: 1 * time.Second,\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\t\n\t// Register connections\n\tvar connectionIDs []string\n\tfor i := 0; i < 3; i++ {\n\t\tmockConn := &mockWebSocketConn{}\n\t\tconnInfo, err := manager.RegisterConnection(mockConn, fmt.Sprintf(\"192.168.1.100:1234%d\", i))\n\t\trequire.NoError(t, err)\n\t\tconnectionIDs = append(connectionIDs, connInfo.ID)\n\t}\n\t\n\t// Verify connections are active\n\tmetrics := manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(3), metrics.ActiveConnections)\n\tassert.True(t, manager.IsHealthy())\n\t\n\t// Shutdown manager\n\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n\tdefer cancel()\n\t\n\terr := manager.Shutdown(ctx)\n\tassert.NoError(t, err)\n\t\n\t// Verify shutdown state\n\tassert.True(t, manager.isShutdown)\n\tassert.False(t, manager.IsHealthy())\n\t\n\t// Verify all connections were cleaned up\n\tmetrics = manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(0), metrics.ActiveConnections)\n\t\n\t// Verify new connections are rejected\n\tmockConn := &mockWebSocketConn{}\n\tconnInfo, err := manager.RegisterConnection(mockConn, \"192.168.1.100:99999\")\n\tassert.Error(t, err)\n\tassert.Nil(t, connInfo)\n\tassert.Contains(t, err.Error(), \"shutting down\")\n}\n\n// TestMemoryLeakPreventionManager_HealthCheck tests health monitoring\nfunc TestMemoryLeakPreventionManager_HealthCheck(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 10,\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        30 * time.Second,\n\t\t\tWebSocketCleanup: 100 * time.Millisecond,\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\tdefer manager.Shutdown(context.Background())\n\t\n\t// Initially healthy\n\tassert.True(t, manager.IsHealthy())\n\t\n\t// Add connections up to near limit (80%)\n\tfor i := 0; i < 8; i++ {\n\t\tmockConn := &mockWebSocketConn{}\n\t\t_, err := manager.RegisterConnection(mockConn, fmt.Sprintf(\"192.168.1.100:1234%d\", i))\n\t\trequire.NoError(t, err)\n\t}\n\t\n\t// Still healthy at 80%\n\tassert.True(t, manager.IsHealthy())\n\t\n\t// Add more connections to exceed 90% threshold\n\tfor i := 8; i < 10; i++ {\n\t\tmockConn := &mockWebSocketConn{}\n\t\t_, err := manager.RegisterConnection(mockConn, fmt.Sprintf(\"192.168.1.100:1234%d\", i))\n\t\trequire.NoError(t, err)\n\t}\n\t\n\t// Now should be unhealthy due to high connection ratio\n\ttime.Sleep(50 * time.Millisecond) // Allow cleanup cycle\n\tisHealthy := manager.IsHealthy()\n\t\n\t// Health check should consider connection ratio\n\tmetrics := manager.GetMemoryMetrics()\n\tconnectionRatio := float64(metrics.ActiveConnections) / float64(10)\n\t\n\tif connectionRatio >= 0.9 {\n\t\tassert.False(t, isHealthy)\n\t} else {\n\t\tassert.True(t, isHealthy)\n\t}\n}\n\n// TestMemoryLeakPreventionManager_MetricsAccuracy tests metrics calculation accuracy\nfunc TestMemoryLeakPreventionManager_MetricsAccuracy(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 10,\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        1 * time.Second,\n\t\t\tWebSocketCleanup: 100 * time.Millisecond,\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\tdefer manager.Shutdown(context.Background())\n\t\n\t// Register connections with known pattern\n\tconnectionCount := 5\n\tfor i := 0; i < connectionCount; i++ {\n\t\tmockConn := &mockWebSocketConn{}\n\t\t_, err := manager.RegisterConnection(mockConn, fmt.Sprintf(\"192.168.1.100:1234%d\", i))\n\t\trequire.NoError(t, err)\n\t}\n\t\n\t// Verify metrics accuracy\n\tmetrics := manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(connectionCount), metrics.ActiveConnections)\n\tassert.Equal(t, int64(connectionCount), metrics.TotalConnections)\n\tassert.Equal(t, int64(connectionCount*2), metrics.ActiveGoroutines) // read + write per connection\n\tassert.Greater(t, metrics.MemoryUsageBytes, int64(0))\n\tassert.Greater(t, metrics.AverageConnectionAge, time.Duration(0))\n\t\n\t// Unregister half the connections\n\tconnections := manager.ListActiveConnections()\n\tfor i := 0; i < len(connections)/2; i++ {\n\t\tmanager.UnregisterConnection(connections[i])\n\t}\n\t\n\t// Verify metrics updated correctly\n\tmetrics = manager.GetMemoryMetrics()\n\texpectedActive := int64(connectionCount - connectionCount/2)\n\tassert.Equal(t, expectedActive, metrics.ActiveConnections)\n\tassert.Equal(t, int64(connectionCount), metrics.TotalConnections) // Total doesn't decrease\n\tassert.Equal(t, int64(connectionCount/2), metrics.CleanedConnections)\n}\n\n// Mock WebSocket connection for testing\ntype mockWebSocketConn struct {\n\tclosed bool\n}\n\nfunc (m *mockWebSocketConn) Close(code websocket.StatusCode, reason string) error {\n\tm.closed = true\n\treturn nil\n}\n\nfunc (m *mockWebSocketConn) Read(ctx context.Context) (websocket.MessageType, []byte, error) {\n\treturn websocket.MessageText, []byte(\"test\"), nil\n}\n\nfunc (m *mockWebSocketConn) Write(ctx context.Context, typ websocket.MessageType, data []byte) error {\n\treturn nil\n}\n\nfunc (m *mockWebSocketConn) Ping(ctx context.Context) error {\n\treturn nil\n}\n\nfunc (m *mockWebSocketConn) SetReadLimit(limit int64) {\n\t// Mock implementation\n}\n\n// TestWebSocketMemoryMetrics_ThreadSafety tests metrics thread safety\nfunc TestWebSocketMemoryMetrics_ThreadSafety(t *testing.T) {\n\tmetrics := &WebSocketMemoryMetrics{}\n\t\n\tconst numGoroutines = 50\n\tconst operationsPerGoroutine = 100\n\t\n\tvar wg sync.WaitGroup\n\t\n\t// Concurrent metric updates\n\tfor i := 0; i < numGoroutines; i++ {\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\t\n\t\t\tfor j := 0; j < operationsPerGoroutine; j++ {\n\t\t\t\t// Simulate metric updates\n\t\t\t\tmetrics.mutex.Lock()\n\t\t\t\tmetrics.ActiveConnections++\n\t\t\t\tmetrics.TotalConnections++\n\t\t\t\tmetrics.CleanedConnections++\n\t\t\t\tmetrics.LastCleanupTime = time.Now()\n\t\t\t\tmetrics.mutex.Unlock()\n\t\t\t}\n\t\t}()\n\t}\n\t\n\twg.Wait()\n\t\n\t// Verify final values\n\texpectedValue := int64(numGoroutines * operationsPerGoroutine)\n\tassert.Equal(t, expectedValue, metrics.ActiveConnections)\n\tassert.Equal(t, expectedValue, metrics.TotalConnections)\n\tassert.Equal(t, expectedValue, metrics.CleanedConnections)\n\tassert.False(t, metrics.LastCleanupTime.IsZero())\n}"
    },
    "tool_response": {
      "type": "create",
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/server/websocket_memory_fix_test.go",
      "content": "package server\n\nimport (\n\t\"context\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/coder/websocket\"\n\t\"github.com/conneroisu/templar/internal/config\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\n// TestMemoryLeakPreventionManager_BasicLifecycle tests basic connection lifecycle\nfunc TestMemoryLeakPreventionManager_BasicLifecycle(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 10,\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        30 * time.Second,\n\t\t\tWebSocketCleanup: 1 * time.Second,\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\tdefer manager.Shutdown(context.Background())\n\t\n\t// Test initial state\n\tmetrics := manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(0), metrics.ActiveConnections)\n\tassert.True(t, manager.IsHealthy())\n\t\n\t// Create mock connection\n\tmockConn := &mockWebSocketConn{}\n\t\n\t// Register connection\n\tconnInfo, err := manager.RegisterConnection(mockConn, \"192.168.1.100:12345\")\n\trequire.NoError(t, err)\n\trequire.NotNil(t, connInfo)\n\tassert.True(t, connInfo.IsActive)\n\tassert.Equal(t, \"192.168.1.100:12345\", connInfo.RemoteAddr)\n\t\n\t// Verify metrics updated\n\tmetrics = manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(1), metrics.ActiveConnections)\n\tassert.Equal(t, int64(1), metrics.TotalConnections)\n\t\n\t// Unregister connection\n\terr = manager.UnregisterConnection(connInfo.ID)\n\tassert.NoError(t, err)\n\t\n\t// Verify cleanup\n\tmetrics = manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(0), metrics.ActiveConnections)\n\tassert.Equal(t, int64(1), metrics.CleanedConnections)\n}\n\n// TestMemoryLeakPreventionManager_ConnectionLimit tests connection limit enforcement\nfunc TestMemoryLeakPreventionManager_ConnectionLimit(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 2, // Low limit for testing\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        30 * time.Second,\n\t\t\tWebSocketCleanup: 1 * time.Second,\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\tdefer manager.Shutdown(context.Background())\n\t\n\tvar connections []*ConnectionInfo\n\t\n\t// Register up to limit\n\tfor i := 0; i < 2; i++ {\n\t\tmockConn := &mockWebSocketConn{}\n\t\tconnInfo, err := manager.RegisterConnection(mockConn, \"192.168.1.100:1234\"+string(rune(i)))\n\t\trequire.NoError(t, err)\n\t\tconnections = append(connections, connInfo)\n\t}\n\t\n\t// Verify limit reached\n\tmetrics := manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(2), metrics.ActiveConnections)\n\t\n\t// Try to exceed limit\n\tmockConn := &mockWebSocketConn{}\n\tconnInfo, err := manager.RegisterConnection(mockConn, \"192.168.1.100:12347\")\n\tassert.Error(t, err)\n\tassert.Nil(t, connInfo)\n\tassert.Contains(t, err.Error(), \"connection limit exceeded\")\n\t\n\t// Verify failed connection metric\n\tmetrics = manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(1), metrics.FailedConnections)\n\t\n\t// Clean up\n\tfor _, conn := range connections {\n\t\tmanager.UnregisterConnection(conn.ID)\n\t}\n}\n\n// TestMemoryLeakPreventionManager_TimeoutCleanup tests automatic timeout cleanup\nfunc TestMemoryLeakPreventionManager_TimeoutCleanup(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 10,\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        100 * time.Millisecond, // Very short timeout for testing\n\t\t\tWebSocketCleanup: 50 * time.Millisecond,  // Frequent cleanup\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\tdefer manager.Shutdown(context.Background())\n\t\n\t// Register connection\n\tmockConn := &mockWebSocketConn{}\n\tconnInfo, err := manager.RegisterConnection(mockConn, \"192.168.1.100:12345\")\n\trequire.NoError(t, err)\n\t\n\t// Verify connection is active\n\tmetrics := manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(1), metrics.ActiveConnections)\n\t\n\t// Wait for timeout and cleanup\n\ttime.Sleep(200 * time.Millisecond)\n\t\n\t// Verify connection was cleaned up automatically\n\tmetrics = manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(0), metrics.ActiveConnections)\n\tassert.Equal(t, int64(1), metrics.TimeoutConnections)\n\tassert.Greater(t, metrics.CleanupOperations, int64(0))\n\t\n\t// Verify connection info is no longer accessible\n\t_, exists := manager.GetConnectionInfo(connInfo.ID)\n\tassert.False(t, exists)\n}\n\n// TestMemoryLeakPreventionManager_ActivityUpdates tests activity tracking\nfunc TestMemoryLeakPreventionManager_ActivityUpdates(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 10,\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        200 * time.Millisecond,\n\t\t\tWebSocketCleanup: 50 * time.Millisecond,\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\tdefer manager.Shutdown(context.Background())\n\t\n\t// Register connection\n\tmockConn := &mockWebSocketConn{}\n\tconnInfo, err := manager.RegisterConnection(mockConn, \"192.168.1.100:12345\")\n\trequire.NoError(t, err)\n\t\n\t// Wait half the timeout period\n\ttime.Sleep(100 * time.Millisecond)\n\t\n\t// Update activity to prevent timeout\n\tmanager.UpdateConnectionActivity(connInfo.ID)\n\t\n\t// Wait another half timeout period\n\ttime.Sleep(100 * time.Millisecond)\n\t\n\t// Connection should still be active due to activity update\n\tmetrics := manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(1), metrics.ActiveConnections)\n\t\n\t// Now wait for full timeout without activity\n\ttime.Sleep(250 * time.Millisecond)\n\t\n\t// Connection should now be cleaned up\n\tmetrics = manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(0), metrics.ActiveConnections)\n\tassert.Equal(t, int64(1), metrics.TimeoutConnections)\n}\n\n// TestMemoryLeakPreventionManager_ConcurrentOperations tests thread safety\nfunc TestMemoryLeakPreventionManager_ConcurrentOperations(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 100,\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        1 * time.Second,\n\t\t\tWebSocketCleanup: 100 * time.Millisecond,\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\tdefer manager.Shutdown(context.Background())\n\t\n\tconst numGoroutines = 10\n\tconst operationsPerGoroutine = 20\n\t\n\tvar wg sync.WaitGroup\n\tconnectionIDs := make(chan string, numGoroutines*operationsPerGoroutine)\n\t\n\t// Concurrent connection registration\n\tfor i := 0; i < numGoroutines; i++ {\n\t\twg.Add(1)\n\t\tgo func(workerID int) {\n\t\t\tdefer wg.Done()\n\t\t\t\n\t\t\tfor j := 0; j < operationsPerGoroutine; j++ {\n\t\t\t\tmockConn := &mockWebSocketConn{}\n\t\t\t\tremoteAddr := fmt.Sprintf(\"192.168.1.%d:%d\", workerID, j)\n\t\t\t\t\n\t\t\t\tconnInfo, err := manager.RegisterConnection(mockConn, remoteAddr)\n\t\t\t\tif err == nil {\n\t\t\t\t\tconnectionIDs <- connInfo.ID\n\t\t\t\t\t\n\t\t\t\t\t// Update activity randomly\n\t\t\t\t\tif j%3 == 0 {\n\t\t\t\t\t\tmanager.UpdateConnectionActivity(connInfo.ID)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}(i)\n\t}\n\t\n\twg.Wait()\n\tclose(connectionIDs)\n\t\n\t// Collect all connection IDs\n\tvar allConnIDs []string\n\tfor connID := range connectionIDs {\n\t\tallConnIDs = append(allConnIDs, connID)\n\t}\n\t\n\t// Verify concurrent operations worked\n\tmetrics := manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(len(allConnIDs)), metrics.ActiveConnections)\n\tassert.Greater(t, metrics.TotalConnections, int64(0))\n\t\n\t// Concurrent unregistration\n\tvar cleanupWg sync.WaitGroup\n\tfor _, connID := range allConnIDs {\n\t\tcleanupWg.Add(1)\n\t\tgo func(id string) {\n\t\t\tdefer cleanupWg.Done()\n\t\t\tmanager.UnregisterConnection(id)\n\t\t}(connID)\n\t}\n\t\n\tcleanupWg.Wait()\n\t\n\t// Verify all connections were cleaned up\n\tmetrics = manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(0), metrics.ActiveConnections)\n\tassert.Equal(t, int64(len(allConnIDs)), metrics.CleanedConnections)\n}\n\n// TestMemoryLeakPreventionManager_ForceCleanup tests manual cleanup functionality\nfunc TestMemoryLeakPreventionManager_ForceCleanup(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 10,\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        50 * time.Millisecond, // Short timeout\n\t\t\tWebSocketCleanup: 10 * time.Second,      // Long cleanup interval\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\tdefer manager.Shutdown(context.Background())\n\t\n\t// Register multiple connections\n\tvar connectionIDs []string\n\tfor i := 0; i < 5; i++ {\n\t\tmockConn := &mockWebSocketConn{}\n\t\tconnInfo, err := manager.RegisterConnection(mockConn, fmt.Sprintf(\"192.168.1.100:1234%d\", i))\n\t\trequire.NoError(t, err)\n\t\tconnectionIDs = append(connectionIDs, connInfo.ID)\n\t}\n\t\n\t// Verify connections are registered\n\tmetrics := manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(5), metrics.ActiveConnections)\n\t\n\t// Wait for connections to become stale\n\ttime.Sleep(100 * time.Millisecond)\n\t\n\t// Force cleanup manually\n\tcleanedCount := manager.ForceCleanupStaleConnections()\n\t\n\t// Verify force cleanup worked\n\tassert.Equal(t, 5, cleanedCount)\n\tmetrics = manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(0), metrics.ActiveConnections)\n\tassert.Equal(t, int64(5), metrics.TimeoutConnections)\n\tassert.Equal(t, int64(1), metrics.ForceCleanups)\n}\n\n// TestMemoryLeakPreventionManager_Shutdown tests graceful shutdown\nfunc TestMemoryLeakPreventionManager_Shutdown(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 10,\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        30 * time.Second,\n\t\t\tWebSocketCleanup: 1 * time.Second,\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\t\n\t// Register connections\n\tvar connectionIDs []string\n\tfor i := 0; i < 3; i++ {\n\t\tmockConn := &mockWebSocketConn{}\n\t\tconnInfo, err := manager.RegisterConnection(mockConn, fmt.Sprintf(\"192.168.1.100:1234%d\", i))\n\t\trequire.NoError(t, err)\n\t\tconnectionIDs = append(connectionIDs, connInfo.ID)\n\t}\n\t\n\t// Verify connections are active\n\tmetrics := manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(3), metrics.ActiveConnections)\n\tassert.True(t, manager.IsHealthy())\n\t\n\t// Shutdown manager\n\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n\tdefer cancel()\n\t\n\terr := manager.Shutdown(ctx)\n\tassert.NoError(t, err)\n\t\n\t// Verify shutdown state\n\tassert.True(t, manager.isShutdown)\n\tassert.False(t, manager.IsHealthy())\n\t\n\t// Verify all connections were cleaned up\n\tmetrics = manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(0), metrics.ActiveConnections)\n\t\n\t// Verify new connections are rejected\n\tmockConn := &mockWebSocketConn{}\n\tconnInfo, err := manager.RegisterConnection(mockConn, \"192.168.1.100:99999\")\n\tassert.Error(t, err)\n\tassert.Nil(t, connInfo)\n\tassert.Contains(t, err.Error(), \"shutting down\")\n}\n\n// TestMemoryLeakPreventionManager_HealthCheck tests health monitoring\nfunc TestMemoryLeakPreventionManager_HealthCheck(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 10,\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        30 * time.Second,\n\t\t\tWebSocketCleanup: 100 * time.Millisecond,\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\tdefer manager.Shutdown(context.Background())\n\t\n\t// Initially healthy\n\tassert.True(t, manager.IsHealthy())\n\t\n\t// Add connections up to near limit (80%)\n\tfor i := 0; i < 8; i++ {\n\t\tmockConn := &mockWebSocketConn{}\n\t\t_, err := manager.RegisterConnection(mockConn, fmt.Sprintf(\"192.168.1.100:1234%d\", i))\n\t\trequire.NoError(t, err)\n\t}\n\t\n\t// Still healthy at 80%\n\tassert.True(t, manager.IsHealthy())\n\t\n\t// Add more connections to exceed 90% threshold\n\tfor i := 8; i < 10; i++ {\n\t\tmockConn := &mockWebSocketConn{}\n\t\t_, err := manager.RegisterConnection(mockConn, fmt.Sprintf(\"192.168.1.100:1234%d\", i))\n\t\trequire.NoError(t, err)\n\t}\n\t\n\t// Now should be unhealthy due to high connection ratio\n\ttime.Sleep(50 * time.Millisecond) // Allow cleanup cycle\n\tisHealthy := manager.IsHealthy()\n\t\n\t// Health check should consider connection ratio\n\tmetrics := manager.GetMemoryMetrics()\n\tconnectionRatio := float64(metrics.ActiveConnections) / float64(10)\n\t\n\tif connectionRatio >= 0.9 {\n\t\tassert.False(t, isHealthy)\n\t} else {\n\t\tassert.True(t, isHealthy)\n\t}\n}\n\n// TestMemoryLeakPreventionManager_MetricsAccuracy tests metrics calculation accuracy\nfunc TestMemoryLeakPreventionManager_MetricsAccuracy(t *testing.T) {\n\tcfg := &config.Config{\n\t\tServer: config.ServerConfig{\n\t\t\tMaxWebSocketConnections: 10,\n\t\t},\n\t\tTimeouts: config.TimeoutConfig{\n\t\t\tWebSocket:        1 * time.Second,\n\t\t\tWebSocketCleanup: 100 * time.Millisecond,\n\t\t},\n\t}\n\t\n\tmanager := NewMemoryLeakPreventionManager(cfg)\n\tdefer manager.Shutdown(context.Background())\n\t\n\t// Register connections with known pattern\n\tconnectionCount := 5\n\tfor i := 0; i < connectionCount; i++ {\n\t\tmockConn := &mockWebSocketConn{}\n\t\t_, err := manager.RegisterConnection(mockConn, fmt.Sprintf(\"192.168.1.100:1234%d\", i))\n\t\trequire.NoError(t, err)\n\t}\n\t\n\t// Verify metrics accuracy\n\tmetrics := manager.GetMemoryMetrics()\n\tassert.Equal(t, int64(connectionCount), metrics.ActiveConnections)\n\tassert.Equal(t, int64(connectionCount), metrics.TotalConnections)\n\tassert.Equal(t, int64(connectionCount*2), metrics.ActiveGoroutines) // read + write per connection\n\tassert.Greater(t, metrics.MemoryUsageBytes, int64(0))\n\tassert.Greater(t, metrics.AverageConnectionAge, time.Duration(0))\n\t\n\t// Unregister half the connections\n\tconnections := manager.ListActiveConnections()\n\tfor i := 0; i < len(connections)/2; i++ {\n\t\tmanager.UnregisterConnection(connections[i])\n\t}\n\t\n\t// Verify metrics updated correctly\n\tmetrics = manager.GetMemoryMetrics()\n\texpectedActive := int64(connectionCount - connectionCount/2)\n\tassert.Equal(t, expectedActive, metrics.ActiveConnections)\n\tassert.Equal(t, int64(connectionCount), metrics.TotalConnections) // Total doesn't decrease\n\tassert.Equal(t, int64(connectionCount/2), metrics.CleanedConnections)\n}\n\n// Mock WebSocket connection for testing\ntype mockWebSocketConn struct {\n\tclosed bool\n}\n\nfunc (m *mockWebSocketConn) Close(code websocket.StatusCode, reason string) error {\n\tm.closed = true\n\treturn nil\n}\n\nfunc (m *mockWebSocketConn) Read(ctx context.Context) (websocket.MessageType, []byte, error) {\n\treturn websocket.MessageText, []byte(\"test\"), nil\n}\n\nfunc (m *mockWebSocketConn) Write(ctx context.Context, typ websocket.MessageType, data []byte) error {\n\treturn nil\n}\n\nfunc (m *mockWebSocketConn) Ping(ctx context.Context) error {\n\treturn nil\n}\n\nfunc (m *mockWebSocketConn) SetReadLimit(limit int64) {\n\t// Mock implementation\n}\n\n// TestWebSocketMemoryMetrics_ThreadSafety tests metrics thread safety\nfunc TestWebSocketMemoryMetrics_ThreadSafety(t *testing.T) {\n\tmetrics := &WebSocketMemoryMetrics{}\n\t\n\tconst numGoroutines = 50\n\tconst operationsPerGoroutine = 100\n\t\n\tvar wg sync.WaitGroup\n\t\n\t// Concurrent metric updates\n\tfor i := 0; i < numGoroutines; i++ {\n\t\twg.Add(1)\n\t\tgo func() {\n\t\t\tdefer wg.Done()\n\t\t\t\n\t\t\tfor j := 0; j < operationsPerGoroutine; j++ {\n\t\t\t\t// Simulate metric updates\n\t\t\t\tmetrics.mutex.Lock()\n\t\t\t\tmetrics.ActiveConnections++\n\t\t\t\tmetrics.TotalConnections++\n\t\t\t\tmetrics.CleanedConnections++\n\t\t\t\tmetrics.LastCleanupTime = time.Now()\n\t\t\t\tmetrics.mutex.Unlock()\n\t\t\t}\n\t\t}()\n\t}\n\t\n\twg.Wait()\n\t\n\t// Verify final values\n\texpectedValue := int64(numGoroutines * operationsPerGoroutine)\n\tassert.Equal(t, expectedValue, metrics.ActiveConnections)\n\tassert.Equal(t, expectedValue, metrics.TotalConnections)\n\tassert.Equal(t, expectedValue, metrics.CleanedConnections)\n\tassert.False(t, metrics.LastCleanupTime.IsZero())\n}",
      "structuredPatch": []
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Glob",
    "tool_input": {
      "pattern": "internal/server/*memory*"
    },
    "tool_response": {
      "filenames": [
        "/home/connerohnesorge/Documents/001Repos/templar/internal/server/websocket_memory_fix.go.disabled",
        "/home/connerohnesorge/Documents/001Repos/templar/internal/server/websocket_memory_fix_test.go"
      ],
      "durationMs": 42,
      "numFiles": 2,
      "truncated": false
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Write",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/hash_benchmark_test.go",
      "content": "package scanner\n\nimport (\n\t\"os\"\n\t\"testing\"\n\t\"time\"\n)\n\n// BenchmarkHashGeneration compares original vs optimized hash generation\nfunc BenchmarkHashGeneration(b *testing.B) {\n\t// Create test data of different sizes\n\ttestSizes := []struct {\n\t\tname string\n\t\tsize int\n\t}{\n\t\t{\"Small_1KB\", 1024},\n\t\t{\"Medium_32KB\", 32 * 1024},\n\t\t{\"Large_256KB\", 256 * 1024},\n\t\t{\"XLarge_1MB\", 1024 * 1024},\n\t\t{\"XXLarge_4MB\", 4 * 1024 * 1024},\n\t}\n\t\n\tfor _, testSize := range testSizes {\n\t\tcontent := make([]byte, testSize.size)\n\t\t// Fill with realistic data (simulate templ file content)\n\t\tfor i := range content {\n\t\t\tcontent[i] = byte(i % 256)\n\t\t}\n\t\t\n\t\t// Create mock file info\n\t\tfileInfo := &mockFileInfo{\n\t\t\tsize:    int64(testSize.size),\n\t\t\tmodTime: time.Now(),\n\t\t}\n\t\t\n\t\t// Test original hash generation\n\t\tb.Run(\"Original_\"+testSize.name, func(b *testing.B) {\n\t\t\tb.ResetTimer()\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t// Original CRC32 hash on full content\n\t\t\t\t_ = strconv.FormatUint(uint64(crc32.Checksum(content, crcTable)), 16)\n\t\t\t}\n\t\t})\n\t\t\n\t\t// Test optimized hash generation\n\t\tscanner := &ComponentScanner{}\n\t\tb.Run(\"Optimized_\"+testSize.name, func(b *testing.B) {\n\t\t\tb.ResetTimer()\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t_, _ = scanner.generateOptimizedHash(content, fileInfo)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// BenchmarkHashStrategies tests different hashing strategies individually\nfunc BenchmarkHashStrategies(b *testing.B) {\n\t// Large file content (1MB)\n\tcontent := make([]byte, 1024*1024)\n\tfor i := range content {\n\t\tcontent[i] = byte(i % 256)\n\t}\n\t\n\tscanner := &ComponentScanner{}\n\tfileInfo := &mockFileInfo{size: int64(len(content)), modTime: time.Now()}\n\t\n\tb.Run(\"FullContent\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\t_ = crc32.Checksum(content, crcTable)\n\t\t}\n\t})\n\t\n\tb.Run(\"SampledHash\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\t_ = scanner.generateSampledHash(content)\n\t\t}\n\t})\n\t\n\tb.Run(\"HierarchicalHash\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\t_ = scanner.generateHierarchicalHash(content, fileInfo)\n\t\t}\n\t})\n}\n\n// mockFileInfo implements os.FileInfo for testing\ntype mockFileInfo struct {\n\tsize    int64\n\tmodTime time.Time\n}\n\nfunc (m *mockFileInfo) Name() string       { return \"test.templ\" }\nfunc (m *mockFileInfo) Size() int64        { return m.size }\nfunc (m *mockFileInfo) Mode() os.FileMode  { return 0644 }\nfunc (m *mockFileInfo) ModTime() time.Time { return m.modTime }\nfunc (m *mockFileInfo) IsDir() bool        { return false }\nfunc (m *mockFileInfo) Sys() interface{}   { return nil }"
    },
    "tool_response": {
      "type": "create",
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/hash_benchmark_test.go",
      "content": "package scanner\n\nimport (\n\t\"os\"\n\t\"testing\"\n\t\"time\"\n)\n\n// BenchmarkHashGeneration compares original vs optimized hash generation\nfunc BenchmarkHashGeneration(b *testing.B) {\n\t// Create test data of different sizes\n\ttestSizes := []struct {\n\t\tname string\n\t\tsize int\n\t}{\n\t\t{\"Small_1KB\", 1024},\n\t\t{\"Medium_32KB\", 32 * 1024},\n\t\t{\"Large_256KB\", 256 * 1024},\n\t\t{\"XLarge_1MB\", 1024 * 1024},\n\t\t{\"XXLarge_4MB\", 4 * 1024 * 1024},\n\t}\n\t\n\tfor _, testSize := range testSizes {\n\t\tcontent := make([]byte, testSize.size)\n\t\t// Fill with realistic data (simulate templ file content)\n\t\tfor i := range content {\n\t\t\tcontent[i] = byte(i % 256)\n\t\t}\n\t\t\n\t\t// Create mock file info\n\t\tfileInfo := &mockFileInfo{\n\t\t\tsize:    int64(testSize.size),\n\t\t\tmodTime: time.Now(),\n\t\t}\n\t\t\n\t\t// Test original hash generation\n\t\tb.Run(\"Original_\"+testSize.name, func(b *testing.B) {\n\t\t\tb.ResetTimer()\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t// Original CRC32 hash on full content\n\t\t\t\t_ = strconv.FormatUint(uint64(crc32.Checksum(content, crcTable)), 16)\n\t\t\t}\n\t\t})\n\t\t\n\t\t// Test optimized hash generation\n\t\tscanner := &ComponentScanner{}\n\t\tb.Run(\"Optimized_\"+testSize.name, func(b *testing.B) {\n\t\t\tb.ResetTimer()\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t_, _ = scanner.generateOptimizedHash(content, fileInfo)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// BenchmarkHashStrategies tests different hashing strategies individually\nfunc BenchmarkHashStrategies(b *testing.B) {\n\t// Large file content (1MB)\n\tcontent := make([]byte, 1024*1024)\n\tfor i := range content {\n\t\tcontent[i] = byte(i % 256)\n\t}\n\t\n\tscanner := &ComponentScanner{}\n\tfileInfo := &mockFileInfo{size: int64(len(content)), modTime: time.Now()}\n\t\n\tb.Run(\"FullContent\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\t_ = crc32.Checksum(content, crcTable)\n\t\t}\n\t})\n\t\n\tb.Run(\"SampledHash\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\t_ = scanner.generateSampledHash(content)\n\t\t}\n\t})\n\t\n\tb.Run(\"HierarchicalHash\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\t_ = scanner.generateHierarchicalHash(content, fileInfo)\n\t\t}\n\t})\n}\n\n// mockFileInfo implements os.FileInfo for testing\ntype mockFileInfo struct {\n\tsize    int64\n\tmodTime time.Time\n}\n\nfunc (m *mockFileInfo) Name() string       { return \"test.templ\" }\nfunc (m *mockFileInfo) Size() int64        { return m.size }\nfunc (m *mockFileInfo) Mode() os.FileMode  { return 0644 }\nfunc (m *mockFileInfo) ModTime() time.Time { return m.modTime }\nfunc (m *mockFileInfo) IsDir() bool        { return false }\nfunc (m *mockFileInfo) Sys() interface{}   { return nil }",
      "structuredPatch": []
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Read",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/server/websocket_memory_fix.go.disabled",
      "limit": 30
    },
    "tool_response": {
      "type": "text",
      "file": {
        "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/server/websocket_memory_fix.go.disabled",
        "content": "package server\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"log\"\n\t\"net/http\"\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/coder/websocket\"\n\t\"github.com/conneroisu/templar/internal/config\"\n)\n\n// MemoryLeakPreventionManager provides comprehensive memory leak prevention for WebSocket connections\n// \n// This manager addresses common WebSocket memory leak sources:\n// 1. Goroutine leaks from improperly terminated read/write pumps\n// 2. Channel leaks from unclosed send channels\n// 3. Connection leaks from abandoned WebSocket connections\n// 4. Client map unbounded growth from stale connections\n// 5. Race conditions in concurrent client management\n//\n// Key Features:\n// - Automatic stale connection cleanup with configurable intervals\n// - Goroutine lifecycle tracking and termination guarantees\n// - Channel resource management with proper cleanup\n// - Connection timeout enforcement with activity monitoring\n// - Resource limit enforcement to prevent unbounded growth",
        "numLines": 30,
        "startLine": 1,
        "totalLines": 502
      }
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Glob",
    "tool_input": {
      "pattern": "internal/server/websocket_memory*"
    },
    "tool_response": {
      "filenames": [
        "/home/connerohnesorge/Documents/001Repos/templar/internal/server/websocket_memory_fix.go.disabled",
        "/home/connerohnesorge/Documents/001Repos/templar/internal/server/websocket_memory_fix_test.go.disabled"
      ],
      "durationMs": 38,
      "numFiles": 2,
      "truncated": false
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Edit",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/hash_benchmark_test.go",
      "old_string": "package scanner\n\nimport (\n\t\"os\"\n\t\"testing\"\n\t\"time\"\n)",
      "new_string": "package scanner\n\nimport (\n\t\"hash/crc32\"\n\t\"os\"\n\t\"strconv\"\n\t\"testing\"\n\t\"time\"\n)"
    },
    "tool_response": {
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/scanner/hash_benchmark_test.go",
      "oldString": "package scanner\n\nimport (\n\t\"os\"\n\t\"testing\"\n\t\"time\"\n)",
      "newString": "package scanner\n\nimport (\n\t\"hash/crc32\"\n\t\"os\"\n\t\"strconv\"\n\t\"testing\"\n\t\"time\"\n)",
      "originalFile": "package scanner\n\nimport (\n\t\"os\"\n\t\"testing\"\n\t\"time\"\n)\n\n// BenchmarkHashGeneration compares original vs optimized hash generation\nfunc BenchmarkHashGeneration(b *testing.B) {\n\t// Create test data of different sizes\n\ttestSizes := []struct {\n\t\tname string\n\t\tsize int\n\t}{\n\t\t{\"Small_1KB\", 1024},\n\t\t{\"Medium_32KB\", 32 * 1024},\n\t\t{\"Large_256KB\", 256 * 1024},\n\t\t{\"XLarge_1MB\", 1024 * 1024},\n\t\t{\"XXLarge_4MB\", 4 * 1024 * 1024},\n\t}\n\t\n\tfor _, testSize := range testSizes {\n\t\tcontent := make([]byte, testSize.size)\n\t\t// Fill with realistic data (simulate templ file content)\n\t\tfor i := range content {\n\t\t\tcontent[i] = byte(i % 256)\n\t\t}\n\t\t\n\t\t// Create mock file info\n\t\tfileInfo := &mockFileInfo{\n\t\t\tsize:    int64(testSize.size),\n\t\t\tmodTime: time.Now(),\n\t\t}\n\t\t\n\t\t// Test original hash generation\n\t\tb.Run(\"Original_\"+testSize.name, func(b *testing.B) {\n\t\t\tb.ResetTimer()\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t// Original CRC32 hash on full content\n\t\t\t\t_ = strconv.FormatUint(uint64(crc32.Checksum(content, crcTable)), 16)\n\t\t\t}\n\t\t})\n\t\t\n\t\t// Test optimized hash generation\n\t\tscanner := &ComponentScanner{}\n\t\tb.Run(\"Optimized_\"+testSize.name, func(b *testing.B) {\n\t\t\tb.ResetTimer()\n\t\t\tfor i := 0; i < b.N; i++ {\n\t\t\t\t_, _ = scanner.generateOptimizedHash(content, fileInfo)\n\t\t\t}\n\t\t})\n\t}\n}\n\n// BenchmarkHashStrategies tests different hashing strategies individually\nfunc BenchmarkHashStrategies(b *testing.B) {\n\t// Large file content (1MB)\n\tcontent := make([]byte, 1024*1024)\n\tfor i := range content {\n\t\tcontent[i] = byte(i % 256)\n\t}\n\t\n\tscanner := &ComponentScanner{}\n\tfileInfo := &mockFileInfo{size: int64(len(content)), modTime: time.Now()}\n\t\n\tb.Run(\"FullContent\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\t_ = crc32.Checksum(content, crcTable)\n\t\t}\n\t})\n\t\n\tb.Run(\"SampledHash\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\t_ = scanner.generateSampledHash(content)\n\t\t}\n\t})\n\t\n\tb.Run(\"HierarchicalHash\", func(b *testing.B) {\n\t\tfor i := 0; i < b.N; i++ {\n\t\t\t_ = scanner.generateHierarchicalHash(content, fileInfo)\n\t\t}\n\t})\n}\n\n// mockFileInfo implements os.FileInfo for testing\ntype mockFileInfo struct {\n\tsize    int64\n\tmodTime time.Time\n}\n\nfunc (m *mockFileInfo) Name() string       { return \"test.templ\" }\nfunc (m *mockFileInfo) Size() int64        { return m.size }\nfunc (m *mockFileInfo) Mode() os.FileMode  { return 0644 }\nfunc (m *mockFileInfo) ModTime() time.Time { return m.modTime }\nfunc (m *mockFileInfo) IsDir() bool        { return false }\nfunc (m *mockFileInfo) Sys() interface{}   { return nil }",
      "structuredPatch": [
        {
          "oldStart": 1,
          "oldLines": 7,
          "newStart": 1,
          "newLines": 9,
          "lines": [
            " package scanner",
            " ",
            " import (",
            "+  \"hash/crc32\"",
            "   \"os\"",
            "+  \"strconv\"",
            "   \"testing\"",
            "   \"time\"",
            " )"
          ]
        }
      ],
      "userModified": false,
      "replaceAll": false
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "go build ./internal/scanner",
      "description": "Test benchmark compilation"
    },
    "tool_response": {
      "stdout": "",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "find internal/server -name \"*memory*\" -type f",
      "description": "Find all memory-related files in server directory"
    },
    "tool_response": {
      "stdout": "internal/server/websocket_memory_fix_test.go.disabled\ninternal/server/websocket_memory_fix.go.disabled",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "ls -la internal/server/websocket*",
      "description": "List all websocket files in server directory"
    },
    "tool_response": {
      "stdout": "-rw-r--r-- 1 connerohnesorge users 22817 Jul 21 14:41 internal/server/websocket_advanced_security_test.go\n-rw-r--r-- 1 connerohnesorge users 12638 Jul 21 14:41 internal/server/websocket_benchmark_test.go\n-rw-r--r-- 1 connerohnesorge users 15415 Jul 21 14:41 internal/server/websocket_enhanced.go\n-rw-r--r-- 1 connerohnesorge users 13702 Jul 21 14:41 internal/server/websocket_enhanced_test.go\n-rw-r--r-- 1 connerohnesorge users  7919 Jul 21 14:41 internal/server/websocket_fuzz_test.go\n-rw-r--r-- 1 connerohnesorge users  6453 Jul 21 14:41 internal/server/websocket.go\n-rw-r--r-- 1 connerohnesorge users 19633 Jul 22 10:01 internal/server/websocket_manager.go\n-rw-r--r-- 1 connerohnesorge users 11579 Jul 22 08:47 internal/server/websocket_manager_test.go\n-rw-r--r-- 1 connerohnesorge users 16810 Jul 22 10:03 internal/server/websocket_memory_fix.go.disabled\n-rw-r--r-- 1 connerohnesorge users 15207 Jul 22 10:04 internal/server/websocket_memory_fix_test.go.disabled\n-rw-r--r-- 1 connerohnesorge users 18640 Jul 21 16:03 internal/server/websocket_optimized.go\n-rw-r--r-- 1 connerohnesorge users  2643 Jul 21 14:13 internal/server/websocket_origin_test.go\n-rw-r--r-- 1 connerohnesorge users 11500 Jul 21 14:41 internal/server/websocket_rate_limit_bypass_test.go\n-rw-r--r-- 1 connerohnesorge users 16032 Jul 21 14:41 internal/server/websocket_security_test.go\n-rw-r--r-- 1 connerohnesorge users  6607 Jul 21 14:42 internal/server/websocket_test.go\n-rw-r--r-- 1 connerohnesorge users  4836 Jul 22 10:02 internal/server/websocket_timeout_test.go",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "go test -bench=BenchmarkHashGeneration -benchmem ./internal/scanner/ | head -20",
      "description": "Run hash generation benchmarks"
    },
    "tool_response": {
      "stdout": "--- FAIL: TestScannerMetricsAccuracy (0.01s)\n    parallel_improvements_test.go:177: Expected 50 files processed, got 100\n    parallel_improvements_test.go:196: First scan metrics: Files=100, Components=99, Cache=0/50, Time=3.795547ms, Memory=18014398509481376KB\n    parallel_improvements_test.go:230: Second scan metrics: Files=100, Components=99, Cache=50/0 (100.0% hit rate), Time=2.043749ms, Memory=415KB\nFAIL\nexit status 1\nFAIL\tgithub.com/conneroisu/templar/internal/scanner\t0.046s\nFAIL",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "ls -la internal/server/ | grep memory",
      "description": "Look specifically for memory files"
    },
    "tool_response": {
      "stdout": "-rw-r--r--  1 connerohnesorge users 16810 Jul 22 10:03 websocket_memory_fix.go.disabled\n-rw-r--r--  1 connerohnesorge users 15207 Jul 22 10:04 websocket_memory_fix_test.go.disabled",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "79175cc1-681b-43ed-91b6-fdc4eb7cdfd7",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/79175cc1-681b-43ed-91b6-fdc4eb7cdfd7.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "go clean -cache && go mod tidy",
      "description": "Clean cache and tidy module to resolve potential build issues"
    },
    "tool_response": {
      "stdout": "",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "b9712494-92dd-4c48-b0b8-4f9096ab071d",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/b9712494-92dd-4c48-b0b8-4f9096ab071d.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Bash",
    "tool_input": {
      "command": "go test -bench=BenchmarkHashGeneration -run=^$ ./internal/scanner/",
      "description": "Run only hash benchmarks without other tests"
    },
    "tool_response": {
      "stdout": "goos: linux\ngoarch: amd64\npkg: github.com/conneroisu/templar/internal/scanner\ncpu: 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz\nBenchmarkHashGeneration/Original_Small_1KB-16         \t17980378\t        64.82 ns/op\nBenchmarkHashGeneration/Optimized_Small_1KB-16        \t 3406338\t       333.2 ns/op\nBenchmarkHashGeneration/Original_Medium_32KB-16       \t 1114566\t      1297 ns/op\nBenchmarkHashGeneration/Optimized_Medium_32KB-16      \t  627352\t      2444 ns/op\nBenchmarkHashGeneration/Original_Large_256KB-16       \t   95737\t     18060 ns/op\nBenchmarkHashGeneration/Optimized_Large_256KB-16      \t  678523\t      1887 ns/op\nBenchmarkHashGeneration/Original_XLarge_1MB-16        \t   31424\t     43797 ns/op\nBenchmarkHashGeneration/Optimized_XLarge_1MB-16       \t  336780\t      3365 ns/op\nBenchmarkHashGeneration/Original_XXLarge_4MB-16       \t    7651\t    147318 ns/op\nBenchmarkHashGeneration/Optimized_XXLarge_4MB-16      \t  405920\t      2478 ns/op\nPASS\nok  \tgithub.com/conneroisu/templar/internal/scanner\t16.636s",
      "stderr": "",
      "interrupted": false,
      "isImage": false
    }
  },
  {
    "session_id": "42e03a9d-fb13-4416-96b7-b2ff5d56bda2",
    "transcript_path": "/home/connerohnesorge/.claude/projects/-home-connerohnesorge-Documents-001Repos-templar/42e03a9d-fb13-4416-96b7-b2ff5d56bda2.jsonl",
    "cwd": "/home/connerohnesorge/Documents/001Repos/templar",
    "hook_event_name": "PostToolUse",
    "tool_name": "Write",
    "tool_input": {
      "file_path": "/home/connerohnesorge/Documents/001Repos/templar/internal/server/websocket_enhanced_manager.go",
      "content": "package server\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"log\"\n\t\"net/http\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/coder/websocket\"\n\t\"github.com/conneroisu/templar/internal/config\"\n)\n\n// EnhancedWebSocketManager combines the original WebSocketManager with comprehensive memory leak prevention\n// \n// This manager provides enterprise-grade WebSocket connection management with:\n// - Comprehensive memory leak prevention and detection\n// - Automatic stale connection cleanup with configurable intervals\n// - Resource limit enforcement and connection pooling\n// - Advanced health monitoring and metrics collection\n// - Graceful shutdown with connection draining\n// - Thread-safe operations with minimal lock contention\n//\n// Memory Leak Prevention Features:\n// - Automatic goroutine lifecycle management and termination\n// - Channel resource tracking and proper cleanup\n// - Connection timeout enforcement with activity monitoring\n// - Periodic cleanup of stale and abandoned connections\n// - Resource limit enforcement to prevent unbounded growth\n// - Comprehensive metrics for leak detection and monitoring\ntype EnhancedWebSocketManager struct {\n\t// Original WebSocket functionality\n\t*WebSocketManager\n\t\n\t// Memory leak prevention system\n\tmemoryManager *MemoryLeakPreventionManager\n\t\n\t// Enhanced connection tracking\n\tconnections     map[string]*EnhancedClientInfo   // Connection registry with enhanced tracking\n\tconnectionsMux  sync.RWMutex                     // Protects enhanced connections map\n\t\n\t// Lifecycle management\n\tshutdownOnce    sync.Once                        // Ensures single shutdown\n\tisShutdown      bool                             // Shutdown state indicator\n\t\n\t// Enhanced metrics\n\tenhancedMetrics *EnhancedWebSocketMetrics        // Advanced metrics collection\n}\n\n// EnhancedClientInfo extends ConnectionInfo with additional WebSocket-specific data\ntype EnhancedClientInfo struct {\n\t*ConnectionInfo                                  // Base connection information\n\t\n\t// WebSocket specific data\n\tMessageCount      int64                         // Total messages processed\n\tBytesSent         int64                         // Total bytes sent to client\n\tBytesReceived     int64                         // Total bytes received from client\n\tLastMessageTime   time.Time                     // Timestamp of last message\n\t\n\t// Error tracking\n\tErrorCount        int32                         // Number of errors encountered\n\tLastError         error                         // Last error encountered\n\tLastErrorTime     time.Time                     // Timestamp of last error\n\t\n\t// Performance metrics\n\tAverageLatency    time.Duration                 // Average message processing latency\n\tPingLatency       time.Duration                 // WebSocket ping latency\n\t\n\t// Resource usage\n\tMemoryUsage       int64                         // Estimated memory usage for this client\n\t\n\tmutex             sync.RWMutex                  // Protects client info updates\n}\n\n// EnhancedWebSocketMetrics provides comprehensive WebSocket performance and health metrics\ntype EnhancedWebSocketMetrics struct {\n\t// Base metrics from memory manager\n\t*WebSocketMemoryMetrics\n\t\n\t// Message processing metrics\n\tTotalMessages         int64                     // Total messages processed\n\tMessagesPerSecond     float64                   // Current message processing rate\n\tAverageMessageSize    int64                     // Average message size in bytes\n\t\n\t// Performance metrics\n\tAverageLatency        time.Duration             // Average message processing latency\n\tP95Latency           time.Duration             // 95th percentile latency\n\tP99Latency           time.Duration             // 99th percentile latency\n\t\n\t// Health indicators\n\tHealthScore          float64                   // Overall health score (0-100)\n\tResourceUtilization  float64                   // Resource utilization percentage\n\tErrorRate           float64                   // Error rate percentage\n\t\n\t// Capacity metrics\n\tConnectionCapacity   float64                   // Connection capacity utilization\n\tThroughputCapacity   float64                   // Throughput capacity utilization\n\t\n\tmutex               sync.RWMutex              // Protects metrics updates\n}\n\n// NewEnhancedWebSocketManager creates a comprehensive WebSocket manager with memory leak prevention\nfunc NewEnhancedWebSocketManager(\n\toriginValidator OriginValidator,\n\trateLimiter *TokenBucketManager,\n\tcfg ...*config.Config,\n) *EnhancedWebSocketManager {\n\t// Use first config if provided\n\tvar config *config.Config\n\tif len(cfg) > 0 {\n\t\tconfig = cfg[0]\n\t}\n\t\n\t// Create base WebSocket manager\n\tbaseManager := NewWebSocketManager(originValidator, rateLimiter, config)\n\t\n\t// Create memory leak prevention manager\n\tmemoryManager := NewMemoryLeakPreventionManager(config)\n\t\n\t// Initialize enhanced manager\n\tenhanced := &EnhancedWebSocketManager{\n\t\tWebSocketManager: baseManager,\n\t\tmemoryManager:   memoryManager,\n\t\tconnections:     make(map[string]*EnhancedClientInfo),\n\t\tenhancedMetrics: &EnhancedWebSocketMetrics{\n\t\t\tWebSocketMemoryMetrics: &WebSocketMemoryMetrics{},\n\t\t},\n\t\tisShutdown: false,\n\t}\n\t\n\t// Start enhanced monitoring\n\tgo enhanced.startEnhancedMonitoring()\n\t\n\treturn enhanced\n}\n\n// HandleWebSocket handles WebSocket connections with comprehensive memory leak prevention\nfunc (em *EnhancedWebSocketManager) HandleWebSocket(w http.ResponseWriter, r *http.Request) {\n\t// Check if manager is shut down\n\tif em.isShutdown {\n\t\thttp.Error(w, \"Service Unavailable\", http.StatusServiceUnavailable)\n\t\treturn\n\t}\n\t\n\t// Use base manager's security validation\n\tem.WebSocketManager.HandleWebSocket(w, r)\n}\n\n// Enhanced connection registration that integrates with memory management\nfunc (em *EnhancedWebSocketManager) registerEnhancedConnection(\n\tconn *websocket.Conn,\n\tremoteAddr string,\n) (*EnhancedClientInfo, error) {\n\t// Register with memory manager for leak prevention\n\tconnInfo, err := em.memoryManager.RegisterConnection(conn, remoteAddr)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"memory manager registration failed: %w\", err)\n\t}\n\t\n\t// Create enhanced client info\n\tenhanced := &EnhancedClientInfo{\n\t\tConnectionInfo:    connInfo,\n\t\tMessageCount:      0,\n\t\tBytesSent:        0,\n\t\tBytesReceived:    0,\n\t\tLastMessageTime:  time.Now(),\n\t\tErrorCount:       0,\n\t\tLastError:        nil,\n\t\tAverageLatency:   0,\n\t\tPingLatency:      0,\n\t\tMemoryUsage:      estimateClientMemoryUsage(),\n\t}\n\t\n\t// Register in enhanced tracking\n\tem.connectionsMux.Lock()\n\tem.connections[connInfo.ID] = enhanced\n\tem.connectionsMux.Unlock()\n\t\n\t// Start enhanced client monitoring\n\tgo em.monitorEnhancedClient(enhanced)\n\t\n\treturn enhanced, nil\n}\n\n// unregisterEnhancedConnection cleans up enhanced connection tracking\nfunc (em *EnhancedWebSocketManager) unregisterEnhancedConnection(connID string) error {\n\t// Remove from enhanced tracking\n\tem.connectionsMux.Lock()\n\tenhanced, exists := em.connections[connID]\n\tif exists {\n\t\tdelete(em.connections, connID)\n\t}\n\tem.connectionsMux.Unlock()\n\t\n\t// Unregister from memory manager\n\tif exists {\n\t\tif err := em.memoryManager.UnregisterConnection(connID); err != nil {\n\t\t\tlog.Printf(\"Memory manager unregistration failed for %s: %v\", connID, err)\n\t\t}\n\t\t\n\t\t// Clean up enhanced client resources\n\t\tem.cleanupEnhancedClient(enhanced)\n\t}\n\t\n\treturn nil\n}\n\n// monitorEnhancedClient provides advanced monitoring for individual clients\nfunc (em *EnhancedWebSocketManager) monitorEnhancedClient(client *EnhancedClientInfo) {\n\tticker := time.NewTicker(30 * time.Second) // Monitor every 30 seconds\n\tdefer ticker.Stop()\n\t\n\tfor {\n\t\tselect {\n\t\tcase <-client.Context.Done():\n\t\t\treturn\n\t\t\t\n\t\tcase <-ticker.C:\n\t\t\tem.updateClientMetrics(client)\n\t\t\t\n\t\tcase <-em.ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// updateClientMetrics updates performance metrics for a client\nfunc (em *EnhancedWebSocketManager) updateClientMetrics(client *EnhancedClientInfo) {\n\tclient.mutex.Lock()\n\tdefer client.mutex.Unlock()\n\t\n\t// Update activity in memory manager\n\tem.memoryManager.UpdateConnectionActivity(client.ID)\n\t\n\t// Calculate performance metrics\n\tnow := time.Now()\n\tconnectionAge := now.Sub(client.CreatedAt)\n\ttimeSinceLastMessage := now.Sub(client.LastMessageTime)\n\t\n\t// Update ping latency by sending ping and measuring response time\n\tstart := time.Now()\n\tctx, cancel := context.WithTimeout(client.Context, 5*time.Second)\n\tif err := client.Conn.Ping(ctx); err == nil {\n\t\tclient.PingLatency = time.Since(start)\n\t} else {\n\t\tclient.ErrorCount++\n\t\tclient.LastError = err\n\t\tclient.LastErrorTime = now\n\t}\n\tcancel()\n\t\n\t// Log performance information periodically\n\tif connectionAge.Minutes() > 1 && int(connectionAge.Minutes())%5 == 0 {\n\t\tlog.Printf(\"Client %s metrics: %d messages, %.2f KB sent, %.2f KB received, %v ping latency, %d errors\",\n\t\t\tclient.ID[:8],\n\t\t\tclient.MessageCount,\n\t\t\tfloat64(client.BytesSent)/1024,\n\t\t\tfloat64(client.BytesReceived)/1024,\n\t\t\tclient.PingLatency,\n\t\t\tclient.ErrorCount,\n\t\t)\n\t}\n}\n\n// cleanupEnhancedClient performs comprehensive cleanup for enhanced clients\nfunc (em *EnhancedWebSocketManager) cleanupEnhancedClient(client *EnhancedClientInfo) {\n\tif client == nil {\n\t\treturn\n\t}\n\t\n\tclient.mutex.Lock()\n\tdefer client.mutex.Unlock()\n\t\n\t// Mark as inactive\n\tclient.IsActive = false\n\t\n\t// Log final statistics\n\tconnectionDuration := time.Since(client.CreatedAt)\n\tlog.Printf(\"Enhanced client cleanup for %s: duration=%v, messages=%d, bytes_sent=%d, bytes_received=%d, errors=%d\",\n\t\tclient.ID[:8],\n\t\tconnectionDuration,\n\t\tclient.MessageCount,\n\t\tclient.BytesSent,\n\t\tclient.BytesReceived,\n\t\tclient.ErrorCount,\n\t)\n}\n\n// startEnhancedMonitoring starts advanced monitoring and metrics collection\nfunc (em *EnhancedWebSocketManager) startEnhancedMonitoring() {\n\tticker := time.NewTicker(1 * time.Minute) // Update metrics every minute\n\tdefer ticker.Stop()\n\t\n\tfor {\n\t\tselect {\n\t\tcase <-em.ctx.Done():\n\t\t\treturn\n\t\t\t\n\t\tcase <-ticker.C:\n\t\t\tem.updateEnhancedMetrics()\n\t\t}\n\t}\n}\n\n// updateEnhancedMetrics calculates and updates comprehensive metrics\nfunc (em *EnhancedWebSocketManager) updateEnhancedMetrics() {\n\t// Get base memory metrics\n\tmemoryMetrics := em.memoryManager.GetMemoryMetrics()\n\t\n\t// Calculate enhanced metrics\n\tem.connectionsMux.RLock()\n\tvar totalMessages, totalBytesSent, totalBytesReceived int64\n\tvar totalLatency time.Duration\n\tvar errorCount int32\n\tconnectionCount := len(em.connections)\n\t\n\tfor _, client := range em.connections {\n\t\tclient.mutex.RLock()\n\t\ttotalMessages += client.MessageCount\n\t\ttotalBytesSent += client.BytesSent\n\t\ttotalBytesReceived += client.BytesReceived\n\t\ttotalLatency += client.AverageLatency\n\t\terrorCount += client.ErrorCount\n\t\tclient.mutex.RUnlock()\n\t}\n\tem.connectionsMux.RUnlock()\n\t\n\t// Calculate rates and averages\n\tvar averageLatency time.Duration\n\tvar messagesPerSecond, averageMessageSize, errorRate float64\n\t\n\tif connectionCount > 0 {\n\t\taverageLatency = totalLatency / time.Duration(connectionCount)\n\t\tif totalMessages > 0 {\n\t\t\taverageMessageSize = (totalBytesSent + totalBytesReceived) / totalMessages\n\t\t\terrorRate = float64(errorCount) / float64(totalMessages) * 100\n\t\t}\n\t}\n\t\n\t// Calculate health score (0-100)\n\thealthScore := em.calculateHealthScore(&memoryMetrics, errorRate)\n\t\n\t// Calculate capacity utilization\n\tmaxConnections := em.memoryManager.maxConnections\n\tconnectionCapacity := float64(memoryMetrics.ActiveConnections) / float64(maxConnections) * 100\n\t\n\t// Update enhanced metrics\n\tem.enhancedMetrics.mutex.Lock()\n\tem.enhancedMetrics.WebSocketMemoryMetrics = &memoryMetrics\n\tem.enhancedMetrics.TotalMessages = totalMessages\n\tem.enhancedMetrics.MessagesPerSecond = messagesPerSecond\n\tem.enhancedMetrics.AverageMessageSize = averageMessageSize\n\tem.enhancedMetrics.AverageLatency = averageLatency\n\tem.enhancedMetrics.HealthScore = healthScore\n\tem.enhancedMetrics.ErrorRate = errorRate\n\tem.enhancedMetrics.ConnectionCapacity = connectionCapacity\n\tem.enhancedMetrics.mutex.Unlock()\n\t\n\t// Log health summary periodically\n\tif int(time.Now().Unix())%300 == 0 { // Every 5 minutes\n\t\tlog.Printf(\"WebSocket Health Summary: %.1f%% health, %d active connections (%.1f%% capacity), %.2f%% error rate\",\n\t\t\thealthScore, memoryMetrics.ActiveConnections, connectionCapacity, errorRate)\n\t}\n}\n\n// calculateHealthScore computes overall system health score\nfunc (em *EnhancedWebSocketManager) calculateHealthScore(\n\tmemoryMetrics *WebSocketMemoryMetrics,\n\terrorRate float64,\n) float64 {\n\tscore := 100.0\n\t\n\t// Deduct for high connection utilization\n\tmaxConnections := float64(em.memoryManager.maxConnections)\n\tconnectionRatio := float64(memoryMetrics.ActiveConnections) / maxConnections\n\tif connectionRatio > 0.8 {\n\t\tscore -= (connectionRatio - 0.8) * 100 // Deduct up to 20 points\n\t}\n\t\n\t// Deduct for high error rate\n\tif errorRate > 1.0 {\n\t\tscore -= errorRate * 2 // Deduct 2 points per percent error rate\n\t}\n\t\n\t// Deduct for memory issues\n\tif memoryMetrics.MemoryUsageBytes > 100*1024*1024 { // 100MB threshold\n\t\tmemoryRatio := float64(memoryMetrics.MemoryUsageBytes) / (100 * 1024 * 1024)\n\t\tscore -= (memoryRatio - 1.0) * 10 // Deduct points for excess memory\n\t}\n\t\n\t// Deduct for failed connections\n\tif memoryMetrics.TotalConnections > 0 {\n\t\tfailureRate := float64(memoryMetrics.FailedConnections) / float64(memoryMetrics.TotalConnections)\n\t\tif failureRate > 0.05 { // More than 5% failure rate\n\t\t\tscore -= failureRate * 50\n\t\t}\n\t}\n\t\n\t// Ensure score stays within bounds\n\tif score < 0 {\n\t\tscore = 0\n\t}\n\tif score > 100 {\n\t\tscore = 100\n\t}\n\t\n\treturn score\n}\n\n// GetEnhancedMetrics returns comprehensive WebSocket metrics\nfunc (em *EnhancedWebSocketManager) GetEnhancedMetrics() EnhancedWebSocketMetrics {\n\tem.enhancedMetrics.mutex.RLock()\n\tdefer em.enhancedMetrics.mutex.RUnlock()\n\t\n\t// Create copy of metrics\n\treturn EnhancedWebSocketMetrics{\n\t\tWebSocketMemoryMetrics: em.enhancedMetrics.WebSocketMemoryMetrics,\n\t\tTotalMessages:         em.enhancedMetrics.TotalMessages,\n\t\tMessagesPerSecond:     em.enhancedMetrics.MessagesPerSecond,\n\t\tAverageMessageSize:    em.enhancedMetrics.AverageMessageSize,\n\t\tAverageLatency:        em.enhancedMetrics.AverageLatency,\n\t\tP95Latency:           em.enhancedMetrics.P95Latency,\n\t\tP99Latency:           em.enhancedMetrics.P99Latency,\n\t\tHealthScore:          em.enhancedMetrics.HealthScore,\n\t\tResourceUtilization:   em.enhancedMetrics.ResourceUtilization,\n\t\tErrorRate:            em.enhancedMetrics.ErrorRate,\n\t\tConnectionCapacity:    em.enhancedMetrics.ConnectionCapacity,\n\t\tThroughputCapacity:    em.enhancedMetrics.ThroughputCapacity,\n\t}\n}\n\n// GetMemoryManager returns the memory leak prevention manager for direct access\nfunc (em *EnhancedWebSocketManager) GetMemoryManager() *MemoryLeakPreventionManager {\n\treturn em.memoryManager\n}\n\n// IsHealthy returns whether the enhanced manager is healthy\nfunc (em *EnhancedWebSocketManager) IsHealthy() bool {\n\tif em.isShutdown {\n\t\treturn false\n\t}\n\t\n\t// Check base manager health\n\tif !em.WebSocketManager.IsShutdown() {\n\t\t// Check memory manager health\n\t\tif !em.memoryManager.IsHealthy() {\n\t\t\treturn false\n\t\t}\n\t\t\n\t\t// Check enhanced metrics\n\t\tmetrics := em.GetEnhancedMetrics()\n\t\treturn metrics.HealthScore > 70.0 && metrics.ErrorRate < 5.0\n\t}\n\t\n\treturn false\n}\n\n// Shutdown gracefully shuts down the enhanced WebSocket manager\nfunc (em *EnhancedWebSocketManager) Shutdown(ctx context.Context) error {\n\tvar shutdownErr error\n\t\n\tem.shutdownOnce.Do(func() {\n\t\tem.isShutdown = true\n\t\t\n\t\tlog.Printf(\"Shutting down enhanced WebSocket manager...\")\n\t\t\n\t\t// Shutdown memory manager first\n\t\tif err := em.memoryManager.Shutdown(ctx); err != nil {\n\t\t\tlog.Printf(\"Memory manager shutdown error: %v\", err)\n\t\t\tshutdownErr = err\n\t\t}\n\t\t\n\t\t// Shutdown base WebSocket manager\n\t\tif err := em.WebSocketManager.Shutdown(ctx); err != nil {\n\t\t\tlog.Printf(\"Base WebSocket manager shutdown error: %v\", err)\n\t\t\tif shutdownErr == nil {\n\t\t\t\tshutdownErr = err\n\t\t\t}\n\t\t}\n\t\t\n\t\t// Clean up enhanced connections\n\t\tem.connectionsMux.Lock()\n\t\tconnectionCount := len(em.connections)\n\t\tfor connID, client := range em.connections {\n\t\t\tem.cleanupEnhancedClient(client)\n\t\t\tdelete(em.connections, connID)\n\t\t}\n\t\tem.connectionsMux.Unlock()\n\t\t\n\t\tlog.Printf(\"Enhanced WebSocket manager shutdown completed. Cleaned up %d connections.\", connectionCount)\n\t})\n\t\n\treturn shutdownErr\n}\n\n// ForceMemoryCleanup immediately cleans up stale connections and resources\nfunc (em *EnhancedWebSocketManager) ForceMemoryCleanup() int {\n\treturn em.memoryManager.ForceCleanupStaleConnections()\n}\n\n// Helper function to estimate client memory usage\nfunc estimateClientMemoryUsage() int64 {\n\t// Rough estimation of memory usage per client\n\t// Includes: connection struct, channels, buffers, metrics\n\treturn 8192 // 8KB per client estimate\n}"
    },
    "tool_response": {
      "type": "create",
      "filePath": "/home/connerohnesorge/Documents/001Repos/templar/internal/server/websocket_enhanced_manager.go",
      "content": "package server\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"log\"\n\t\"net/http\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/coder/websocket\"\n\t\"github.com/conneroisu/templar/internal/config\"\n)\n\n// EnhancedWebSocketManager combines the original WebSocketManager with comprehensive memory leak prevention\n// \n// This manager provides enterprise-grade WebSocket connection management with:\n// - Comprehensive memory leak prevention and detection\n// - Automatic stale connection cleanup with configurable intervals\n// - Resource limit enforcement and connection pooling\n// - Advanced health monitoring and metrics collection\n// - Graceful shutdown with connection draining\n// - Thread-safe operations with minimal lock contention\n//\n// Memory Leak Prevention Features:\n// - Automatic goroutine lifecycle management and termination\n// - Channel resource tracking and proper cleanup\n// - Connection timeout enforcement with activity monitoring\n// - Periodic cleanup of stale and abandoned connections\n// - Resource limit enforcement to prevent unbounded growth\n// - Comprehensive metrics for leak detection and monitoring\ntype EnhancedWebSocketManager struct {\n\t// Original WebSocket functionality\n\t*WebSocketManager\n\t\n\t// Memory leak prevention system\n\tmemoryManager *MemoryLeakPreventionManager\n\t\n\t// Enhanced connection tracking\n\tconnections     map[string]*EnhancedClientInfo   // Connection registry with enhanced tracking\n\tconnectionsMux  sync.RWMutex                     // Protects enhanced connections map\n\t\n\t// Lifecycle management\n\tshutdownOnce    sync.Once                        // Ensures single shutdown\n\tisShutdown      bool                             // Shutdown state indicator\n\t\n\t// Enhanced metrics\n\tenhancedMetrics *EnhancedWebSocketMetrics        // Advanced metrics collection\n}\n\n// EnhancedClientInfo extends ConnectionInfo with additional WebSocket-specific data\ntype EnhancedClientInfo struct {\n\t*ConnectionInfo                                  // Base connection information\n\t\n\t// WebSocket specific data\n\tMessageCount      int64                         // Total messages processed\n\tBytesSent         int64                         // Total bytes sent to client\n\tBytesReceived     int64                         // Total bytes received from client\n\tLastMessageTime   time.Time                     // Timestamp of last message\n\t\n\t// Error tracking\n\tErrorCount        int32                         // Number of errors encountered\n\tLastError         error                         // Last error encountered\n\tLastErrorTime     time.Time                     // Timestamp of last error\n\t\n\t// Performance metrics\n\tAverageLatency    time.Duration                 // Average message processing latency\n\tPingLatency       time.Duration                 // WebSocket ping latency\n\t\n\t// Resource usage\n\tMemoryUsage       int64                         // Estimated memory usage for this client\n\t\n\tmutex             sync.RWMutex                  // Protects client info updates\n}\n\n// EnhancedWebSocketMetrics provides comprehensive WebSocket performance and health metrics\ntype EnhancedWebSocketMetrics struct {\n\t// Base metrics from memory manager\n\t*WebSocketMemoryMetrics\n\t\n\t// Message processing metrics\n\tTotalMessages         int64                     // Total messages processed\n\tMessagesPerSecond     float64                   // Current message processing rate\n\tAverageMessageSize    int64                     // Average message size in bytes\n\t\n\t// Performance metrics\n\tAverageLatency        time.Duration             // Average message processing latency\n\tP95Latency           time.Duration             // 95th percentile latency\n\tP99Latency           time.Duration             // 99th percentile latency\n\t\n\t// Health indicators\n\tHealthScore          float64                   // Overall health score (0-100)\n\tResourceUtilization  float64                   // Resource utilization percentage\n\tErrorRate           float64                   // Error rate percentage\n\t\n\t// Capacity metrics\n\tConnectionCapacity   float64                   // Connection capacity utilization\n\tThroughputCapacity   float64                   // Throughput capacity utilization\n\t\n\tmutex               sync.RWMutex              // Protects metrics updates\n}\n\n// NewEnhancedWebSocketManager creates a comprehensive WebSocket manager with memory leak prevention\nfunc NewEnhancedWebSocketManager(\n\toriginValidator OriginValidator,\n\trateLimiter *TokenBucketManager,\n\tcfg ...*config.Config,\n) *EnhancedWebSocketManager {\n\t// Use first config if provided\n\tvar config *config.Config\n\tif len(cfg) > 0 {\n\t\tconfig = cfg[0]\n\t}\n\t\n\t// Create base WebSocket manager\n\tbaseManager := NewWebSocketManager(originValidator, rateLimiter, config)\n\t\n\t// Create memory leak prevention manager\n\tmemoryManager := NewMemoryLeakPreventionManager(config)\n\t\n\t// Initialize enhanced manager\n\tenhanced := &EnhancedWebSocketManager{\n\t\tWebSocketManager: baseManager,\n\t\tmemoryManager:   memoryManager,\n\t\tconnections:     make(map[string]*EnhancedClientInfo),\n\t\tenhancedMetrics: &EnhancedWebSocketMetrics{\n\t\t\tWebSocketMemoryMetrics: &WebSocketMemoryMetrics{},\n\t\t},\n\t\tisShutdown: false,\n\t}\n\t\n\t// Start enhanced monitoring\n\tgo enhanced.startEnhancedMonitoring()\n\t\n\treturn enhanced\n}\n\n// HandleWebSocket handles WebSocket connections with comprehensive memory leak prevention\nfunc (em *EnhancedWebSocketManager) HandleWebSocket(w http.ResponseWriter, r *http.Request) {\n\t// Check if manager is shut down\n\tif em.isShutdown {\n\t\thttp.Error(w, \"Service Unavailable\", http.StatusServiceUnavailable)\n\t\treturn\n\t}\n\t\n\t// Use base manager's security validation\n\tem.WebSocketManager.HandleWebSocket(w, r)\n}\n\n// Enhanced connection registration that integrates with memory management\nfunc (em *EnhancedWebSocketManager) registerEnhancedConnection(\n\tconn *websocket.Conn,\n\tremoteAddr string,\n) (*EnhancedClientInfo, error) {\n\t// Register with memory manager for leak prevention\n\tconnInfo, err := em.memoryManager.RegisterConnection(conn, remoteAddr)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"memory manager registration failed: %w\", err)\n\t}\n\t\n\t// Create enhanced client info\n\tenhanced := &EnhancedClientInfo{\n\t\tConnectionInfo:    connInfo,\n\t\tMessageCount:      0,\n\t\tBytesSent:        0,\n\t\tBytesReceived:    0,\n\t\tLastMessageTime:  time.Now(),\n\t\tErrorCount:       0,\n\t\tLastError:        nil,\n\t\tAverageLatency:   0,\n\t\tPingLatency:      0,\n\t\tMemoryUsage:      estimateClientMemoryUsage(),\n\t}\n\t\n\t// Register in enhanced tracking\n\tem.connectionsMux.Lock()\n\tem.connections[connInfo.ID] = enhanced\n\tem.connectionsMux.Unlock()\n\t\n\t// Start enhanced client monitoring\n\tgo em.monitorEnhancedClient(enhanced)\n\t\n\treturn enhanced, nil\n}\n\n// unregisterEnhancedConnection cleans up enhanced connection tracking\nfunc (em *EnhancedWebSocketManager) unregisterEnhancedConnection(connID string) error {\n\t// Remove from enhanced tracking\n\tem.connectionsMux.Lock()\n\tenhanced, exists := em.connections[connID]\n\tif exists {\n\t\tdelete(em.connections, connID)\n\t}\n\tem.connectionsMux.Unlock()\n\t\n\t// Unregister from memory manager\n\tif exists {\n\t\tif err := em.memoryManager.UnregisterConnection(connID); err != nil {\n\t\t\tlog.Printf(\"Memory manager unregistration failed for %s: %v\", connID, err)\n\t\t}\n\t\t\n\t\t// Clean up enhanced client resources\n\t\tem.cleanupEnhancedClient(enhanced)\n\t}\n\t\n\treturn nil\n}\n\n// monitorEnhancedClient provides advanced monitoring for individual clients\nfunc (em *EnhancedWebSocketManager) monitorEnhancedClient(client *EnhancedClientInfo) {\n\tticker := time.NewTicker(30 * time.Second) // Monitor every 30 seconds\n\tdefer ticker.Stop()\n\t\n\tfor {\n\t\tselect {\n\t\tcase <-client.Context.Done():\n\t\t\treturn\n\t\t\t\n\t\tcase <-ticker.C:\n\t\t\tem.updateClientMetrics(client)\n\t\t\t\n\t\tcase <-em.ctx.Done():\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// updateClientMetrics updates performance metrics for a client\nfunc (em *EnhancedWebSocketManager) updateClientMetrics(client *EnhancedClientInfo) {\n\tclient.mutex.Lock()\n\tdefer client.mutex.Unlock()\n\t\n\t// Update activity in memory manager\n\tem.memoryManager.UpdateConnectionActivity(client.ID)\n\t\n\t// Calculate performance metrics\n\tnow := time.Now()\n\tconnectionAge := now.Sub(client.CreatedAt)\n\ttimeSinceLastMessage := now.Sub(client.LastMessageTime)\n\t\n\t// Update ping latency by sending ping and measuring response time\n\tstart := time.Now()\n\tctx, cancel := context.WithTimeout(client.Context, 5*time.Second)\n\tif err := client.Conn.Ping(ctx); err == nil {\n\t\tclient.PingLatency = time.Since(start)\n\t} else {\n\t\tclient.ErrorCount++\n\t\tclient.LastError = err\n\t\tclient.LastErrorTime = now\n\t}\n\tcancel()\n\t\n\t// Log performance information periodically\n\tif connectionAge.Minutes() > 1 && int(connectionAge.Minutes())%5 == 0 {\n\t\tlog.Printf(\"Client %s metrics: %d messages, %.2f KB sent, %.2f KB received, %v ping latency, %d errors\",\n\t\t\tclient.ID[:8],\n\t\t\tclient.MessageCount,\n\t\t\tfloat64(client.BytesSent)/1024,\n\t\t\tfloat64(client.BytesReceived)/1024,\n\t\t\tclient.PingLatency,\n\t\t\tclient.ErrorCount,\n\t\t)\n\t}\n}\n\n// cleanupEnhancedClient performs comprehensive cleanup for enhanced clients\nfunc (em *EnhancedWebSocketManager) cleanupEnhancedClient(client *EnhancedClientInfo) {\n\tif client == nil {\n\t\treturn\n\t}\n\t\n\tclient.mutex.Lock()\n\tdefer client.mutex.Unlock()\n\t\n\t// Mark as inactive\n\tclient.IsActive = false\n\t\n\t// Log final statistics\n\tconnectionDuration := time.Since(client.CreatedAt)\n\tlog.Printf(\"Enhanced client cleanup for %s: duration=%v, messages=%d, bytes_sent=%d, bytes_received=%d, errors=%d\",\n\t\tclient.ID[:8],\n\t\tconnectionDuration,\n\t\tclient.MessageCount,\n\t\tclient.BytesSent,\n\t\tclient.BytesReceived,\n\t\tclient.ErrorCount,\n\t)\n}\n\n// startEnhancedMonitoring starts advanced monitoring and metrics collection\nfunc (em *EnhancedWebSocketManager) startEnhancedMonitoring() {\n\tticker := time.NewTicker(1 * time.Minute) // Update metrics every minute\n\tdefer ticker.Stop()\n\t\n\tfor {\n\t\tselect {\n\t\tcase <-em.ctx.Done():\n\t\t\treturn\n\t\t\t\n\t\tcase <-ticker.C:\n\t\t\tem.updateEnhancedMetrics()\n\t\t}\n\t}\n}\n\n// updateEnhancedMetrics calculates and updates comprehensive metrics\nfunc (em *EnhancedWebSocketManager) updateEnhancedMetrics() {\n\t// Get base memory metrics\n\tmemoryMetrics := em.memoryManager.GetMemoryMetrics()\n\t\n\t// Calculate enhanced metrics\n\tem.connectionsMux.RLock()\n\tvar totalMessages, totalBytesSent, totalBytesReceived int64\n\tvar totalLatency time.Duration\n\tvar errorCount int32\n\tconnectionCount := len(em.connections)\n\t\n\tfor _, client := range em.connections {\n\t\tclient.mutex.RLock()\n\t\ttotalMessages += client.MessageCount\n\t\ttotalBytesSent += client.BytesSent\n\t\ttotalBytesReceived += client.BytesReceived\n\t\ttotalLatency += client.AverageLatency\n\t\terrorCount += client.ErrorCount\n\t\tclient.mutex.RUnlock()\n\t}\n\tem.connectionsMux.RUnlock()\n\t\n\t// Calculate rates and averages\n\tvar averageLatency time.Duration\n\tvar messagesPerSecond, averageMessageSize, errorRate float64\n\t\n\tif connectionCount > 0 {\n\t\taverageLatency = totalLatency / time.Duration(connectionCount)\n\t\tif totalMessages > 0 {\n\t\t\taverageMessageSize = (totalBytesSent + totalBytesReceived) / totalMessages\n\t\t\terrorRate = float64(errorCount) / float64(totalMessages) * 100\n\t\t}\n\t}\n\t\n\t// Calculate health score (0-100)\n\thealthScore := em.calculateHealthScore(&memoryMetrics, errorRate)\n\t\n\t// Calculate capacity utilization\n\tmaxConnections := em.memoryManager.maxConnections\n\tconnectionCapacity := float64(memoryMetrics.ActiveConnections) / float64(maxConnections) * 100\n\t\n\t// Update enhanced metrics\n\tem.enhancedMetrics.mutex.Lock()\n\tem.enhancedMetrics.WebSocketMemoryMetrics = &memoryMetrics\n\tem.enhancedMetrics.TotalMessages = totalMessages\n\tem.enhancedMetrics.MessagesPerSecond = messagesPerSecond\n\tem.enhancedMetrics.AverageMessageSize = averageMessageSize\n\tem.enhancedMetrics.AverageLatency = averageLatency\n\tem.enhancedMetrics.HealthScore = healthScore\n\tem.enhancedMetrics.ErrorRate = errorRate\n\tem.enhancedMetrics.ConnectionCapacity = connectionCapacity\n\tem.enhancedMetrics.mutex.Unlock()\n\t\n\t// Log health summary periodically\n\tif int(time.Now().Unix())%300 == 0 { // Every 5 minutes\n\t\tlog.Printf(\"WebSocket Health Summary: %.1f%% health, %d active connections (%.1f%% capacity), %.2f%% error rate\",\n\t\t\thealthScore, memoryMetrics.ActiveConnections, connectionCapacity, errorRate)\n\t}\n}\n\n// calculateHealthScore computes overall system health score\nfunc (em *EnhancedWebSocketManager) calculateHealthScore(\n\tmemoryMetrics *WebSocketMemoryMetrics,\n\terrorRate float64,\n) float64 {\n\tscore := 100.0\n\t\n\t// Deduct for high connection utilization\n\tmaxConnections := float64(em.memoryManager.maxConnections)\n\tconnectionRatio := float64(memoryMetrics.ActiveConnections) / maxConnections\n\tif connectionRatio > 0.8 {\n\t\tscore -= (connectionRatio - 0.8) * 100 // Deduct up to 20 points\n\t}\n\t\n\t// Deduct for high error rate\n\tif errorRate > 1.0 {\n\t\tscore -= errorRate * 2 // Deduct 2 points per percent error rate\n\t}\n\t\n\t// Deduct for memory issues\n\tif memoryMetrics.MemoryUsageBytes > 100*1024*1024 { // 100MB threshold\n\t\tmemoryRatio := float64(memoryMetrics.MemoryUsageBytes) / (100 * 1024 * 1024)\n\t\tscore -= (memoryRatio - 1.0) * 10 // Deduct points for excess memory\n\t}\n\t\n\t// Deduct for failed connections\n\tif memoryMetrics.TotalConnections > 0 {\n\t\tfailureRate := float64(memoryMetrics.FailedConnections) / float64(memoryMetrics.TotalConnections)\n\t\tif failureRate > 0.05 { // More than 5% failure rate\n\t\t\tscore -= failureRate * 50\n\t\t}\n\t}\n\t\n\t// Ensure score stays within bounds\n\tif score < 0 {\n\t\tscore = 0\n\t}\n\tif score > 100 {\n\t\tscore = 100\n\t}\n\t\n\treturn score\n}\n\n// GetEnhancedMetrics returns comprehensive WebSocket metrics\nfunc (em *EnhancedWebSocketManager) GetEnhancedMetrics() EnhancedWebSocketMetrics {\n\tem.enhancedMetrics.mutex.RLock()\n\tdefer em.enhancedMetrics.mutex.RUnlock()\n\t\n\t// Create copy of metrics\n\treturn EnhancedWebSocketMetrics{\n\t\tWebSocketMemoryMetrics: em.enhancedMetrics.WebSocketMemoryMetrics,\n\t\tTotalMessages:         em.enhancedMetrics.TotalMessages,\n\t\tMessagesPerSecond:     em.enhancedMetrics.MessagesPerSecond,\n\t\tAverageMessageSize:    em.enhancedMetrics.AverageMessageSize,\n\t\tAverageLatency:        em.enhancedMetrics.AverageLatency,\n\t\tP95Latency:           em.enhancedMetrics.P95Latency,\n\t\tP99Latency:           em.enhancedMetrics.P99Latency,\n\t\tHealthScore:          em.enhancedMetrics.HealthScore,\n\t\tResourceUtilization:   em.enhancedMetrics.ResourceUtilization,\n\t\tErrorRate:            em.enhancedMetrics.ErrorRate,\n\t\tConnectionCapacity:    em.enhancedMetrics.ConnectionCapacity,\n\t\tThroughputCapacity:    em.enhancedMetrics.ThroughputCapacity,\n\t}\n}\n\n// GetMemoryManager returns the memory leak prevention manager for direct access\nfunc (em *EnhancedWebSocketManager) GetMemoryManager() *MemoryLeakPreventionManager {\n\treturn em.memoryManager\n}\n\n// IsHealthy returns whether the enhanced manager is healthy\nfunc (em *EnhancedWebSocketManager) IsHealthy() bool {\n\tif em.isShutdown {\n\t\treturn false\n\t}\n\t\n\t// Check base manager health\n\tif !em.WebSocketManager.IsShutdown() {\n\t\t// Check memory manager health\n\t\tif !em.memoryManager.IsHealthy() {\n\t\t\treturn false\n\t\t}\n\t\t\n\t\t// Check enhanced metrics\n\t\tmetrics := em.GetEnhancedMetrics()\n\t\treturn metrics.HealthScore > 70.0 && metrics.ErrorRate < 5.0\n\t}\n\t\n\treturn false\n}\n\n// Shutdown gracefully shuts down the enhanced WebSocket manager\nfunc (em *EnhancedWebSocketManager) Shutdown(ctx context.Context) error {\n\tvar shutdownErr error\n\t\n\tem.shutdownOnce.Do(func() {\n\t\tem.isShutdown = true\n\t\t\n\t\tlog.Printf(\"Shutting down enhanced WebSocket manager...\")\n\t\t\n\t\t// Shutdown memory manager first\n\t\tif err := em.memoryManager.Shutdown(ctx); err != nil {\n\t\t\tlog.Printf(\"Memory manager shutdown error: %v\", err)\n\t\t\tshutdownErr = err\n\t\t}\n\t\t\n\t\t// Shutdown base WebSocket manager\n\t\tif err := em.WebSocketManager.Shutdown(ctx); err != nil {\n\t\t\tlog.Printf(\"Base WebSocket manager shutdown error: %v\", err)\n\t\t\tif shutdownErr == nil {\n\t\t\t\tshutdownErr = err\n\t\t\t}\n\t\t}\n\t\t\n\t\t// Clean up enhanced connections\n\t\tem.connectionsMux.Lock()\n\t\tconnectionCount := len(em.connections)\n\t\tfor connID, client := range em.connections {\n\t\t\tem.cleanupEnhancedClient(client)\n\t\t\tdelete(em.connections, connID)\n\t\t}\n\t\tem.connectionsMux.Unlock()\n\t\t\n\t\tlog.Printf(\"Enhanced WebSocket manager shutdown completed. Cleaned up %d connections.\", connectionCount)\n\t})\n\t\n\treturn shutdownErr\n}\n\n// ForceMemoryCleanup immediately cleans up stale connections and resources\nfunc (em *EnhancedWebSocketManager) ForceMemoryCleanup() int {\n\treturn em.memoryManager.ForceCleanupStaleConnections()\n}\n\n// Helper function to estimate client memory usage\nfunc estimateClientMemoryUsage() int64 {\n\t// Rough estimation of memory usage per client\n\t// Includes: connection struct, channels, buffers, metrics\n\treturn 8192 // 8KB per client estimate\n}",
      "structuredPatch": []
    }
  }
]